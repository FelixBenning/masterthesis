% !TEX root = ../ms.tex

\chapter{Introduction}

\section{Optimization in Machine Learning}

In statistics or machine learning we usually want to find some model
which ``best describes'' our data. So let \(\model(X)\) be a prediction for
\(Y\), parametrized by weights \(\weights\), where \(Z=(X,Y)\) is drawn from a
distribution \(\dist\) of real world examples. We formalize the prediction error
of our parameter \(\weights\) with a loss function \(l(\weights, z)\), e.g. 
a squared loss
%
\begin{align}\label{eq: square loss}
	\loss(\weights, z) := (\model(x) - y)^2.
\end{align}
%
Then we want to minimize the expected loss over all (possible) examples, i.e.
``(theoretical) risk''
%
\begin{align*}
	\Loss(\weights) := \E_{Z\sim \dist} [\loss(\weights, Z)].
\end{align*}
In general, we do not have access to this expected loss. Instead we usually
assume to be in possession of only a sample \(\sample=(Z_1, \dots,
Z_{\sampleSize})\) of data, drawn (independently) from distribution \(\dist\).
This gives rise to the ``empirical risk''
\begin{align}\label{eq: empirical risk}
	\Loss_\sample(\weights)
	:= \frac{1}{\sampleSize}\sum_{k=1}^\sampleSize \loss(\weights, Z_k)
	= \E_{Z\sim \dist_\sample}[\loss(\weights, Z)],
\end{align}
where \(\dist_\sample\) is the empirical distribution generated by sampling
uniformly from the set \(\sample\). The empirical risk usually approaches
the theoretical risk as \(\sampleSize\to\infty\), since some law of large number
is usually applicable (e.g. if we sample from \(\sample\) independently). Thus the
next best thing to minimizing \(\Loss\) is often to minimize \(\Loss_\sample\).
In the case of a linear model
\begin{align*}
	\model(x) = \langle x, \weights \rangle
\end{align*}
minimizing the empirical risk (\ref{eq: empirical risk}) induced by a square
loss (\ref{eq: square loss})
\begin{align*}
	\Loss_\sample(\weights)
	= \frac{1}{\sampleSize}\sum_{k=1}^\sampleSize (\model(X_k)-Y_k)^2
	= \frac{1}{\sampleSize}\| \mathbf{X}^T\weights - \mathbf{Y}\|^2
\end{align*}
can be done analytically, resulting in the well known least squared
regression
\begin{align*}
	\minimum = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} \quad \text{where}\quad 
	\begin{aligned}
		\mathbf{X}&=(X_1,\dots,X_\sampleSize)^T\\
		\mathbf{Y}&=(Y_1,\dots,Y_\sampleSize)^T
	\end{aligned}.
\end{align*}
But this is not possible in general. And even if it was possible, it might still
be difficult to come up with the solution, slowing down experimentation
with different types of models. We are therefore interested in (numeric)
algorithms which can minimize arbitrary loss functions.

This train of thought leads us to various ``black box'' models where we assume
no knowledge of the loss function itself but assume we have some form of oracle
telling us things about the loss function. ``Zero order oracles'' allow us to
evaluate the loss function at a particular point, i.e.\ retrieve \(\Loss(\weights)\)
for any \(\weights\) at a fixed cost. ``First order oracles'' provide us not
only with \(\Loss(\weights)\) but also with \(\nabla\Loss(\weights)\), and so on. 

In some sense these black box models are extremely wasteful as we do not utilize
any of the inherent structure of our model. ``Structural optimization'' addresses
this problem and opens up the black box and makes further assumptions about
the loss function. Proximal methods for example assume that some form of convex
regularization function (e.g. distance penalty) is part of our overall loss
function \parencite[e.g.][]{bottouOptimizationMethodsLargeScale2018}:
\begin{align*}
	\Loss(\weights)
	= F(\weights) + \lambda \underbrace{\Omega(\weights)}_{\mathclap{\text{regularizer}}}.
\end{align*}
Nevertheless we will mostly navigate around structural optimization in
this work, only touching on it again with ``mirror descent'' in
Remark~\ref{rem: mirror descent}.

We will also assume that the set of possible weights is a real
vector space \(\reals^\dimension\). Since projections \(\Pi_\Omega\) to convex
subsets \(\Omega\) are contractions \parencite[Lemma
3.1]{bubeckConvexOptimizationAlgorithms2015}, i.e.
\begin{align*}
	\| \Pi_\Omega(x) - \Pi_\Omega(y) \| \le \| x - y \|,
\end{align*}
they can often be applied without any complications to the proofs for unbounded
optimization we will cover, which show a reduction in the distance to the
minimum
\begin{align*}
	\|\weights_{n+1} - \minimum \|
	&= \|\Pi_\Omega(\tilde{\weights}_{n+1}) - \Pi_\Omega(\minimum)\|
	\le \|\tilde{\weights}_{n+1} - \minimum \|\\
	&\le \dots \text{ [unbounded arguments] }\\
	&\le \|\weights_n - \minimum\|.
\end{align*}

The next section will explain why we will mostly skip zero order
optimization.

\section{Zero Order Optimization}

If we assume an infinite number of possible inputs \(\weights\), it does not matter
how often and where we sample \(\Loss\), there is always a different \(\weights\) where
\(\Loss\) might be arbitrarily lower (see various ``No-Free-Lunch Theorems'').
We must therefore start making assumptions about \(\Loss\), to make this
problem tractable.

\subsection{Grid Search}

It is reasonable to assume that the value of our loss \(\Loss\) at some point
\(\weights\) will not differ much from the loss of points in close proximity.
Otherwise we would have to sample every point, which is not possible for continuous
and thus infinite parameter sets. This assumption is continuity of the loss
\(\Loss\). One possible formulation is ``\(\lipConst\)-Lipschitz continuity''
%
\begin{align*}
	| \Loss(\weights_1) - \Loss(\weights_2) | < \lipConst | \weights_1 - \weights_2 |.
\end{align*}
%
And while Lipschitz continuity is sufficient to find an \(\lossError\)-solution
\(\hat{\weights}\) inside a unit (hyper-)cube
%
\begin{align*}
	\Loss(\hat{\weights}) \le \inf_{ 0\le\weights_i\le 1} \Loss(\weights) + \lossError
\end{align*}
%
in a finite number of evaluations of the loss, it still requires a full
``grid search'' (i.e.\ tiling our parameter set with a grid into (tiny) cells
and trying out one point for every cell) with time complexity \parencite[pp.
12,13]{nesterovLecturesConvexOptimization2018}
%
\begin{align*}
	\left(\left\lfloor \frac{\lipConst}{2\lossError}\right\rfloor + 1\right)^\dimension
\end{align*}
%
where \(\dimension\) is the dimension of \(\weights\). The fact that grid search is
sufficient can be seen by tiling the space into a grid and asking the oracle
for the value of the loss function at each center. If the minimum is in
some tile, the Lipschitz continuity forces the center of that tile to be \(\lossError\)
close to it \parencite[cf.][p. 11]{nesterovLecturesConvexOptimization2018}.

The fact that this is necessary can be seen by imagining a ``resisting
oracle'' which places the minimum in just the last grid element where we look
\parencite[cf.][p. 13]{nesterovLecturesConvexOptimization2018}. The exponential
increase of complexity in the dimension makes the problem intractable for even
mild dimensions and precisions (e.g. \(\dimension=11\), \(\lossError=0.01\), \(\lipConst=2\)
implies millions of years of computation).

\subsection{Other Algorithms}

There are more sophisticated zero order optimization techniques than grid
search like ``simulated annealing'' \parencite[e.g.][]{bouttierConvergenceRateSimulated2019}
or ``evolutionary algorithms'' \parencite[e.g.][]{heConditionsConvergenceEvolutionary2001}.
But in the end they all scale relatively poor in the dimension. Calculating the
\mbox{(sub-)gradient} on the other hand scales only linearly in the dimension.
Calculating the second derivative (Hessian) requires \(O(\dimension^2)\)
operations, and doing something useful with it (e.g. solving a linear
equation) usually causes \(O(\dimension^3)\) operations. For this reason first
order methods seem most suited for high dimensional optimization, as their
convergence guarantees are independent of the dimension. But there are attempts
to make second order methods viable by avoiding the calculation of the Hessian.
These methods are usually called ``Quasi Newton methods'' of which we will 
sketch the most common methods in Chapter~\ref{chap: other methods}.

\section{Related Work}

A similar review was conducted by
\textcite{bottouOptimizationMethodsLargeScale2018} with a stronger focus on
second order methods but only passing interest in
momentum methods which are quite successful in practice.
The second half of \textcite{nesterovLecturesConvexOptimization2018} introduces
more ideas for structural optimization, the first half covers both first order
and second order methods extensively but does not consider stochasticity much
and provides little intuition.
A review with heavier focus on geometric methods (Section~\ref{sec: geometric methods}) 
was conducted by \textcite{bubeckConvexOptimizationAlgorithms2015}.

\section{Readers Guide}

In Chapter~\ref{chap: gradient descent} we will review the well
established method of gradient descent which can minimize deterministic
functions. It might be tempting to skip this chapter, but note that this chapter 
introduces important building blocks we will use in subsequent chapters. We
prove convergence of gradient descent mostly as motivation for these building blocks.
The convergence proofs themselves are actually very short.

The complexity bounds in Section~\ref{sec: complexity bounds} on the other hand,
are not required for subsequent chapters. Neither is the loss surface analysis
in Section~\ref{sec: loss surface} and backtracking in Section~\ref{sec:
backtracking}, but they motivate assumptions we make.

While gradient descent is applicable to \(\Loss_\sample\), i.e.\ allows us
to minimize empirical risk (\ref{eq: empirical risk}), we will question this
approach in Chapter~\ref{chap: sgd} where we show that it is possible to
minimize \(\Loss\) itself, provided we have an infinite supply of samples. If we
start to reuse samples we would just end up minimizing \(\Loss_\sample\) again.

In Chapter~\ref{chap: momentum} we will discuss methods to speed up
gradient descent. This chapter is mostly independent of Chapter~\ref{chap: sgd},
so they can be read in any order.

Chapter~\ref{chap: other methods} provides a review of all the methods we did
not have time to go over in detail. This review should provide enough intuition
to understand their importance, but we will skip convergence proofs entirely
and sometimes only sketch the ideas.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
