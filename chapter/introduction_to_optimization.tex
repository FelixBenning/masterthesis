% !TEX root = ../Masterthesis.tex

\chapter{Introduction}

\section{Optimization in Machine Learning}

In statistics or machine learning we usually want to find some model
which ``best describes'' our data. So let \(\model(X)\) be a prediction for
\(Y\), parametrized by weights \(\weights\), where \(Z=(X,Y)\) is drawn from a
distribution \(\dist\) of real world examples. We formalize the prediction error
of our parameter \(\weights\) with a loss function \(l(\weights, z)\), e.g. 
a squared loss
%
\begin{align}\label{eq: square loss}
	\loss(\weights, z) := (\model(x) - y)^2.
\end{align}
%
Then we want to minimize the expected loss over all (possible) examples, i.e.
``(theoretical) risk''
%
\begin{align*}
	\Loss(\weights) := \E_{Z\sim \dist} [\loss(\weights, Z)].
\end{align*}
In general we do not have access to this expected loss. Instead we usually
assume to be in possession of only a sample \(\sample=(Z_1, \dots,
Z_{\sampleSize})\) of data, drawn (independently) from distribution \(\dist\).
This gives rise to the ``empirical risk''
\begin{align}\label{eq: empirical risk}
	\Loss_\sample(\weights)
	:= \frac{1}{\sampleSize}\sum_{k=1}^\sampleSize \loss(\weights, Z_k)
	= \E_{Z\sim \dist_\sample}[\loss(\weights, Z)],
\end{align}
where \(\dist_\sample\) is the empirical distribution generated by sampling
uniformly from the set \(\sample\). The empirical risk usually approaches
the theoretical risk as \(\sampleSize\to\infty\), since some law of large number
is usually applicable (e.g. if we sample from \(\sample\) independently). Thus the
next best thing to minimizing \(\Loss\) is often to minimize \(\Loss_\sample\).
In the case of a linear model
\begin{align*}
	\model(x) = \langle x, \weights \rangle
\end{align*}
minimizing the empirical risk (\ref{eq: empirical risk}) induced by a square
loss (\ref{eq: square loss})
\begin{align*}
	\Loss_\sample(\weights)
	= \frac{1}{\sampleSize}\sum_{k=1}^\sampleSize (\model(X_k)-Y_k)^2
	= \frac{1}{\sampleSize}\| \mathbf{X}^T\weights - \mathbf{Y}\|^2
\end{align*}
can be done analytically, resulting in the well known least squared
regression
\begin{align*}
	\minimum = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} \quad \text{where}\quad 
	\begin{aligned}
		\mathbf{X}&=(X_1,\dots,X_\sampleSize)^T\\
		\mathbf{Y}&=(Y_1,\dots,Y_\sampleSize)^T
	\end{aligned}.
\end{align*}
But this is not possible in general. And even if it was possible, it might still
be difficult to come up with the solution, slowing down experimentation
with different types of models. We are therefore interested in (numeric)
algorithms which can minimize arbitrary loss functions.

This train of thought leads us to various ``black box'' models where we assume
no knowledge of the loss function itself but assume we have some form of oracle
telling us things about the loss function. ``Zero order oracles'' allow us to
evaluate the loss function at a particular point, i.e. retrieve \(\Loss(\weights)\)
for any \(\weights\) at a fixed cost. ``First order oracles'' provide us not
only with \(\Loss(\weights)\) but also with \(\nabla\Loss(\weights)\), and so on. 

In some sense these black box models are extremely wasteful as we do not utilize
any of the inherent structure of our model. ``Structural optimization'' addresses
this problem and opens up the black box and makes further assumptions about
the loss function. Proximal methods for example assume that some form of convex
regularization function (e.g. distance penalty) is part of our overall loss
function \parencite[e.g.][]{bottouOptimizationMethodsLargeScale2018}:
\begin{align*}
	\Loss(\weights)
	= F(\weights) + \lambda \underbrace{\Omega(\weights)}_{\mathclap{\text{regularizer}}}.
\end{align*}
Nevertheless we are going to mostly navigate around structural optimization in
this work, only touching on it again with ``mirror descent'' in
Remark~\ref{rem: mirror descent}.

We are also going to assume that the set of possible weights is a real
vector space \(\reals^\dimension\). Since projections \(\Pi_\Omega\) to convex
subsets \(\Omega\) are contractions \parencite[Lemma
3.1]{bubeckConvexOptimizationAlgorithms2015}, i.e.
\begin{align*}
	\| \Pi_\Omega(x) - \Pi_\Omega(y) \| \le \| x - y \|,
\end{align*}
they can often be applied without any complications to the proofs for unbounded
optimization we will cover, which show a reduction in the distance to the
minimum
\begin{align*}
	\|\weights_{n+1} - \minimum \|
	&= \|\Pi_\Omega(\tilde{\weights}_{n+1}) - \Pi_\Omega(\minimum)\|
	\le \|\tilde{\weights}_{n+1} - \minimum \|\\
	&\le \text{ unbounded arguments }\\
	&\le \|\weights_n - \minimum\|.
\end{align*}

The next section will explain why we are going to mostly skip zero order
optimization.

\section{Zero Order Optimization}

If we assume an infinite number of possible inputs \(\weights\), it does not matter
how much and where we sample \(\Loss\) there is always a different \(\weights\) where
\(\Loss\) might be arbitrarily lower (see various ``No-Free-Lunch Theorems'').
We must therefore start making assumptions about \(\Loss\) to make this
problem tractable.

\subsection{Grid Search}

It is reasonable to assume that the value of our loss \(\Loss\) at some point
\(\weights\) will not differ much from the loss of points in close proximity.
Otherwise we would have to sample every point which is not possible for continuous
and thus infinite parameter sets. This assumption is continuity of the loss
\(\Loss\). One possible formulation is ``\(\lipConst\)-Lipschitz continuity''
%
\begin{align*}
	| \Loss(\weights_1) - \Loss(\weights_2) | < \lipConst | \weights_1 - \weights_2 |.
\end{align*}
%
And while Lipschitz continuity is sufficient to find an \(\lossError\)-solution
\(\hat{\weights}\) inside a unit (hyper-)cube
%
\begin{align*}
	\Loss(\hat{\weights}) \le \inf_{ 0\le\weights_i\le 1} \Loss(\weights) + \lossError
\end{align*}
%
in a finite number of evaluations of the loss, it still requires a full
``grid search'' (i.e. tiling our parameter set with a grid into (tiny) cells
and trying out one point for every cell) with time complexity \parencite[pp.
12,13]{nesterovLecturesConvexOptimization2018}
%
\begin{align*}
	\left(\left\lfloor \frac{\lipConst}{2\lossError}\right\rfloor + 1\right)^\dimension
\end{align*}
%
where \(\dimension\) is the dimension of \(\weights\). The fact that grid search is
sufficient can be seen by tiling the space into a grid and asking the oracle
for the value of the loss function at each center. If the minimum is placed in
some tile, the Lipschitz continuity forces the center of the tile to be \(\lossError\)
close to it \parencite[cf.][p. 11]{nesterovLecturesConvexOptimization2018}.

The fact that this is necessary can easily be seen by imagining a ``resisting
oracle'' which places the minimum in just the last grid element where we look
\parencite[cf.][p. 13]{nesterovLecturesConvexOptimization2018}. The exponential
increase of complexity in the dimension makes the problem intractable for even
mild dimensions and precisions (e.g. \(\dimension=11\), \(\lossError=0.01\), \(\lipConst=2\)
implies millions of years of computation).

\subsection{Other Algorithms}

There are more sophisticated zero order optimization techniques than grid
search like ``simulated annealing'' \parencite[e.g.][]{bouttierConvergenceRateSimulated2019}
or ``evolutionary algorithms'' \parencite[e.g.][]{heConditionsConvergenceEvolutionary2001}.
But in the end they all scale relatively poor in the dimension. Calculating the
\mbox{(sub-)gradient} on the other hand scales only linearly in the dimension.
Calculating the second derivative (hesse matrix) requires \(O(\dimension^2)\)
operations and doing something useful with it (e.g. solving a linear
equation), usually causes \(O(\dimension^3)\) operations. For this reason first
order methods seem most suited for high dimensional optimization, as their
convergence guarantees are independent of the dimension.

There are also first order optimization techniques we will not discuss like
Vaidya's cutting plane method
\parencite[e.g.][]{bubeckConvexOptimizationAlgorithms2015} with complexity
\(O(\dimension^4)\) or \(O(\dimension^3)\) ``up to logarithmic factors''
\parencite[Section 2.3]{bubeckConvexOptimizationAlgorithms2015} as they are
too expensive for our purposes.

\section{Gameplan}

As motivated above, we will mostly focus on first order methods. More
issues with second order methods are mentioned in Section~\ref{sec: loss
surface}. A more terse but comprehensive review of first order methods can be found in
\textcite{bubeckConvexOptimizationAlgorithms2015} which also considers cutting
plane algorithms as mentioned above. A review with a stronger focus on second
order methods can be found in
\textcite{bottouOptimizationMethodsLargeScale2018}. The second half of
\textcite{nesterovLecturesConvexOptimization2018} introduces more ideas for
structural optimization, the first half covers both first order and second
order methods extensively but does not consider stochasticity.

In Chapter~\ref{chap: gradient descent} we will review the well
established method of gradient descent which can minimize deterministic
functions. While this method is applicable to \(\Loss_\sample\), i.e. allows us
to minimize empirical risk (\ref{eq: empirical risk}), we will question this
approach in Chapter~\ref{chap: sgd} where we show that it is possible to
minimize \(\Loss\) itself provided we have an infinite supply of samples. If we
start to reuse samples we would just end up minimizing \(\Loss_\sample\) again.
In Chapter~\ref{chap: momentum} we will discuss methods to speed up
gradient descent.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
