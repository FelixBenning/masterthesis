% !TEX root = ../Masterthesis.tex

\chapter{Gradient Descent (GD)}\label{chap: gradient descent}

One relatively obvious way to utilize the gradient information
\(\nabla\Loss(\weights)\) provided by first order oracles, is to incrementally
move in the direction of steepest descent
\parencite{cauchyMethodeGeneralePour1847}, i.e.
%
\begin{align*}
	\weights_{n+1} = \weights_n - \lr\nabla \Loss(\weights_n).
\end{align*}
%
Where \(\lr\) denotes the ``learning rate''. A useful way to look at this
equation is to notice that it is the discretization of an ordinary differential
equation (ODE)\footnote{
	Using Newton's (dot) notation for the derivative.
}
%
\begin{align*}
	\dot{\weights}_n \approx \frac{\weights_{n+1} - \weights_n}{\lr}
	= - \nabla \Loss(\weights_n).
\end{align*}
%
If we view the learning rate as a time delta between times
\(t_n\) and \(t_{n+1}\), which implies \(t_n = n\lr\). Then for
\(\lr\to\infty\), we arrive at the ODE in time \(t\)
%
\begin{align}\label{eq: velocity is gradient}
	\tag{Gradient Flow}
	\dot{\weights}(t) = -\nabla \Loss(\weights(t)).
\end{align}
%
If you are familiar with Lyapunov functions you will not be surprised by the
next argument:
%
\begin{align}\label{eq: gradient integral}
	\Loss(\weights(t_1)) - \Loss(\weights(t_0))
	&= \int_{t_0}^{t_1} \nabla \Loss(\weights(s)) \cdot \dot{\weights}(s) ds
	= \int_{t_0}^{t_1} -\|\nabla \Loss(\weights(s))\|^2 ds
	\le 0
\end{align}
%
which immediately implies
%
\begin{align*}
	\Loss(\weights(t_0)) \ge \Loss(\weights(t_1)) \ge \dots \ge \inf_\weights \Loss(\weights) \ge 0
\end{align*}
implying convergence of \(\Loss(\weights(t))\). But it does not necessitate a
convergent \(\weights(t)\) nor convergence to \(\inf_\weights
\Loss(\weights)\).

Before we begin let us note the three types of convergence we are interested in
\begin{description}
	\item[Convergence of the Weights]
	\begin{align*}
		\|\weights_n - \minimum\| \to 0,
	\end{align*}
	\item[Convergence of the Loss]
	\begin{align*}
		|\Loss(\weights) - \Loss(\minimum)|
		=\Loss(\weights_n) - \Loss(\minimum) \to 0
	\end{align*}
	\item[Convergence of the Gradient]
	\begin{align*}
		\|\nabla\Loss(\weights_n)\|
		= \|\nabla\Loss(\weights_n) - \nabla\Loss(\minimum)\| \to 0
	\end{align*}
\end{description}

\section{Visualizing the 2nd Taylor Approximation}\label{sec: visualize gd}

To build some intuition what leads to convergence of the weights let us consider
more simple cases. For the second order Taylor approximation \(T_2\Loss\) of the
loss \(\Loss\)
%
\begin{align*}
	\Loss(\weights+x) \approx T_2\Loss(\weights+x)
	= \Loss(\weights) + x^T \nabla \Loss(\weights) + \tfrac12 x^T \nabla^2 \Loss(\weights) x,
\end{align*}
%
using only the first order condition, i.e. by setting the first derivative to zero
%
\begin{align}\label{eq: Taylor approx derivative}
	\nabla T_2\Loss(\weights+x) = \nabla \Loss(\weights) + \nabla^2\Loss(\weights) x \xeq{!} 0
\end{align}
%
we can find the minimum 
%
\begin{align}\label{eq: newton minimum approx relative coords}
	\hat{x} = -(\nabla^2 \Loss(\weights))^{-1}\nabla \Loss(\weights),
\end{align}
%
if we assume our Hesse matrix \(\nabla^2 \Loss(\weights)\) exists and is
positive definite (all eigenvalues positive) which also means it is invertible.
The next equation explains itself if you plug (\ref{eq: newton minimum approx
relative coords}) into it and compare it to (\ref{eq: Taylor approx derivative})
%
\begin{align*}
	\nabla T_2\Loss(\weights+x) = \nabla^2 \Loss(\weights)(x-\hat{x}).
\end{align*}
%
Similarly we can rewrite the original taylor approximation
%
\begin{align*}
	T_2\Loss(\weights+x)
	&= \Loss(\weights) - x^T \nabla^2 \Loss(\weights) \hat{x} + \tfrac12 x^T \nabla^2 \Loss(\weights) x \\
	&= \underbrace{
		\Loss(\weights) - \tfrac12 \hat{x} \nabla^2 \Loss(\weights) \hat{x}
	}_{
		=: c(\weights) \text{ (const.)}
	} + \tfrac12 (x-\hat{x})^T \nabla^2 \Loss(\weights)(x-\hat{x}).
\end{align*}
%
So far we have expressed the location of the minimum \(\hat{x}\) of the Taylor
approximation only relative to \(\weights\). To obtain its absolute coordinates
we define\footnote{We now motivated the Newton-Raphson Method in passing, which is
a second order method utilizing the second derivative.}
%
\begin{align}\label{eq: newton minimum approx}
	\tag{Newton-Raphson Step}
	\hat{\weights} := \weights + \hat{x}
	\xeq{(\ref{eq: newton minimum approx relative coords})}
	\weights -(\nabla^2 \Loss(\weights))^{-1}\nabla \Loss(\weights),
\end{align}
%
and obtain the notion of a paraboloid centered around vertex \(\hat{\weights}\)
%
\begin{align}\label{paraboloid approximation of L}
	\Loss(\theta)
	= \tfrac12 (\theta- \hat{\weights}) \nabla^2 \Loss(\weights) (\theta-\hat{\weights})
	+ c(\weights) + o(\|\theta-\weights\|^2)
\end{align}
%
\begin{wrapfigure}{O}{0.65\textwidth}
	\centering
	\def\svgwidth{0.65\textwidth}
	\input{media/contour.pdf_tex}
	\caption{Assuming center \(\hat{\weights}=0\), eigenvalues \(\hesseEV_1=1,
	\hesseEV_2=2\), and the respective eigenvectors \(v_1=(\sin(1), \cos(1))\)}
	\label{fig: 2d paraboloid}
\end{wrapfigure}
%
To fully appreciate Figure~\ref{fig: 2d paraboloid}, we now only need to realize
that the diagonizability of \(\nabla^2 \Loss(\weights)\)
%
\begin{align}\label{eq: diagnalization of the Hesse matrix}
	V \nabla^2 \Loss(\weights) V^T
	= \diag(\hesseEV_1,\dots,\hesseEV_d), \qquad V=(v_1,\dots, v_d)
\end{align}
%
implies that once we have selected the center \(\hat{\weights}\) and the direction of
one eigenspace in two dimensions, the other eigenspace has to be
orthogonal to the first one (due to orthogonality of eigenspaces with different
eigenvalues of symmetric matrices). This fully determines the direction of the
second eigenspace.

Before we move on I want to briefly mention that we only really needed the
positive definiteness of \(\nabla^2 \Loss(\weights)\) to make it invertible, so that
the vertex \(\hat{x}\) is well defined. If it was not positive definite but still invertible,
then \(\hat{x}\) would not be a minimum but all the other arguments would still
hold.
In that case the eigenvalues might be negative as well as positive, which would
represent a saddle point (cf. Figure~\ref{fig: visualize saddle point gd}), or
all negative which would represent a maximum.

Using the representation (\ref{paraboloid approximation of L}) of \(\Loss\) we
can write the gradient at \(\weights\) as
%
\begin{align}\label{eq: hesse representation of gradient}
	\nabla \Loss(\weights)
	=  \nabla^2 \Loss(\weights)(\weights-\hat{\weights})
	\ (= -\nabla^2 \Loss(\weights)\hat{x})
\end{align}
%
But note that \(\hat{\weights}\) depends on \(\weights\), so we need to index both
by \(n\) to rewrite gradient descent:
%
\begin{align*}
	\weights_{n+1} &= \weights_n - \lr\nabla \Loss(\weights_n)
	= \weights_n - \lr\nabla^2 \Loss(\weights_n)(\weights_n - \hat{\weights}_n).
\end{align*}
%
Subtracting \(\hat{\weights}_n\) from both sides we obtain the following
transformation 
%
\begin{align}\label{eq: Matrix GD Formulation}
	\weights_{n+1} - \hat{\weights}_n
	&= (\identity - \lr\nabla^2 \Loss(\weights_n) ) (\weights_n - \hat{\weights}_n).
\end{align}
%
Taking a closer look at this transformation matrix we can use (\ref{eq:
diagnalization of the Hesse matrix}) to see
%
\begin{align*}
	\identity - \lr\nabla^2 \Loss(\weights_n)
	&= V(\identity - \lr\cdot\diag(\hesseEV_1,\dots,\hesseEV_d) )V^T \\
	&= V\cdot\diag(1-\lr\hesseEV_1, \dots,1-\lr\hesseEV_d)V^T.
\end{align*}
%
Now if we assume like \textcite{gohWhyMomentumReally2017}, that the second
taylor approximation is accurate and thus that \(\nabla^2 \Loss(\weights)=H\) is a
constant, then \(\hat{\weights}_n = \minimum\) is the real minimum and we get
%
\begin{align}
	\weights_n - \minimum
	= V\cdot\diag[(1-\lr\hesseEV_1)^n,\dots,(1-\lr\hesseEV_d)^n] V^T (\weights_0 - \minimum)
\end{align}
%
by induction. Decomposing the distance to the minimum \(\weights - \minimum\)
into the eigenspaces of \(H\) we can see that each component scales
exponentially on its own 
%
\begin{align*}
	\langle \weights_n -\minimum, v_i\rangle
	= (1-\lr\hesseEV_i)^n \langle \weights_0 - \minimum, v_i\rangle
\end{align*}
%
This is beautifully illustrated with interactive graphs in
\citetitle{gohWhyMomentumReally2017} by \citeauthor{gohWhyMomentumReally2017}.

\section{Assumptions on the Loss \texorpdfstring{\(\Loss\)}{â„’}}

\subsection{Negative Eigenvalues}\label{subsec: Negative Eigenvalues}

Now if \(\hesseEV_i<0\), then \(1-\lr\hesseEV_i\) would be greater
than one which would repel this component \(\langle \weights_0 - \minimum,
v_i\rangle\) away from \(\minimum\). This is a good thing, since \(\minimum\)
is a maximum in this component. This means we will walk down local minima and
saddle points no matter the learning rate assuming  this component  was not
zero to begin with. In case of a maximum this would mean that we would start
right on top of the maximum, in case of a saddle point it implies starting on
the rim such that one can not roll down either side.

But since we are ultimately interested in stochastic loss functions, the
stochasticity would push us off these narrow equilibria. So being right on
top of them is of little concern. But being close to zero in such a component
still means slow movement away, since we are multiplying by it. This
(Figure~\ref{fig: visualize saddle point gd}) is a possible explanation for the
common observation in deep learning of ``long
plateaus on the way down when the error hardly change[s], followed
by sharp drops'' \parencite{sejnowskiUnreasonableEffectivenessDeep2020} when
the exponential factor ramps up.
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_saddlepoint.pdf_tex}
	\caption{Start=\(0.001v_1+4v_2\), \(\hesseEV_1=-1, \hesseEV_2=2\), learning rate\(=0.34\)}
	\label{fig: visualize saddle point gd}
\end{figure}

\subsection{Necessary Assumptions in the Quadratic Case}
\label{subsec: necessary assumptions in the quadratic case}

Since negative eigenvalues repel, let us consider strictly positive eigenvalues
now and assume that our eigenvalues are already sorted
%
\begin{align}
	0 < \hesseEV_1 \le \dots \le \hesseEV_d.
\end{align}
%
Then for positive learning rates all exponentiation bases are smaller than one
%
\begin{align*}
	1-\lr\hesseEV_d \le \dots \le 1-\lr\hesseEV_1 \le 1.
\end{align*}
%
But to ensure convergence we also need that all of them are larger than \(-1\).
This leads us to the condition
\begin{align}\label{eq: learning rate restriction (eigenvalue)}
	0< \lr < 2/\hesseEV_d
\end{align}
%
Selecting \(\lr = 1/\hesseEV_d\) reduces the exponentiation base (``rate'') of the
corresponding eigenspace to zero ensuring convergence in one step.
But if we want to maximize the convergence rate of \(\weights_n\), this is not
the best selection.

We can reduce the rates of the other eigenspaces if we
increase the learning rate further, getting them closer to zero. At this point
we are of course overshooting zero with the largest eigenvalue, so we only
continue with this until we get closer to \(-1\) with the largest eigenvalue
than we are to \(1\) with the smallest:
%
\begin{align*}
	\rate(\lr)=\max_{i} |1-\lr\hesseEV_i| = \max\{|1-\lr\hesseEV_1|, |1-\lr\hesseEV_d|\}.
\end{align*}
%
This is minimized when
%
\begin{align*}
	1-\lr\hesseEV_1 = \lr\hesseEV_d -1,
\end{align*}
%
implying an optimal learning rate
%
\begin{align}\label{eq: optimal SGD lr eigenvalue representation}
	\lr^* = \frac{2}{\hesseEV_1 + \hesseEV_d}.
\end{align}
%
If \(\hesseEV_1\) is much smaller than \(\hesseEV_d\), this leaves \(\lr\)
at the upper end of the interval in (\ref{eq: learning rate restriction
(eigenvalue)}). And the optimal convergence rate
%
\begin{align*}
	\rate(\lr^*)
	% &= 1-\lr^* \hesseEV_1
	= 1 - \frac{2}{1+\condition}
	\qquad \condition:=\hesseEV_d/\hesseEV_1 \ge 1
\end{align*}
%
becomes close to one if the condition number \(\condition\) is large.
If all the eigenvalues are the same on the other hand, the condition number
becomes one and the rate is zero, implying instant convergence. This is not
surprising if we recall our visualization in Figure~\ref{fig: 2d paraboloid}.
When the eigenvalues are the same, the contour lines are concentric circles and
the gradient points right at the center.

\subsection{Generalizing Assumptions}

To prove convergence in a more general setting we need to formulate the assumptions
we made about the eigenvalues of the hesse matrix without actually requiring
the existence of a hesse matrix if possible. 
\begin{definition}
	We call a matrix \(A\) ``positive definite'', if
	\begin{align*}
		\langle x, Ax \rangle \ge 0 \quad \forall x
	\end{align*}
	which is equivalent to \(A\) having only positive eigenvalues, as can be seen
	by testing the definition with eigenvectors and utilizing linearity to generalize.
	We denote
	\begin{align*}
		A \precsim B \iff B-A \text{ is positive definite}
	\end{align*}
\end{definition}

\subsubsection{Convexity}

Instead of non-negative eigenvalues we can demand ``convexity'' which does
not require the existence of a hesse matrix but is equivalent to its positive
definiteness (non-negative eigenvalues) if it exists.
%
\begin{definition}[Convexity]\label{def: convexity}
	A function \(f:\reals^\dimension\to \reals\) is called \emph{convex}, if 
	\begin{align}\label{eq: convex combination definition}
		f(x + \hesseEV(y-x)) \le f(x) + \hesseEV (f(y)-f(x))
		\qquad \forall x,y\in\reals^\dimension,\ \forall\hesseEV\in[0,1].
	\end{align}
	It can be shown \parencite[e.g.][Prop. 1.1]{bubeckConvexOptimizationAlgorithms2015} that this
	definition is equivalent to non-emptyness of the set of subgradients for all \(x\)
	\begin{align}\label{eq: subgradient definition}
		\partial f(x) = \{
			G \in\reals^\dimension: f(x) + \langle G, y-x\rangle \le f(y)
			\quad \forall y\in\reals^\dimension
		\}.
	\end{align}
	If \(f\) is differentiable and convex, then the gradient \(\nabla f\) is in
	this set of subgradients \parencite[e.g.][Prop.
	1.1]{bubeckConvexOptimizationAlgorithms2015} and therefore makes it nonempty.
	Which means that for differentiable \(f\) the statement
	\begin{align*}
		f(x) + \langle\nabla f(x), y-x\rangle \le f(y)\qquad \forall x,y\in\reals^\dimension
	\end{align*}
	is another possible definition. If \(f\) is twice differentiable then another
	equivalent statement is \parencite[e.g.][Theorem 2.1.4]{nesterovLecturesConvexOptimization2018}
	\begin{align*}
		0\precsim\nabla^2 f.
	\end{align*}
\end{definition}
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_convexity.pdf_tex}
	\caption{
		Visualize Convexity Definitions: We can see that a linear combination
		of two function values will stay above the linear combination of its inputs
		(\ref{eq: convex combination definition}) and that there is a slope inducing
		a line which touches the function in some point and stays below it
		(\ref{eq: subgradient definition}).
	}
	\label{fig: visualize convexity definition}
\end{figure}
%
\subsubsection{Strong Convexity and Lipschitz Continuous Gradient}

If you recall how the rate of convergence was related to the condition number
\(\hesseEV_\dimension/\hesseEV_1 =\condition\), then it is not surprising that
the lack of a lower and upper bound will have us struggle to get convergence as
\(\hesseEV_\dimension\) can be arbitrarily large and \(\hesseEV_1\) arbitrarily
small. To get similar convergence results we have to assume something like
%
\begin{align}\label{eq: upper lower bound on hesse matrix}
	\lbound \identity \precsim \nabla^2 \Loss(\weights) \precsim \ubound \identity.
\end{align}
%
We will see that the lower bound is not necessary to get convergence of the
loss function, but it is necessary to get convergence of the weights. It is
also necessary for a linear\footnote{
	In the Numerical Analysis, exponential rate of
	convergence is called ``linear convergence'' because the relationship
	\[
		\|\weights_{n+1} - \minimum\| \le c \|\weights_n - \minimum\|
	\]
	is linear. In
	general convergence order \(q\) implies 
	\[
		\|\weights_{n+1} -\minimum\| \le c\|\weights_n - \minimum\|^q.
	\]
	This is likely because quadratic convergence (\(q=2\)) already results in an ``actual
	rate of convergence'' of \(c^{\left(2^n\right)}\) which is double exponential and
	repeated application of \(\exp\) is annoying to write, so writers prefer the
	recursive notation above which motivates this seemingly peculiar naming.
	Slower convergence orders are referred to as ``sub-linear'' convergence.
}
rate of convergence. When it comes to the upper bound on the other hand recall
that we have used it to bound the learning rate, ensuring that
\(1-\lr\hesseEV_\dimension\) is larger than \(-1\). If we did not have an upper
bound then we could not guarantee this ``stability'' of gradient descent. Now
we do not want to assume the hesse matrix exists so let us integrate (\ref{eq:
upper lower bound on hesse matrix}) and use the constants to make sure that we
have equality of the functions and derivatives in \(\weights\), then we get
\begin{align}\label{eq: bregman divergence upper and lower bound}
	\frac{\lbound}{2}\| \theta - \weights\|^2
	\le \underbrace{
		\Loss(\theta) - \Loss(\weights) - \langle \nabla \Loss(\weights), \theta-\weights\rangle
	}_{=:\bregmanDiv{\Loss}(\theta, \weights)\quad\text{(Bregman Divergence)}}
	\le \frac{\ubound}{2}\| \theta - \weights\|^2.
\end{align}
The ``Bregman Divergence'' is the distance between the real function and its first
taylor approximation. If we would move the first taylor approximation from the
middle to the sides on the other hand, this represents a quadratic upper and lower
bound on \(\Loss\) as illustrated in Figure~\ref{fig: visualize strong
convexity}. For \(\lbound=0\) we get the definition of convexity back.
\begin{definition}\label{def: strong convexity}
	A function \(\Loss\) is \(\lbound\)-\emph{strongly convex} for some
	\(\lbound>0\), if for all \(x,y\)
	\begin{align*}
		\Loss(x) + \langle \nabla \Loss(x), y-x\rangle + \frac{\lbound}{2}\|y-x\|^2 \le \Loss(y)
	\end{align*}
\end{definition}

The upper bound can be motivated in another way: First notice that
the operator norm is equal to the largest absolute eigenvalue
\begin{align*}
	\|A\| := \sup_{\|x\| =1} \|Ax\|
	= \sup_{\|x\| =1} \sqrt{\langle Ax, Ax\rangle}
	= \sup_{\|x\| =1} \sqrt{\sum_{i=1}^\dimension \hesseEV_i^2 x_i^2},
	= \max_{i=1,\dots,\dimension} |\hesseEV_i|
\end{align*}
where we have used the orthonormal basis of eigenvectors \((v_i)_i\) with
eigenvalues \((\hesseEV_i)_i\) of a (symmetric) matrix A to represent a point
\(x\in\reals^\dimension\)
\begin{align*}
	x = \sum_{i=1}^{d}x_i v_i.
\end{align*}
As we have no negative eigenvalues in the convex case, the upper bound on the
hesse matrix is equivalent to a bound on its operator norm. And since a
bounded derivative is equivalent to Lipschitz continuity of the function itself
(cf. Lemma~\ref{lem-appendix: lipschitz and bounded derivative}) another way
to express our upper bound is by requiring Lipschitz continuity of the gradient
\(\nabla \Loss\). Under convexity Lipschitz continuity and our upper bound are
in fact equivalent.
%
\begin{lemma}[\cite{nesterovLecturesConvexOptimization2018}]
	\label{lem: Lipschitz Gradient implies taylor inequality}
	If \(\nabla \Loss\) is \(\ubound\)-Lipschitz continuous, then
	\begin{align*}
		| \Loss(y) - \Loss(x) - \langle \nabla \Loss(x), y-x\rangle |
		\le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}
	If \(\Loss\) is convex, then the opposite direction is also true.
\end{lemma}
\begin{proof}
	See Appendix \ref{Appdx-lem: Lipschitz Gradient implies taylor inequality}.
\end{proof}
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_strong_convexity.pdf_tex}
	\caption{Strong convexity provides a tighter lower bound than convexity
	while Lipschitz continuity of the gradient provides an upper bound.}
	\label{fig: visualize strong convexity}
\end{figure}
%
\begin{definition}
	Finally let us borrow some notation from \citeauthor{nesterovLecturesConvexOptimization2018}:
	\begin{description}
		\item[{\(\lipGradientSet[q,p]{\ubound}\)}] the set of \(q\) times
		differentiable, convex functions with an \(\ubound\)-Lipschitz continuous
		\(p\)-th derivative
		\item[{\(\strongConvex[q,p]{\lbound}{\ubound}\)}] the \(\lbound\)-strongly convex
		functions from \(\lipGradientSet[q,p]{\ubound}\).
	\end{description}
\end{definition}

\section{The Role of ``Lipschitz Continuous Gradient''}

Using \(x\le|x|\) on Lemma~\ref{lem: Lipschitz Gradient implies taylor inequality}
explains Lipschitz continuity of the gradient as a distance penalty

\begin{align}\label{eq: lin approx + distance penalty notion}
	\Loss(\theta)
	= \overbrace{
		\Loss(\weights_n)+ \langle\nabla\Loss(\weights_n), \theta-\weights_n\rangle 
	}^{\text{linear approximation}}
	+ \overbrace{\underbrace{\bregmanDiv{\Loss}(\theta,\weights_n)}_{
		\xle{(\ref{eq: bregman divergence upper and lower bound})}
		\tfrac\ubound{2} \|\theta-\weights_n\|^2 
	}}^{\text{distance penalty}},
\end{align}
%
penalizing the fact that we do not have enough information about the loss if we
move farther away from our current point. Our limit on the change of the gradient
allows us to create this worst case upper bound though.
Gradient Descent with learning rate \(1/\ubound\) minimizes this upper bound.
%
\begin{lemma}\label{lem: smallest upper bound}
	Let \(\nabla \Loss\) be \(\ubound\)-Lipschitz continuous, then for any \(\weights\in\reals^d\)
	\begin{align}
		\weights - \tfrac{1}{\ubound}\nabla \Loss(\weights) 
		&= \arg\min_{\theta}\{
			\Loss(\weights)+ \langle\nabla \Loss(\weights), \theta-\weights\rangle + \tfrac{\ubound}{2}\|\theta-\weights\|^2 
		\}\\
		\label{eq: min approximation}
		\Loss\left(\weights-\tfrac{1}{\ubound}\nabla\Loss(\weights)\right)
		&\le \min_{\theta} \{
			\Loss(\weights)+ \langle\nabla \Loss(\weights), \theta-\weights\rangle
			+ \tfrac{\ubound}{2}\|\theta-\weights\|^2 
		\} \\ \nonumber
		&= \Loss(\weights) - \tfrac{1}{2\ubound} \|\nabla \Loss(\weights)\|^2
	\end{align}
	More generally, we have for all \(\lr\in(0, \tfrac{2}{\ubound})\)
	\begin{align*}
		\Loss(\weights - \lr\nabla\Loss(\weights)) - \Loss(\weights)
		\le-\lr(1-\tfrac{\ubound}2 \lr)\|\nabla\Loss(\weights)\|^2.
	\end{align*}
\end{lemma}
\begin{proof}
	The direction is irrelevant for the distance penalty term \(\|\theta-\weights\|^2\).
	So if we keep the length of the vector \(\theta-\weights\) constant at \(r\)
	we can first optimize over the direction and then later optimize over its
	length separately. Now the Cauchy-Schwarz inequality implies the optimal
	direction is along the gradient vector:
	\begin{align*}
		\langle \nabla\Loss(\weights),-\tfrac{r}{\|\nabla\Loss(\weights)\|}\nabla\Loss(\weights)\rangle
		&= - \|\Loss(\weights)\| r\\
		&\lxle{\text{CS.}}
		\min_{\theta} \{\langle \nabla \Loss(\weights), \theta-\weights \rangle : \|\theta-\weights\|=r \}.
	\end{align*}
	Now we only need to find the right step size to optimize the entire equation
	\begin{align*}
		&\min_{\theta} \{
			\Loss(\weights)+ \langle\nabla \Loss(\weights), \theta-\weights\rangle
			+ \tfrac{\ubound}{2}\|\theta-\weights\|^2 
		\} \\
		&= \min_{\lr} \{ \Loss(\weights) + \langle \nabla \Loss(\weights), -\lr \nabla \Loss(\weights)\rangle
		+ \tfrac{\ubound}{2}\|\lr\nabla \Loss(\weights)\|^2\}
	\end{align*}
	And a simple expansion of terms results in
	\begin{align*}
		\Loss(\weights-\lr\nabla\Loss(\weights))
		&\lxle{(\ref{eq: lin approx + distance penalty notion})}
		\Loss(\weights) + \langle \nabla \Loss(\weights), -\lr \nabla \Loss(\weights)\rangle
		+ \tfrac{\ubound}{2}\|\lr\nabla \Loss(\weights)\|^2\\
		&=\Loss(\weights) - \underbrace{\lr(1-\tfrac{\ubound}2\lr)}_{=\tfrac{2}{\ubound}\alpha(1-\alpha)}\|\nabla\Loss(\weights)\|^2
	\end{align*}
	using the reparametrization
	\begin{align*}
		\lr=\tfrac{2}{\ubound}\alpha\qquad \alpha \in (0,1).
	\end{align*}
	Therefore the learning rate \(\lr=1/\ubound\) right in the middle of the
	interval \((0, 2/\ubound)\) of guaranteed decrease minimizes our upper bound.
\end{proof}
\begin{remark}[Markovian Optimality of GD]
	What we have seen is that gradient descent is optimal in a ``markovian''
	sense with regard to a certain upper bound. As every step optimizes the
	decrease of our upper bound regardless of the history leading up to this
	point. But it is not optimal over \emph{multiple} steps as it might sometimes
	be sensible to incorporate past gradient information into our current step
	(Chapter~\ref{chap: momentum} Momentum).
\end{remark}

\begin{remark}[Mirror Descent]\label{rem: mirror descent}
	The basic idea of ``Mirror Descent''
	\parencite[e.g.][]{guptaAdvancedAlgorithmsFall2020,chenLargeScaleOptimizationData2019,bubeckConvexOptimizationAlgorithms2015}
	is: If we had a better upper bound on the Bregman divergence than Lipschitz
	continuity of the gradient 
	\begin{align*}
		\min_y f(y)
		&= \min_y \{f(x) + \langle f(x), y-x\rangle +\bregmanDiv{f}(y,x)\} \\
		&\le \min_y\{f(x) + \langle f(x), y-x\rangle +\tfrac{\ubound}{2} \|y-x\|^2\},
	\end{align*}
	then we could also improve on gradient descent by taking the minimum of this
	improved upper bound.
\end{remark}

\subsection{Convergence of the Gradient}

\begin{theorem}\label{thm: convergence of the gradient (only lip cont)}
	For a Loss function with \(\ubound\)-Lipschitz gradient and finite lower bound
	(i.e. \(\inf_{\weights}\Loss(\weights) >-\infty\)) Gradient Descent with
	learning rate \(\lr\in(0,2/\ubound)\) induces convergence of the gradient.
	The convergence rate is at least \(o(1/n)\). More precisely the average
	gradient converges with rate \(O(1/n)\)
	\begin{align*}
		\frac1{n}\sum_{k=0}^n \|\nabla\Loss(\weights_k)\|^2
		&\le \frac{\Loss(\weights_0) - \inf_{\weights}\Loss(\weights)}{n\lr(1-\tfrac{\ubound}2\lr)}.
	\end{align*} 
\end{theorem}
\begin{proof}
	Let \(\weights_n\) be generated with gradient descent with learning rate \(\lr\).
	Then we have	
	\begin{align*}
		\sum_{k=0}^n \|\nabla\Loss(\weights_k)\|^2
		&\xle{(\ref{eq: loss implies gradient convergence})}
		\sum_{k=0}^n \frac{\Loss(\weights_k) - \Loss(\weights_{k+1})}{\lr(1-\tfrac{\ubound}2\lr)}
		= \frac{\Loss(\weights_0) - \Loss(\weights_{n+1})}{\lr(1-\tfrac{\ubound}2\lr)}
		\\
		&\le \frac{\Loss(\weights_0) - \inf_{\weights}\Loss(\weights)}{\lr(1-\tfrac{\ubound}2\lr)}
	\end{align*}
	Taking \(n\to\infty\) this implies summability and thus convergence of the
	gradients. Since the harmonic series \(\sum_{k=0}^n 1/k\) is divergent,
	the gradients must necessarily converge faster.
\end{proof}

Since we can not guarantee convergence to a (global) minimum as convergence of
the gradient only implies convergence to a critical point, it is the weakest
form of convergence.

\begin{theorem}[Convergence Chain]\label{thm: convergence chain}
	If the loss \(\Loss\) has Lipschitz continuos gradient, then
	\begin{align*}
		\text{Weight Convergence}
		\implies \text{Loss Convergence}
		\implies \text{Gradient Convergence}
	\end{align*}
	more precisely
	\begin{align}
	\label{eq: weight implies loss convergence}
		\tfrac{1}{2\ubound}\|\nabla\Loss(\weights)\|^2
		&\le \Loss(\weights) - \Loss(\minimum)\\
	\label{eq: loss implies gradient convergence}
		&\le \tfrac{\ubound}{2}\|\weights-\minimum\|^2.
	\end{align}
\end{theorem}
\begin{proof}
	The first claim follows from Lemma~\ref{lem: smallest upper bound} (\ref{eq: min approximation})
	\begin{align*}
		\Loss(\minimum) \le \Loss(\weights -\tfrac{1}{\ubound}\nabla\Loss(\weights))
		\le \Loss(\weights) - \tfrac{1}{2\ubound}\|\nabla\Loss(\weights)\|^2.
	\end{align*}
	Claim (\ref{eq: loss implies gradient convergence}) follows from Lemma~\ref{lem:
	Lipschitz Gradient implies taylor inequality} using
	\(\nabla\Loss(\minimum)=0\):
	\begin{align*}
		\Loss(\weights) - \Loss(\minimum)
		= |\Loss(\weights) - \Loss(\minimum) - \langle \nabla\Loss(\minimum), \weights-\minimum\rangle|
		&\le \tfrac{\ubound}{2}\|\weights-\minimum\|^2
		\qedhere
	\end{align*}
\end{proof}

\subsection{Discussion}

Lipschitz Continuity of the Gradient provides us with a ``Trust Bound''
\(\lr\in(0,1/\ubound)\) on the gradient. It limits the rate of change of the
gradient and thus allows us to
formulate and optimize over an upper bound of the loss function resulting in
learning rate \(1/\ubound\).

If the gradient can change faster, we can trust it less and have to take smaller
steps. Even with infinitesimal steps (i.e. in the \ref{eq: velocity is
gradient} case) we need at least uniform continuity of \(\nabla\Loss\) such that
the inequality
\begin{align}\label{eq: bounded gradient integral}
	\int_{t_0}^\infty \|\nabla \Loss(\weights(s))\|^2 ds
	&= \Loss(\weights(t_0)) - \lim_{t\to\infty} \Loss(\weights(t)) \\
	&\le \Loss(\weights(t_0)) - \inf_{\weights} \Loss(\weights) < \infty \nonumber
\end{align}
%
derived from (\ref{eq: gradient integral}) is sufficient for a convergent
\(\|\nabla \Loss(\weights(t))\|\) otherwise the gradients could be arbitrarily
large or small. The discrete version of this integral inequality is the inequality
from Theorem~\ref{thm: convergence of the gradient (only lip cont)}
\begin{align*}
	\frac1{n}\sum_{k=0}^n \|\nabla\Loss(\weights_k)\|^2
	&\le \frac{\Loss(\weights_0) - \inf_{\weights}\Loss(\weights)}{n\lr(1-\tfrac{\ubound}2\lr)}.
\end{align*} 
Notice that if we use \(\tfrac{1}{\ubound}\) as the time increment \(\lr\)
between the \(\weights_k\), we only lose the factor \(1/2\) in our
discretization of the continuos version (\ref{eq: bounded gradient integral}) so
the bound is quite tight.

Another motivation for Lipschitz continuity is the fact that the gradient defines
the \ref{eq: velocity is gradient} ODE, and it is very common to use Lipschitz
continuity to argue for existence, uniqueness and stability of ODEs.

If the gradient does not converge neither can the weights, since their updates
are proportional to the difference between \(\weights_{n+1}\) and \(\weights_n\)
is proportional to the gradient.

\subsubsection{Gradient Convergence does not imply Weight Convergence}

If we only have that the derivative \(\dot{\weights}(t) = -\nabla \Loss(\weights(t))\)
converges, then the logarithm is an obvious example where this goes wrong.
Convergence of the gradient is therefore necessary but not sufficient.

If the series of unsquared gradient norms were finite as well on the other hand,
the \(\weights_n\) would be a cauchy sequence
%
\begin{align*}
	\|\weights_n - \weights_m \|
	\le \sum_{k=m}^{n-1} \|\weights_{k+1} - \weights_k\|
	= \lr \sum_{k=m}^{n-1} \|\nabla \Loss(\weights_k)\|.
\end{align*}
%
This provides us with some intuition how a situation might look like when the
gradient converges but not the sequence of \(\weights_n\). The gradient would
behave something like the harmonic series, as its squares converges and the
\(\weights_n\) would behave like the partial sums of the harmonic series which
behaves like the logarithm in the limit. So for
%
\begin{align*}
	\Loss(\weights) = \exp(-\weights)
\end{align*}
%
the Gradient Flow ODE is solved by
%
\begin{align*}
	\weights(t) &= \log(t)\\
	\nabla \Loss(t) &= -\tfrac1t
\end{align*}
%
which provides intuition how ``flat'' a minima has to be to cause such behavior.
The minimum at \(\infty\) has an infinitely wide basin which flattens out
more and more. If we wanted such an example in a bounded space we would have
to try and coil up such an infinite slope into a spiral, which spirals outwards
to avoid convergence. A spiral is of course not convex, so this hints
at why we are going to need a notion of increasing or infinite dimensions to
provide complexity bounds in Section~\ref{sec: complexity bounds} essentially
spiralling through the dimensions.

Since our example is one dimensional we can also immediately see the ``eigenvalue''
%
\begin{align*}
	\nabla^2 \Loss(\weights(t)) = \exp(-\weights(t)) = 1/t
\end{align*}
%
which decreases towards zero, stalling the movement towards the minimum.
We also have convexity in this example so we can get convergence of the loss
as we will see in the next section.

\section{Convergence with Convexity}\label{sec: convex convergence theorems}

We have used Lemma~\ref{lem: smallest upper bound} for an upper bound on the
sum of gradients using the fact that the distance of the current loss to the
minimal loss is bounded. But if we want to ensure convergence of the loss we
want to flip this around and find a lower bound on the gradient to ensure
the loss decreases at a reasonable speed.

One way to ensure such a lower bound on the gradient is using convexity.

\begin{theorem}[\cite{nesterovLecturesConvexOptimization2018}]
	\label{thm: convex function GD loss upper bound}
	Let \(\Loss\in\lipGradientSet{\ubound}\), let \(\minimum\) be the unique minimum.
	Then Gradient Descent with learning rate \(0 < \lr < 2/\ubound\) results in
	\begin{align}\label{eq: convex function loss upper bound}
		\Loss(\weights_n) - \Loss(\minimum)
		\le \frac{2\ubound\|\weights_0 - \minimum\|}{4 + n\ubound\lr(2-\ubound\lr)}
	\end{align}
	The optimal rate of convergence 
	\begin{align*}
		\Loss(\weights_n) - \Loss(\minimum)
		\le \frac{2\ubound\|\weights_0-\minimum\|}{4+n}
		\in O\Big(\frac{\ubound\|\weights_0-\minimum\|}{n}\Big)
	\end{align*}
	is achieved for \(\lr=1/\ubound\).
\end{theorem}
\begin{proof}
	To lower bound our gradient we use convexity and Cauchy-Schwarz
	\begin{align*}
		\Loss(\weights_n)-\Loss(\minimum)
		\xle{\text{convexity}} -\langle \nabla\Loss(\weights_n), \minimum-\weights_n\rangle
		\xle{\text{C-S}} \|\weights_n - \minimum\| \|\nabla\Loss(\weights_n)\|.
	\end{align*}
	This provides us with a lower bound 
	\begin{align}\label{eq: lower bound gradient size}
		\|\nabla\Loss(\weights)\|
		\ge \frac{\Loss(\weights_n)-\Loss(\minimum)}{\|\weights_n-\minimum\|}
		\ge \frac{\Loss(\weights_n)-\Loss(\minimum)}{\|\weights_0-\minimum\|}
	\end{align}
	if we can make sure that our weights do not move away from our minimum \(\minimum\).
	Utilizing convexity again we will prove in Lemma~\ref{lem: bermanDiv lower
	bound} that
	\begin{align*}
		\langle\nabla\Loss(\weights) - \nabla\Loss(\minimum), \weights-\minimum\rangle
		\ge \tfrac1{\ubound} \|\nabla\Loss(\weights) - \nabla\Loss(\minimum)\|^2.
	\end{align*}
	Keeping in mind \(\nabla \Loss(\minimum)=0\) this implies
	\begin{align}
		\nonumber
		\|\weights_{n+1} - \minimum\|^2
		&= \|\weights_n - \minimum\|^2
		- 2\lr\underbrace{
			\langle \nabla\Loss(\weights_n), \weights_n - \minimum\rangle
		}_{
			\ge \tfrac{1}\ubound \|\nabla \Loss (\weights_n) - \nabla\Loss(\minimum)\|^2
			\mathrlap{\quad \text{using Lemma~\ref{lem: bermanDiv lower bound}}}
		} + \lr^2\|\nabla \Loss(\weights_n)\|^2
		\\
		&\le \|\weights_n - \minimum\|^2 - 
		\underbrace{\lr}_{>0}\underbrace{(\tfrac{2}{\ubound}-\lr)}_{>0}
		\|\nabla\Loss(\weights_n)\|^2.
		\label{eq: decreasing weight difference}
	\end{align}
	%
	Now that we justified our lower bound of the gradient, we can bound the
	convergence of the loss
	\begin{align*}
		\Loss(\weights_{n+1}) - \Loss(\minimum)
		&\le \Loss(\weights_n)-\Loss(\minimum)
		- \overbrace{\lr(1-\lr\ubound/2)}^{=:\xi} \|\nabla \Loss(\weights_n)\|^2\\
		&\lxle{(\ref{eq: lower bound gradient size})}
		\Loss(\weights_n)-\Loss(\minimum)
		\underbrace{
			\left(1 - \tfrac{\xi}{\|\weights_0-\minimum\|^2}(\Loss(\weights_n)-\Loss(\minimum))\right)
		}_{\text{diminishing contraction}}
	\end{align*}
	%
	The convergence rate of such a recursion can be bounded (cf.
	Lemma~\ref{lem-appendix: diminishing contraction})\fxnote{move out of appendix} to be 
	%
	\begin{align*}
		\Loss(\weights_n)-\Loss(\minimum)
		&\le \frac{1}{
			\frac{1}{\Loss(\weights_0)-\Loss(\minimum)} + \tfrac{\xi}{\|\weights_0-\minimum\|^2}n
		}
	\end{align*}
	%
	Since our upper bound is increasing in \(\Loss(\weights_0)-\Loss(\minimum)\)
	we can use
	%
	\begin{align*}
		\Loss(\weights_0) - \Loss(\minimum)
		&\xle{(\ref{eq: weight implies loss convergence})}
		\frac{\ubound}{2}\|\weights_0-\minimum\|^2
	\end{align*}
	%
	to get an upper bound only dependent on \(\|\weights_0 - \minimum\|^2\)
	%
	\begin{align*}
		\Loss(\weights_n)-\Loss(\minimum)
		&\le \frac{\|\weights_0-\minimum\|^2}{
			\frac{2}{\ubound} + \xi n
		}
		= \frac{2\ubound\|\weights_0-\minimum\|^2}{
			4 + n 2\ubound \xi
		}.
	\end{align*}
	Finally we obtain (\ref{eq: convex function loss upper bound}) using our
	definition of \(\xi\).
\end{proof}
%
Now to the Lemma we promised to ensure that our weights do not move further away
from the minimum.
%
\begin{lemma}\label{lem: bermanDiv lower bound}
	Let \(f\in\lipGradientSet{\ubound}\)
	\begin{subequations}
	\begin{align}
		\bregmanDiv{f}(y,x)
		&\ge \tfrac{1}{2\ubound}\|\nabla f(x) - \nabla f(y)\|^2\\
		\label{eq: bregmanDiv lower bound b}
		\langle \nabla f(x) - \nabla f(y), x-y\rangle
		&\ge \tfrac{1}{\ubound}\|\nabla f(x) - \nabla f(y)\|^2
	\end{align}	
	\end{subequations}	
\end{lemma}
\begin{proof}
	Since \(y\mapsto \langle\nabla f(x), y-x\rangle\) is linear, we know that
	%
	\begin{align*}
		\phi(y):=\bregmanDiv{f}(y,x) = f(y)-f(x)-\langle \nabla f(x), y-x\rangle 
	\end{align*}
	%
	still has \(\ubound\)-Lipschitz gradient
	%
	\begin{align}\label{eq: bregman divergence gradient}
		\nabla\phi(y) = \nabla f(y) - \nabla f(x).
	\end{align}
	%
	This gradient is equal to the change of gradient from \(x\) to \(y\) and 
	therefore represents the error ``rate'' we are making assuming a constant gradient
	and linearly approximating \(f\), which is by definition the derivative of the size of
	\(\bregmanDiv{f}(y,x)\). Now this error ``rate'' can be translated
 	back into a real error using the stickiness of the gradient due to
	its Lipschitz continuity. I.e. by Lemma~\ref{lem: smallest upper bound} we know that
	%
	\begin{align*}
		\bregmanDiv{f}(x,x) = 0
		\xeq{f\text{ convex}} \min_z\phi(z)
		\xle{(\ref{eq: min approximation})} \phi(y) - \tfrac{1}{2\ubound}\|\nabla \phi(y)\|^2.
	\end{align*}
	%
	Now (\ref{eq: bregman divergence gradient}) implies our first result
	%
	\begin{align*}
		\tfrac{1}{2\ubound}\|\nabla f(y) - \nabla f(x)\|^2
		\le \phi(y) =\bregmanDiv{f}(y,x).
	\end{align*}
	The second inequality follows from adding \(\bregmanDiv{f}(x,y)\) to
	\(\bregmanDiv{f}(y,x)\).
\end{proof}

\subsection{Discussion}

Convexity allows us to find explicit rates not just limiting rates. In case
of Theorem~\ref{thm: convergence of the gradient (only lip cont)} we do not
know how many times the gradients got really low being close to a saddle
point for example until they actually stay low. This is all eaten up by the
constant in \(o(1/n)\).

As we have seen in Subsection~\ref{subsec: Negative Eigenvalues}, negative
eigenvalues act as a repelling force, preventing the transformation (\ref{eq:
Matrix GD Formulation}) from being a contraction and pushing us into convex
regions.

More specifically if we assume that our loss function \(\Loss\) goes to infinity when
our parameters go to infinity, then using the fact that we are descending down
the loss we get a bounded (compact) area 
%
\begin{align*}
	\{\weights : \Loss(\weights) \le \Loss(\weights_0)\} = \Loss^{-1}([0, \Loss(\weights_0)])
\end{align*}
%
in which we will stay. We can use that boundedness to argue that we will
eventually end up in a convex region if we are pushed out of the non-convex
regions.

That such a convex region exists follows from the existence of a minimum of
\(\Loss\) in that compact region which necessitates non-negative
eigenvalues. And continuity of the second derivative allows us to extend that to 
a local ball around the minimum. Eigenvalues equal to zero
are a bit of an issue, but if they become negative in some epsilon ball
then moving in that direction would lead us further down. Which is a contradiction
to our assumption that we created a ball around the minimum of \(\Loss\).

The statement that we are being pushed out of the non-convex regions is a bit
dubious as we could start right on top of local maxima or rims of saddle points.
But as we are ultimately interested in stochastic gradient descent, these zero
measure areas are of little concern. But as we already found out in
Subsection~\ref{subsec: Negative Eigenvalues}, starting close to such a feature
increases the amount of time it takes for us to escape that area. Which means
we can not really provide an upper bound on the time it takes to end up in
a locally convex area, as we can arbitrarily increase the escape time by
setting the starting point closer and closer to the local maxima or rim of a
saddle point.

So the best we can do without actually getting into probability theory is to
hand-wave this starting phase away with the unlikelihood of it taking too long
due to the exponential repulsion from negative eigenvalues. The probability
theory to make more precise statements is still in active development and will
require a metastability analysis of the SDE induced by stochastic gradient descent
\parencite[e.g.][]{bovierMetastabilityPotentialTheoreticApproach2015,nguyenFirstExitTime2019}
resulting in exit time bounds as well as an upper bound on the distance of SGD
to this limiting SDE
\parencite[e.g.][]{liStochasticModifiedEquations2017,ankirchnerApproximatingStochasticGradient2021}.

This difficulty is the reason why we like to skip this starting phase and only
provide a convergence analysis once we enter the final convex area.

\subsection{Convergence without Lipschitz Continuous Gradient}\label{subsec: subgradient method}

We might need Lipschitz continuity of the gradient for convergence of the
gradients. But if we can not have convergence of the parameters \(\weights\) we
might not care as much about the convergence of the gradient either. In that
case it turns out that we can even get convergence of the loss if we only assume
convexity and \(\lipConst\)-Lipschitz continuity of \(\Loss\) itself
(boundedness of the gradient), i.e.
\begin{align*}
	\Loss \in \lipGradientSet[0,0]{\lipConst}
\end{align*}
%
Since we can not guarantee a monotonic decrease with subgradients, the name
``descent'' is generally avoided and we have to keep a running minimum or
average all the weights. You can either look into \textcite[Section
2.2.3]{nesterovLecturesConvexOptimization2018} or \textcite[Section
2.1]{bubeckConvexOptimizationAlgorithms2015} for a proper
treatment or wait til Subsection~\ref{subsec: SGD with Averaging} where we use
the same proof technique in a more general stochastic setting. The convergence
rate decreases to
\begin{align*}
	O\Big(\frac{\|\weights_0-\weights_*\|\lipConst}{\sqrt{n}}\Big).
\end{align*}

\section{Convergence with Strong Convexity}\label{sec: Strong Convexity}

Now given that we have proven (\ref{eq: decreasing weight difference}), i.e.
\begin{align}\label{eq: weight iteration}
	\|\weights_{n+1}-\minimum\|^2
	\le \|\weights_n - \minimum\|^2 - \lr(\tfrac2{\ubound} -\lr)\|\nabla\Loss(\weights_n)\|^2
\end{align}
for convex functions and provided a lower bound for the gradient it might seem
like we should be able to state a convergence statement about the weights as
well. But the lower bound on our gradient uses the current loss delta. And if
our loss function is really flat, then the loss difference might already be
really small even if we are still far away in parameter space.

Strong Convexity prevents this by requiring a minimum of curvature (cf.
Figure~\ref{fig: visualize strong convexity}). With it we can provide a lower
bound of the gradient using the weight difference directly
\begin{align}
	\nonumber
	\|\nabla\Loss(\weights)\|\|\weights -\minimum\|
	&\lxge{\text{C-S}} \langle \nabla\Loss(\weights) -\nabla\Loss(\minimum), \weights - \minimum \rangle\\
	\nonumber
	&= \bregmanDiv{\Loss}(\weights, \minimum) + \bregmanDiv{\Loss}(\minimum, \weights)\\
	\label{eq: strong convexity implies PL}
	&\lxge{\text{strong convexity}} \lbound \|\weights-\minimum\|^2.
\end{align}
The resulting lower bound on the gradient 
\begin{align*}
	\|\nabla\Loss(\weights)\|\ge \lbound\|\weights -\minimum\|
\end{align*}
allows us to prove convergence of the weights with convergence of the gradients.
More specifically we can bound our weight iteration (\ref{eq: weight iteration})
\begin{align*}
	\|\weights_{n+1}-\minimum\|^2
	\le (1-\lr(\tfrac2{\ubound} -\lr)\lbound)\|\weights_n - \minimum\|^2.
\end{align*}
In other words we are closing the loop of Theorem~\ref{thm: convergence chain}.
Optimizing over the learning rate \(\lr\) (similarly to Lemma~\ref{lem: smallest
upper bound}) results in \(\lr=1/\ubound\) and convergence rate
\begin{align*}
	\|\weights_{n+1}-\minimum\|^2
	\le (1-\tfrac{\lbound}{2\ubound})\|\weights_n - \minimum\|^2
	\le (1-\tfrac{1}{2\condition})^{n+1}\|\weights_0-\minimum\|^2
\end{align*}
%
which is in the same ballpark as we got in the quadratic problem. But with a bit
more work we can actually achieve the same rates.
%
\begin{theorem}[\cite{nesterovLecturesConvexOptimization2018}]
	\label{thm: gd strong convexity convergence rate}
	If \(\Loss\in\strongConvex{\lbound}{\ubound}\), then for learning rate
	\[0 \le \lr \le \tfrac{2}{\ubound+\lbound}\]
	Gradient Descent generates a sequence \(\weights_n\) satisfying
	\begin{subequations}
	\begin{align}
		\label{eq: gd strong convexity convergence rate 1}
		\|\weights_n - \minimum\|
		&\le \left(
			1- 2\lr\frac{\ubound\lbound}{\ubound+\lbound}
		\right)^{n/2}
		\|\weights_0 - \minimum\| \\
		\label{eq: gd strong convexity convergence rate 2}
		\Loss(\weights_n) - \Loss(\minimum)
		&\le \frac\ubound{2} \left(
			1- 2\lr\frac{\ubound\lbound}{\ubound+\lbound}
		\right)^{n}
		\|\weights_0 - \minimum\|^2
	\end{align}
	\end{subequations}
	In particular for \(\lr=\tfrac2{\ubound+\lbound}\) we achieve the optimal
	rate of convergence
	\begin{subequations}\label{eq: gd strong convexity optimal rate}
	\begin{align}
		\|\weights_n - \minimum\|
		&\le \left(
			1- \frac{2}{1+\condition}
		\right)^n
		\|\weights_0 - \minimum\| \\
		\Loss(\weights_n) - \Loss(\minimum)
		&\le \frac\ubound{2} \left(
			1- \frac{2}{1+\condition}
		\right)^{2n}
		\|\weights_0 - \minimum\|^2
	\end{align}
	\end{subequations}
	where \(\condition=\ubound/\lbound\) is the condition number.
\end{theorem}
\begin{proof}[Proof {\parencite[Theorem 2.1.15]{nesterovLecturesConvexOptimization2018}}]
	This proof starts like the proof for convex functions, cf. (\ref{eq:
	decreasing weight difference})
 \begin{align*}
		\|\weights_{n+1} - \minimum\|^2
		&= \| \weights_n -\minimum - \lr\nabla\Loss(\weights_n)\|^2\\
		&= \| \weights_n - \minimum\|^2
		- 2\lr\langle \nabla\Loss(\weights_n), \weights_n - \minimum\rangle
		+ \lr^2 \| \nabla\Loss(\weights_n)\|^2.
	\end{align*}
	But now we need to find a better lower bound for the scalar product in the
	middle. Previously we used the lower bound on the Bregman Divergence from
	Lemma~\ref{lem: bermanDiv lower bound}. Now we could also use the lower bound
	from the strong convexity property directly. But we can not use both which
	seems like we are loosing sharpness in our bound. And indeed this bound can
	be improved to be
 	\begin{align*}
		&\langle \nabla\Loss(\weights_n) -\nabla\Loss(\minimum), \weights_n - \minimum\rangle	\\
		&\ge \tfrac{\ubound\lbound}{\ubound+\lbound}\|\weights_n - \minimum\|^2
		+ \tfrac{1}{\ubound+\lbound}
		\|\nabla\Loss(\weights_n) - \nabla\Loss(\minimum)\|^2
	\end{align*}
	Using this lower bound, which we will prove in Lemma~\ref{lem: bermanDiv
	lower bound (strongly convex)}, we get
 \begin{align*}
		\|\weights_{n+1} - \minimum\|^2
		\le \left(1- 2\lr \tfrac{\ubound\lbound}{\ubound+\lbound}\right)
		\|\weights_n - \minimum\|^2
		+ \underbrace{\lr}_{\ge 0}
		\underbrace{\left(\lr-\tfrac{2}{\ubound+\lbound}\right)}_{\le0}
		\|\nabla\Loss(\weights_n)\|^2
	\end{align*}
	which immediately proves (\ref{eq: gd strong convexity convergence rate
	1}) using induction. Note that we could additionally use our lower bound on the
	gradient. But this does not represent a significant improvement,
	especially for the optimal rates where we max out the learning rate making
	the term on the right equal to zero. (\ref{eq: gd strong convexity
	convergence rate 2}) follows from our upper bound on the loss using weights,
	cf. Theorem~\ref{thm: convergence chain} (\ref{eq: weight implies loss convergence})
	\begin{align*}
		\Loss(\weights_n) - \Loss(\minimum)
		\le \tfrac\ubound{2} \| \weights_n - \minimum\|^2.
	\end{align*}
	To get (\ref{eq: gd strong convexity optimal rate}) we simply have to plug
	in the presumed optimal learning rate \(\lr=2/(\ubound+\lbound)\) into
	our general rate and show that the rate is still positive which means that
	we have not overshot zero, which means that the upper bound on our learning
	rate which we presume to be optimal is binding:
	\begin{align*}
		\left(1- 4 \frac{\ubound\lbound}{(\ubound+\lbound)^2}\right)
		= \frac{(\ubound^2 + 2\ubound\lbound + \lbound^2) - 4 \ubound\lbound}{(\ubound+\lbound)^2}
		= \left(\frac{\ubound-\lbound}{\ubound+\lbound}\right)^2
		= \left(1-\frac{2\lbound}{\ubound+\lbound}\right)^2
	\end{align*}
	To get the representation in (\ref{eq: gd strong convexity optimal rate}) we
	just have to divide both enumerator and denominator by \(\lbound\).
\end{proof}

Now as promised we still have to show the improved lower bound.

\begin{lemma}
	\label{lem: bermanDiv lower bound (strongly convex)}
	If \(f\in\strongConvex{\lbound}{\ubound}\), then for any
	\(x,y\in\reals^\dimension\) we have
	\begin{align*}
		\langle \nabla f(x) - \nabla f(y), x-y\rangle 
		\ge \tfrac{\lbound\ubound}{\lbound+\ubound} \| x-y\|^2
		+ \tfrac{1}{\lbound+\ubound}\|\nabla f(x) -\nabla f(y)\|^2
	\end{align*}
\end{lemma}
\begin{proof}
	Similarly to Lemma~\ref{lem: bermanDiv lower bound} we want to
	add \(\bregmanDiv{f}(x,y)\) and \(\bregmanDiv{f}(y,x)\) together to
	get a lower bound for the scalar product equal to this sum. But since we
	already have a lower bound due to strong convexity
	\begin{align}\label{eq: strong convexity implies bregmanDiv lower bound}
		\bregmanDiv{f}(y,x) = f(y) - f(x) -\langle\nabla f(x), y-x\rangle
		\ge \tfrac{\lbound}2 \|y-x\|^2,
	\end{align}
	we first have to ``remove'' this lower bound from \(f\) to apply our other
	lower bound. So we define
	\begin{align*}
		g_x(y) := f(y) - \tfrac{\lbound}2\|y-x\|^2 \qquad
		\nabla g_x(y) = \nabla f(y) - \lbound(y-x)
	\end{align*}
	which still has positive Bregman Convergence, i.e. is convex
	\begin{align*}
		\bregmanDiv{g_x}(y,z)
		&= g_x(y) - g_x(z) - \langle\nabla g_x(z), y-z\rangle \\
		&= 
		\begin{aligned}[t]
			& f(y) - f(z) -\langle\nabla f(z), y-z\rangle \\
			&- \tfrac\lbound{2}\underbrace{\|y-x\|^2}_{
				=\|y-z\|^2 + \mathrlap{2\langle y-z, z-x\rangle + \|z-x\|^2}
			}
			+ \tfrac\lbound{2}\|z-x\|^2
			+ \lbound\langle z-x, y-z\rangle
		\end{aligned}\\
		&= \bregmanDiv{f}(y,z) - \tfrac{\lbound}2\|y-z\|^2
		\xge{(\ref{eq: strong convexity implies bregmanDiv lower bound})} 0
	\end{align*}
	and has \((\ubound-\lbound)\)-Lipschitz continuous gradient by
	Lemma~\ref{lem: Lipschitz Gradient implies taylor inequality} and 
	\begin{align}
		\label{eq: bregmanDiv upper bound}
		\bregmanDiv{f}(y,z) \le \tfrac{\ubound}2\|y-z\|^2.
	\end{align}
	The inequality above also follows from Lemma~\ref{lem: Lipschitz Gradient
	implies taylor inequality}. Therefore
	\(g_x\in\lipGradientSet{\ubound-\lbound}\) and if \(\ubound -\lbound>0\) we
	can now apply Lemma~\ref{lem: bermanDiv lower bound} to get
	\begin{align}
		\bregmanDiv{f}(y,x)
		&= \bregmanDiv{g_x}(y,x) + \tfrac{\lbound}2 \|y-x\|^2
		\nonumber \\
		&\ge \tfrac{1}{2(\ubound-\lbound)} \|\nabla g_x(x) - \nabla g_x(y)\|^2
		+ \tfrac{\lbound}2 \|y-x\|^2 
		\nonumber \\
		\label{eq: improved lower bound strong convex bregman Divergence}
		&\ge \tfrac{1}{2(\ubound-\lbound)}
		\|\nabla f(x) - \lbound x - (\nabla f(y)-\lbound y)\|^2
		+ \tfrac{\lbound}2 \|y-x\|^2.
	\end{align}
	Since the \(\|y-x\|^2\) lower bound improves the convergence rate in
	our convergence theorem, it is helpful to view it as the ``good'' part of
	our lower bound and view the application of the gradient lower bound as
	a fallback.

	The case \(\ubound=\lbound\) which we have to cover separately offers some
	more insight into that. In this case our lower bound  (\ref{eq:
	strong convexity implies bregmanDiv lower bound}) makes (\ref{eq: bregmanDiv
	upper bound}) an equality. This means we could get
	\begin{align*}
		\langle \nabla f(x) - \nabla f(y), x-y\rangle
		= \bregmanDiv{f}(x,y) + \bregmanDiv{f}(y,x) = \lbound \|y-x\|^2.
	\end{align*}
	But instead we apply Lemma~\ref{lem: bermanDiv lower bound} to half of it
	which results in the statement of this lemma.

	Now we just have to finish the case \(\ubound>\lbound\). In equation
	(\ref{eq: improved lower bound strong convex bregman Divergence}) we have
	already removed the dependence on our helper function \(g_x\) and will now
	use its symmetry to add together the mirrored Bregman Divergences to get
	\begin{align*}
		&\langle \nabla f(x) - \nabla f(y), x-y\rangle
		= \bregmanDiv{f}(x,y) + \bregmanDiv{f}(y,x) \\
		&\ge \tfrac{1}{\ubound-\lbound}
		\underbrace{\|\nabla f(x) - \lbound x - (\nabla f(y)-\lbound y)\|^2}_{
			=\|\nabla f(x) - \nabla f(y)\|^2
			- 2\lbound \langle \nabla f(x) - \nabla f(y), x-y\rangle
			\mathrlap{+ \lbound^2 \| x-y\|^2}
		}
		+ \lbound \|y-x\|^2.
	\end{align*}
	Moving the scalar product to the left we get
	\begin{align*}
		\overbrace{(\ubound+\lbound)/(\ubound-\lbound)}^{
			= (1+\tfrac{2\lbound}{\ubound-\lbound})
		}
		&\langle \nabla f(x) - \nabla f(y), x-y\rangle \\
		&\ge \tfrac{1}{\ubound-\lbound}\|\nabla f(x) - \nabla f(y)\|^2
		+ \underbrace{(\tfrac{\lbound^2}{\ubound-\lbound}-\lbound)}_{
			=\ubound\lbound/(\ubound-\lbound)
		} \|y-x\|^2.
	\end{align*}
	Dividing by the factor on the left finishes this proof.
 \end{proof}

\subsection{Discussion}

\textcite{karimiLinearConvergenceGradient2020} show that virtually all
attempts to generalize strong convexity are special cases of the
Polyak-\L{}ojasiewicz condition
\begin{align}
	\label{eq: PL condition}\tag{PL}
	\|\nabla \Loss(\weights) \|^2 \ge c(\Loss(\weights)-\inf_{\theta}\Loss(\theta)).
\end{align}
This condition does not require convexity but rather ``invexity'' of which
convexity is a special case. Considering Lemma~\ref{lem: smallest upper bound}
and Theorem~\ref{thm: convergence chain} it is perhaps not too surprising that a
lower bound on the gradient for a given distance we want to cover allows for
convergence proofs. In fact the convergence proof is just two lines:
Using Lemma~\ref{lem: smallest upper bound} and the \ref{eq: PL condition}-assumption we get
\begin{align*}
	\Loss(\weights_{n+1})- \Loss(\weights_n) \le -\tfrac1{2\ubound} \|\nabla\Loss(\weights_n)\|^2
	\le -\tfrac{c}{2\ubound}(\Loss(\weights_n) - \Loss(\minimum)),
\end{align*}
which then only requires subtracting \(\Loss(\minimum)-\Loss(\weights_n)\) from
both sides to get
\begin{align*}
	\Loss(\weights_{n+1}) - \Loss(\minimum)
	\le (1-\tfrac{c}{2\ubound})(\Loss(\weights_n) - \Loss(\minimum)).
\end{align*}
But while convexity comes with a geometric intuition this \ref{eq: PL
condition}-condition does not.

And since we have built intuition on how non-convex regions
repel iterates into convex regions we now have intuition how general smooth
functions behave and it is more difficult to build this intuition for non-invex
regions so we do not really gain much intuition by this generalization. But
this generalization highlights the attributes of (strong)-convexity we
actually need. And we have basically already proven this condition with
(\ref{eq: strong convexity implies PL}). We only need to use (\ref{eq: weight
implies loss convergence}) from Theorem~\ref{thm: convergence chain}, i.e.
weight convergence implies loss convergence.

\section{Loss Surface}\label{sec: loss surface}

A pain point of arguing that we are going to end up in \emph{some} convex area
as we are repelled from non-convex regions is that we are only going to
converge to some \emph{local} minima. This problem has been addressed by
\textcite{pascanuSaddlePointProblem2014} who summarize a distribution analysis
by \textcite{brayStatisticsCriticalPoints2007} of critical points in random
gaussian fields.

Intuitively the dimension increases the number of eigenvalues
of the hessian, making it exponentially less likely that \emph{all} of them are
either positive or negative. In other words: In high dimension almost all
critical points are saddle points! But the lower the loss at the critical point,
the likelier it is in fact a minimum. This means that on a gaussian random field
critical points which are local minima are likely close to the global minimum.

\textcite{pascanuSaddlePointProblem2014}
then confirm this hypothesis empirically by analyzing the share of saddle
points in all critical points found at a certain loss levels of a loss function
induced by training models on a reduced MNIST dataset. So it \emph{appears} that
random gaussian fields approximate deep learning problems sufficiently well to
argue that local minima will generally be close enough to the global minima,
since all other critical points are overwhelmingly likely to be saddle points.
Further theoretical justification for neuronal networks in particular is
provided by \textcite{choromanskaLossSurfacesMultilayer2015}.

\textcite{garipovLossSurfacesMode2018} found in an empirical study that local
minima can be connected by a simple path (e.g. linear spline with just one
free knot) with near-constant loss. In other words the local minima do not only
appear to be of similar height (in loss), they also appear to be connected
by a network of valleys. They then propose purposefully making SGD unstable
with higher learning rates once it converged, to explore these valleys and find
other minima to create ensemble models with (e.g. bagging, boosting, etc.). 

\textcite{izmailovAveragingWeightsLeads2019} find that averaging the weights
of these minima instead of creating ensembles from them seems to work as well.
In a convex area this is no surprise as a convex combination of points
has smaller loss than the linear combination of losses (see also
Subsection~\ref{subsec: SGD with Averaging}).


\subsection{Second Order Methods}

The assumption of strong convexity can lead people to suggest using second order
methods. In particular the Newton-Raphson method which has much better convergence
guarantees. But while we do assume convexity to be able to bound convergence
rates, we still want to use our method on non-convex functions. And the
Newton-Raphson method has a bunch of issues in this case.

As we can see in (\ref{eq: newton minimum approx}) the method jumps right to the
vertex of a quadratic function. This is nice if the vertex is a minimum
(which convexity guarantees) but it is not so nice if the vertex is a saddle point
or maximum. Additionally the method becomes unstable (impossible) for small 
(zero) eigenvalues, since then the hesse matrix is hard to numerically invert (no
longer invertible).
On top of these fundamental problems, the storage of the Hesse matrix requires
\(O(\dimension^2)\) space already and its inversion takes \(O(\dimension^3)\)
time which is infeasible for high dimensional data, considering that gradient
descent will do \(O(\dimension^2)\) steps while the Newton-Raphson method does
one inversion. Lastly there are issues obtaining the Hesse matrix in a stochastic
setting.

So the difficulty in creating a second order method lies in addressing all these
concerns at once. One approach by \textcite{martensDeepLearningHessianfree2010},
called ``Hessian Free Optimization'' was reasonably successful and is based on
the ``Conjugate Gradient Method'' \parencite[for an introduction see
e.g.][]{shewchukIntroductionConjugateGradient1994}\fxnote{or own chapter/section?}.
While CG converges in \(\dimension\) steps on quadratic loss functions, there
are no convergence guarantees for general loss functions. Another Quasi-Newton
method was proposed by \textcite{vinyalsKrylovSubspaceDescent2012} which was
adapted into a saddle-point-repelling algorithm by
\textcite{dauphinIdentifyingAttackingSaddle2014}. L-BFGS
\parencite[e.g.][]{haghighiNumericalOptimizationUnderstanding2014} does probably(?)
not repel from saddle points as it attempts to estimate the real hessian.






\section{Complexity Bounds}\label{sec: complexity bounds}

Now that we found convergence rates for gradient descent on (strongly) convex
problems, the question is can we do better? I.e. how fast could an algorithms
possibly be? To tackle this problem let us first make an assumption about what
this algorithm can do. Unrolling our gradient descent algorithm
%
\begin{align*}
	\weights_n = \weights_0 - \lr\sum_{k=0}^{n-1} \nabla \Loss(\weights_k)
\end{align*}
%
we can see, that even if we allowed custom learning rates for every iteration
%
\begin{align*}
	\weights_n = \weights_0 - \sum_{k=0}^{n-1} \lr_k \nabla \Loss(\weights_k)
\end{align*}
%
we would still end up in the linear span of all gradients, shifted by \(\weights_0\).
And since we are in the class of first order optimization methods where we are
only provided with the function evaluation itself and the gradient, an obvious
assumption for a class of optimization methods would be
%
\begin{assumption}[\citeauthor{nesterovLecturesConvexOptimization2018}]
	\label{assmpt: parameter in linear hull of gradients}
	The \(n\)-th iterate of the optimization method is contained in the span of all
	previous gradients shifted by the initial starting point
	\begin{align*}
		\weights_n \in \linSpan\{\nabla \Loss(\weights_k) : 0\le k \le n-1\} + \weights_0
	\end{align*}
\end{assumption}
%
Now the question becomes: How can we utilize this assumption to construct a
function which is difficult to optimize?
\textcite{gohWhyMomentumReally2017} provides an intuitive interpretation for a loss
function taken from \textcite[Section 2.1.2]{nesterovLecturesConvexOptimization2018}
which I further modify into the following example.

\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_coloring_problem.pdf_tex}
	\caption{Even after \(\dimension\) steps (when all tiles have a non-zero value)
	gradient descent is still far off from the solution even using the optimal
	learning rate \(1/\ubound\)}
	\label{fig: visualize coloring problem}
\end{figure}
%
Consider a cold, zero temperature, 1-dimensional rod which is heated from one side
by an infinite heat source at temperature one. Then the second segment of the rod
will only start to get hot once the first segment of it has heated up. Similarly
the \(n\)-th segment will only increase in temperature once the \((n-1)\)-th segment
is no longer at zero temperature. If the heat transfer would only depend on the
current heat level difference, one could thus express a simplified heat level
recursion as follows\footnote{
	Taking first the learning rate to zero and afterwards the space discretization, would
	result in the well known differential equation for heat with boundary
	conditions.
}: 
%
\begin{align*}
	\weights_{n+1}^{(i)}
	&= \weights_{n}^{(i)}
	+ \lr 
	\begin{cases}
		[\weights_{n}^{(i-1)} - \weights_{n}^{(i)}] + [\weights_{n}^{(i+1)} - \weights_{n}^{(i)}]
		&  1 < i < \dimension \\
		[1 - \weights_{n}^{(1)}] + [\weights_{n}^{(2)} - \weights_{n}^{(1)}]
		& i = 1 \\
		[\weights_{n}^{(i-1)} - \weights_{n}^{(i)}]
		& i = \dimension
	\end{cases}
\end{align*}
%
For the loss function
%
\begin{align}\label{eq: naive complexity counterexample loss}
	\tilde{\Loss}(\weights)
	= \frac12 (\weights^{(1)}-1)^2
	+ \frac12 \sum_{k=1}^{\dimension-1} (\weights^{(k)}-\weights^{(k+1)})^2
\end{align}
%
this is just the gradient descent recursion. Now here is the crucial insight:

Since any weight (parameter component) is only affected once its neighbors are
affected, the second weight can not be affected before the second step since
the first weight is only heated in the first step. And since the second weight
will be unaffected until the second step, the third weight will be unaffected
until the third step, etc.

Therefore the \(\dimension - n\) last components will still be zero in
step \(n\), because the linear span of all the gradients so far is still
contained in the subspace \(\reals^n\) of \(\reals^\dimension\). Formalizing
this argument results in the following theorem inspired by \textcite[Theorem
2.1.7]{nesterovLecturesConvexOptimization2018} and \textcite{gohWhyMomentumReally2017}.

\begin{remark}
	This type of loss function is not unlikely to be encountered in real
	problems. E.g. in reinforcement learning with sparse rewards the estimation
	of the value function over states requires the back-propagation of this
	reward through the states that lead up to it. ``Eligibility traces''
	\parencite[Chapter 12]{suttonReinforcementLearningIntroduction2018}
	are an attempt to speed this process up.
\end{remark}
\begin{theorem}\label{thm: convex function complexity bound}
	For any \(\weights_0\in\reals^d\), any \(n\) such that 
	\[0\le n\le \tfrac12 (d-1),\]
	there exists \(\Loss\in\lipGradientSet[\infty,1]{\ubound}\)
	s.t. for any first order method \(\firstOrderMethod\)
	satisfying Assumption~\ref{assmpt: parameter in linear hull of gradients}
	we have
	\begin{subequations}
	\begin{align}
		\Loss(\weights_n) - \Loss(\minimum)
		&\ge \frac{\ubound \|\weights_0 - \minimum\|^2}{16(n+1)^2} \\
		\|\weights_n -\minimum\|^2 
		&\ge \frac12 \|\weights_0 - \minimum\|^2
	\end{align}
	\end{subequations}
	where \(\minimum = \arg\min_\weights \Loss(\weights)\) is the unique minimum.
\end{theorem}
\begin{proof}
	First, since we could always define
	\(\Loss(\weights):=\tilde{\Loss}(\weights-\weights_0)\)
	we can assume without loss of generality that \(\weights_0=\mathbf{0}\). 
	
	Second, we can define \(\tilde{\dimension} := 2n +1 \le \dimension	\)	
	and interpret \(\weights_0 = \mathbf{0} \in\reals^\dimension\) as an element of
	\(\reals^{\tilde{\dimension}}\). If our constructed loss simply
	ignores the dimensions between \(\tilde{\dimension}\) and \(\dimension\) then
	this part of the gradients will then be zero keeping all \(\weights_n\) in
	\(\reals^{\tilde{\dimension}}\). We can therefore assume without loss of
	generality that 
	\begin{align}\label{eq: w.l.o.g dim=2n+1}
		\dimension=2n+1.
	\end{align}

	Third, we want to make sure that our loss function is actually convex. To
	see this notice that we can write the gradient as
	%
	\begin{align}\label{eq: naive complexity example loss gradient}
		\nabla \tilde{\Loss}(\weights)
		= \graphLaplacian\weights + (\weights^{(1)} -1) \stdBasis_1
		= (\graphLaplacian + \stdBasis_1 \stdBasis_1^T)\weights - \stdBasis_1
	\end{align}
	%
	where \(\stdBasis_1=(1,0,\dots,0)^T\) is the first standard basis
	vector and \(\graphLaplacian\) is the ``Graph Laplacian``\footnote{
		It might seem a bit excessive to introduce the concept of graphs when
		we could have simply calculated the derivative by hand to get the same
		result. But this graph view is very suggestive how one would generalize
		this ``heat spread'' problem to more complicated structures than a
		1-dimensional rod. In particular the connectedness of graphs is related
		to its condition number \parencite{gohWhyMomentumReally2017}. And this
		provides some more intuition for how ``worst case problems'' look like.
	} for the 
	\fxnote{tikz graph}{undirected graph} \(\graph=(\vertices, \edges)\) with 
	\begin{align*}
		\vertices = \{1,\dots, \dimension\}
		\qquad
		\edges = \{(i, i+1) : 1\le i \le \dimension-1\}
	\end{align*}
	%
	\begin{align*}
		\graphLaplacian^{(i,j)} 
		&= 
		\begin{cases}
			[\text{degree of vertex i}] & i=j\\
			-1 & (i,j)\in\edges \text{ or } (j,i)\in\edges\\
			0 & \text{ else}
		\end{cases}
		\\
		&=
		\begin{pmatrix*}[r]
			1 & -1 & 0  & \cdots & \cdots & 0 \\
			-1 & 2 & -1 & \ddots &  &  \vdots \\ 
			0 & -1 & 2 & \ddots & \ddots & \vdots \\
			\vdots & \ddots & \ddots & \ddots & -1 & 0 \\
			\vdots &  & \ddots & -1 & 2 & -1 \\
			0 & \cdots & \cdots & 0  & -1 & 1
		\end{pmatrix*}
	\end{align*}
	%
	This means our loss function is quadratic with
	\begin{align*}
		\nabla^2\tilde{\Loss}(\weights) = H = (A_G + \stdBasis_1 \stdBasis_1^T)
	\end{align*}
	And the Hesse matrix is positive definite because
	%
	\begin{align*}
		\langle \weights , H \weights\rangle
		&= \langle \weights, (A_G + \stdBasis_1 \stdBasis_1^T) \weights \rangle
		= \langle \weights, \stdBasis_1\rangle^2
		+ \langle \weights, A_G \weights\rangle
		\\
		&= (\weights^{(1)})^2
		+ \sum_{k=1}^{\dimension-1}(\weights^{(k)}-\weights^{(k+1)})^2
		\ge 0.
	\end{align*}
	%
	The last equality can be verified by taking the derivative of both and
	realizing they are the same, which is sufficient because they are both zero	
	in the origin. We simply used the fact that our original loss function
	was essentially already a quadratic function except for the constant influx
	at the first tile. And that constant in the derivative has no bearing on the
	hesse matrix.


	To calculate the operator norm of \(H\) we can use the equality above and	
	\((a-b)^2 \le 2(a^2 + b^2)\) to get
	\begin{align*}
		\langle \weights, H\weights\rangle
		\le (\weights^{(1)})^2 + 2\sum_{k=1}^{d-1}(\weights^{(k)})^2 + (\weights^{(k+1)})^2 
		\le 4 \sum_{k=1}^{d-1} (\weights^{(k)})^2
		= 4 \|\weights\|^2
	\end{align*}
	Which immediately implies that the largest eigenvalue is smaller than 4. To
	obtain a convex function with a Lipschitz continuos gradient with constant
	\(\ubound\), we can now simply define
	\begin{align*}
		\Loss(\weights):= \frac{\ubound}{4}\tilde{\Loss}(\weights)
		= \frac\ubound{8}\left[
			(\weights^{(1)}-1)^2
			+ \sum_{k=1}^{\dimension-1} (\weights^{(k)}-\weights^{(k+1)})^2
		\right].
	\end{align*}
	Further, we know that \(\minimum = (1,\dots,1)^T\) achieves a loss of zero which is
	the unique minimum as the loss function is positive and convex. Since we
	assumed w.l.o.g. that \(\weights_0=\mathbf{0}\) we get
	%
	\begin{align}\label{eq: initial distance}
		\|\minimum - \weights_0\|^2 = \dimension.
	\end{align}
	%
	We also know
	that Assumption~\ref{assmpt: parameter in linear hull of gradients} implies
	%
	\begin{align*}
		\weights_n \in \linSpan\{\nabla\Loss(\weights_k): 0 \le k \le n-1\}
		\subseteq \reals^n \subseteq \reals^\dimension
	\end{align*}
	%
	which immediately results in the second claim of the theorem
	\begin{align*}
		\|\minimum - \weights_n\|^2
		&\ge \sum_{k=n+1}^d \underbrace{(\minimum^{(k)}- \weights_n^{(k)})^2}_{=1}
		= d-n \\
		&\lxeq{(\ref{eq: w.l.o.g dim=2n+1})} n+1
		\xeq{(\ref{eq: w.l.o.g dim=2n+1})} \tfrac{n+1}{2n+1} d
		\xge{(\ref{eq: initial distance})} \tfrac12 \|\minimum - \weights_0\|^2. 
	\end{align*}

	To get the bound on the loss function we have to be a bit more subtle.
	On \(\reals^n\subseteq \reals^\dimension\) the modified loss function
	%
	\begin{align}\label{eq: sink loss}
		\Loss_n(\weights) := \frac\ubound{8}\left[
			(\weights^{(1)} -1)^2
			+ (\weights^{(n)} - 0)^2
			+ \sum_{k=1}^{n-1} (\weights^{(k)} - \weights^{(k+1)})^2
		\right]
	\end{align}
	%
	is equal to \(\Loss(\weights)\) because \(\weights^{(i)}=0\) for \(i>n\).
	Therefore
	%
	\begin{align*}
		\Loss(\weights_n) - \inf_{\weights}\Loss(\weights)
		\ge \Loss\restriction_{\reals^n}(\weights_n) = \Loss_n(\weights_n)
		\ge \inf_{\weights}\Loss_n(\weights).
	\end{align*}
	%
	Having a closer look at \(\Loss_n\) will therefore give us a lower bound
	without having to know anything about \(\firstOrderMethod\). Since \(\Loss_n\)
	is similarly convex, setting its derivative equal to zero will actually provide us
	with its minimum. And similarly to \(\Loss\) itself, \(\Loss_n\) is a
	quadratic function, so the gradient is just an affine function with a
	constant hesse matrix. Therefore finding the minimum only requires solving
	a linear equation. One can just go through the same motions as we did
	before to obtain the equation:
	%
	\begin{align}\label{eq: optimality condition for sink loss}
		A_n \weights - \stdBasis_1 \xeq{!} 0 \qquad 
		A_n =\begin{cases}
			2 & i=j\le n \\
			-1 & i=j+1\le n \text{ or } j=i+1\le n \\
			0 & \text{else}
		\end{cases} 
	\end{align}
	%
	But at least for solving it, it helps to have an intuition what \(\Loss_n\)
	actually represents. Recall \(\Loss\) represents just
	a single heat source at interval \(\weights^{(1)}\) at constant
	temperature one which slowly heats up our rod to this level. Now
	\(\Loss_n\) not only has a source, it also has a sink with constant
	temperature zero at \(n\) which cools down the rod from this other side.

	It is therefore quite intuitive that the equilibrium solution
	\(\hat{\weights}_n\) (optimal solution for \(\Loss_n\)) should be a linearly
	decreasing slope
	%
	\begin{align*}
		\hat{\weights}_n^{(i)} = \begin{cases}
			1 - \tfrac{i}{n+1} & i \le n+1\\
			0	& i \ge n+1
		\end{cases}
	\end{align*}	
	%
	And this is in fact the solution of our linear equation (\ref{eq: optimality
	condition for sink loss}) as can be verified by calculation. Since
	\(\weights_n \in \reals^n\) on which \(\Loss\) and \(\Loss_n\) are equal
	we finally get
	%
	\begin{align*}
		\Loss(\weights_n)-\inf_\weights\Loss(\weights)
		&=\Loss(\weights_n) = \Loss_n(\weights_n) \ge \Loss_n(\hat{\weights}_n)
		= \Loss(\hat{\weights}_n)\\
		&= \frac{\ubound}{8}\left[
			\left(-\tfrac1{n+1}\right)^2 + \left(1-\tfrac{n}{n+1}\right)^2
			+ \sum_{k=1}^{n-1}\left(\tfrac{k+1}{n+1}-\tfrac{k}{n+1}\right)^2
		\right]\\
		&= \frac{\ubound}{8}\sum_{k=0}^n \tfrac{1}{(n+1)^2}
		=\frac{\ubound}{8(n+1)}
		\xeq{(\ref{eq: initial distance})} \frac{L\|\weights_0 - \minimum\|^2}{8(n+1)d}\\
		&\lxge{(\ref{eq: w.l.o.g dim=2n+1})}
		\frac{L\|\weights_0 - \minimum\|^2}{16(n+1)^2}
		\qedhere
	\end{align*}
\end{proof}


\subsection{Complexity Bounds for Strongly Convex Losses}

For a strongly convex loss we simply add a regularization term:
%
\begin{align*}
	\Loss(\weights)
	&= \frac{\ubound -\lbound}{8} \left[
		(\weights^{(1)}-1)^2
		+ \sum_{k=1}^{\dimension-1} (\weights^{(k)}-\weights^{(k+1)})^2
	\right]
	+ \frac\lbound{2} \| \weights \|^2\\
	&= \frac{\ubound - \lbound}{4} \tilde{\Loss}(\weights)
	+ \frac{\lbound}{2}\| \weights \|^2
\end{align*}
%
From the proof of Theorem~\ref{thm: convex function complexity bound} we know
about \(\tilde{\Loss}\) that
%
\begin{align*}
	0 \precsim \nabla^2\tilde{\Loss} \precsim 4\identity.
\end{align*}
%
Therefore we know that \(\Loss\in\strongConvex[\infty,1]{\lbound}{\ubound}\) because
%
\begin{align*}
	\lbound\identity
	\precsim \nabla^2\Loss
	&= \frac{\ubound - \lbound}{4} \nabla^2 \tilde{\Loss} + \lbound\identity\\
	&\precsim (\ubound - \lbound + \lbound)\identity = \ubound \identity.
\end{align*}
%
Using \(\lbound(\condition -1)=\ubound-\lbound\) and (\ref{eq: naive complexity
example loss gradient}) we can set the gradient to zero
\begin{align*}
	0 &\lxeq{!} \nabla \Loss(\weights)
	=  \frac{\lbound(\condition -1)}{4} \nabla\tilde{\Loss}(\weights) - \lbound \weights\\
	&= \left[
		\frac{\lbound(\condition-1)}{4}(A_G + \stdBasis_1\stdBasis_1^T) - \lbound\identity
	\right]\weights - \frac{\lbound(\condition-1)}{4}\stdBasis_1.
\end{align*}
%
Assuming \(\condition>1\) (\(\condition=1\) implies \(\Loss=\lbound\|\cdot\|^2\) and
\(\minimum=\mathbf{0}\)),
this condition can be rewritten as
%
\begin{align*}
	0 =  \left[
		A_G + \stdBasis_1\stdBasis_1^T + \frac{4}{\condition-1}\identity
	\right] - \stdBasis_1
	\weights.
\end{align*}
%
All entries on the diagonal except for the last dimension (which only has one
connection and no source or sink) are equal to
%
\begin{align*}
	2+\frac{4}{\condition-1} = 2\frac{\condition +1}{\condition-1}
\end{align*}
%
This results in the system of equations
%
\begin{subequations}
\label{eq: solution to the strongly convex coloring problem}
\begin{align*}
	0&=2\frac{\condition+1}{\condition -1}\weights^{(1)} - \weights^{(2)} -1 \\
	0&=2\frac{\condition+1}{\condition -1}\weights^{(i)}
	- \weights^{(i+1)} - \weights^{(i-1)}  && 2\le i <d \\
	0&= \left(2\frac{\condition+1}{\condition -1} -1 \right)\weights^{(d)}
	- \weights^{(d-1)}
\end{align*}
\end{subequations}
Defining \(\weights^{(0)} =1\) we can unify the first two equations (flipping
the sign)
\begin{align*}
	0&= 
	\frac{\weights^{(i+1)}}{\weights^{(i)}}\frac{\weights^{(i)}}{\weights^{(i-1)}}
	- 2\frac{\condition+1}{\condition -1}\frac{\weights^{(i)}}{\weights^{(i-1)}}
	+ 1  && i <d
\end{align*}
%
Which means that for a solution \(\) of the quadratic equation
\begin{align}\label{eq: coloring solution quadratic equation}
	0&= x^2 - 2\frac{\condition+1}{\condition -1}x + 1,
\end{align}
%
\(\weights^{(i)} = x^i\) will be a solution to the first \(d-1\) equations.
Unfortunately neither of the solutions
%
\begin{align*}
	x_{1/2} &= \frac{\condition+1}{\condition -1} \pm 
	\sqrt{\left(\tfrac{\condition+1}{\condition-1}\right)^2 -1}
	=\frac{\condition +1 \pm \sqrt{4\condition}}{\condition -1}\\
	&= \frac{(\sqrt{\condition}\pm 1)^2}{(\sqrt{\condition}-1)(\sqrt{\condition}+1)}
\end{align*}
%
is a solution to the last equation. Which is why we are going to formulate the
following theorem only for \(\dimension=\infty\)\footnote{
	For large dimensions the real solution seems to be almost
	indistinguishable from the limiting case (e.g. \(\dimension=40,
	\condition=10\) results in a maximal error of the order \(10^{-11}\). The
	error in the first 35 dimensions is even of the order \(10^{-13}\))
}.
We formalize \(\dimension=\infty\) as the sequence space with euclidean norm
\begin{align*}
	\sequenceSpace := \Big\{
		f:\naturals \to \reals : \| f\|^2 = \sum_{k\in\naturals} f(k)^2< \infty
	\Big\}.
\end{align*}
%
\begin{theorem}[{\cite[Theorem 2.1.13]{nesterovLecturesConvexOptimization2018}}]
	\label{thm: strong convexity complexity bound}
	For any \(\weights_0\in\sequenceSpace\) there exists
	\(\Loss\in\strongConvex[\infty,1]{\lbound}{\ubound}\) such that for any
	first order method \(\firstOrderMethod\) satisfying Assumption~\ref{assmpt:
	parameter in linear hull of gradients}, we have
	\begin{subequations}
	\begin{align}
		\|\weights_n - \minimum\|
		&\ge \left(1-\frac2{1+\condition}\right)^n \|\weights_0 - \minimum\| \\
		\Loss(\weights_n) - \Loss(\minimum)
		&\ge \tfrac{\lbound}{2}
		\left(1-\frac2{1+\condition}\right)^{2n} \|\weights_0 - \minimum\|^2
	\end{align}
	\end{subequations}
	where \(\minimum\) is the unique minimum of \(\Loss\).
\end{theorem}
\begin{proof}
	Since our factor becomes zero for \(\condition=1\) we can assume without loss
	of generality that \(\condition>1\).
	
	Further, we can again assume without loss of generality that
	\(\weights_0=\mathbf{0}\).  Of the two solutions to the linear equations
	(\ref{eq: solution to the strongly convex coloring problem}) induced by the
	two solutions \(q_{1/2}\) of (\ref{eq: coloring solution quadratic equation})
	only
	\begin{align*}
		\minimum^{(i)} = \left(\frac{\sqrt{\condition}-1}{\sqrt{\condition}+1}\right)^i
		= \left(1 - \frac{2}{1+\sqrt{\condition}}\right)^i =: q^i
	\end{align*}
	is an element of \(\sequenceSpace\) and therefore the unique minimum. Since 
	\(\weights_n\) stays in the subspace \(\reals^n\) by assumption we know
	\begin{align*}
		\|\weights_n -\minimum\|^2
		\ge \sum_{k=n+1}^\infty (\minimum^{(k)})^2
		= q^{2n} \sum_{k=0}^\infty q^{2k}
		= q^{2n} \| \minimum\|^2
		= q^{2n} \|\weights_0 - \minimum\|^2.
	\end{align*}
	Taking the root results in the first claim. The second claim immediately
	follows from the definition of strong convexity (Def~\ref{def: strong convexity})
	and \(\nabla\Loss(\minimum) = 0\):
	\begin{align*}
		\Loss(\weights_n) - \Loss(\minimum)
		&\ge \langle \nabla\Loss(\minimum), \weights_n -\minimum\rangle
		+\tfrac\lbound{2} \|\weights_n -\minimum\|^2
		\qedhere
	\end{align*}
\end{proof}

In view of Lemma~\ref{lem: smallest upper bound} which states that gradient
descent is in some sense optimal (minimizes the upper bound induced by
Lipschitz continuity of the gradient), it might be surprising how far away
Gradient Descent is from these lower bounds. Maybe these lower bounds are not
tight enough? This is not the case. In Chapter~\ref{chap: momentum} we will get
to know a family of methods which do achieve these rates of convergence.

So can we
explain the Lemma? Well, first of all it is of course just an upper bound we
are minimizing. But the more important issue is that we are taking a very
local perspective. We only care about minimizing the next loss, using 
nothing but the current gradient. In Assumption~\ref{assmpt: parameter in
linear hull of gradients} on the other hand, we consider the linear span of all
gradients so far. This distinction is important as it will make the
``momentum method'' discussed in Chapter~\ref{chap: momentum} seem much more obvious.

Lastly since this generalization of assumptions improved our rate of convergence
we might ask if we could generalize our assumption further. Heuristics like
the well known Adagrad, Adadelta and RMSProp use different learning rates for
different components of the gradients for example. Their iterates can thus
be expressed by
%
\begin{align*}
	\weights_n = \weights_0 + \sum_{k=0}^{n-1} H_k \nabla\Loss(\weights_k)
\end{align*}
%
where \(H_k\) are diagonal matrices. But at least upper triangle matrices do
not change the fact that \(\weights_n\) is contained in subspace \(\reals^n\).
Since we only use this fact, one could thus generalize Assumption~\ref{assmpt:
parameter in linear hull of gradients} to
%
\begin{assumption}\label{assmpt: parameter in generalized linear hull of gradients}
	The \(n\)-th iterate of the optimization method can be expressed as
	\begin{align*}
		\weights_n = \weights_0 + \sum_{k=0}^{n-1} H_k \nabla\Loss(\weights_k)
	\end{align*}
	where \(H_k\) are upper triangular matrices.
\end{assumption}

Both Complexity Bounds continue to hold with this Assumption in place of
Assumption~\ref{assmpt: parameter in linear hull of gradients}. And since one
could just switch the relation \((\weights_n-\weights_{n+1})^2\) to
\((\weights_n+\weights_{n+1})^2\) to force a flip from positive to negative
at this point, it is unclear how one should extrapolate to components not
available in any gradient. But making further assumptions like continuity of
the underlying function \(\model\) inducing \(\Loss(\weights)\) it might
be possible to find better bounds.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput