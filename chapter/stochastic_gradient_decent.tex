% !TEX root = ../Masterthesis.tex

\chapter{Stochastic Gradient Decent (SGD)}

Until now we have always assumed we had access to the theoretical loss \(\Loss\) 
directly. In reality we almost never have. What we can do instead is use the
stochastic gradient
%
\begin{align*}
	\nabla_\weights\loss (\weights, X,Y) \qquad (X,Y)\sim\dist.
\end{align*}
%
Its expected value is the theoretical loss again\footnote{
	We need to be able to swap the expectation with differentiation. A sufficient 
	condition is continuity of \(\nabla\loss\) in \(\weights\), which allows us
	to use the mean value theorem
	\begin{align*}
		\frac{\partial}{\partial \weights_k}\E[\loss(\weights, X,Y)]
		&= \lim_{n\to\infty}
		\int\frac{\loss(\weights+\stdBasis_k/n, X,Y)-\loss(\weights,X,Y)}{1/n}d\Pr
		\\
		&=\lim_{n\to\infty} \int\frac{\partial}{\partial \weights_k}\loss(\xi_n, X,Y)d\Pr
		\qquad \xi_n \in [\weights, \weights + \stdBasis_k/n].
	\end{align*}
	Then we use the boundedness of a continuous function on the compact interval
	\([\weights, \weights + \stdBasis_k/n]\) to move the limit in using
	dominated convergence.
}. To shorten notation we will assume that the gradient is always meant
with respect to \(\weights\) unless otherwise stated.
Sampling an independent sequence \(((X_k,Y_k), k\ge 1)\) from the 
distribution \(\dist\) results in \emph{Stochastic Gradient Decent}
%
\begin{align*}
	\weights_n
	&= \weights_{n-1} - \lr\nabla\loss(\weights_{n-1}, X_n,Y_n)\\
	&= \weights_{n-1} - \lr\nabla\Loss(\weights_{n-1})
	+ \lr\underbrace{
		[\nabla\Loss(\weights_{n-1}) - \nabla\loss(\weights_{n-1}, X_n, Y_n)]
	}_{=:\martIncr_n}
\end{align*}
Where
\begin{align*}
	\martingale_n := \martingale_{n-1} + \martIncr_n \qquad \martingale_0:=0
\end{align*}
is a martingale for the filtration
\begin{align*}
	\filtration_n :=\sigma((X_k, Y_k): k\le n )
	\qquad (X_k,Y_k)\stackrel{\text{iid}}{\sim}\dist,
\end{align*}
since \(\weights_n\) is \(\filtration_n\)-measurable and thus
\begin{align*}
	\E[\martingale_{n+1}\mid \filtration_n]
	= \underbrace{\E[\nabla\Loss(\weights_n) - \nabla\loss(\weights_n, X_{n+1}, Y_{n+1})\mid \filtration_n]}_{
		=0 \quad (X_{n+1},Y_{n+1}) \indep \filtration_n
	} + \martingale_n
	= \martingale_n.
\end{align*}
%
So if we view our recursion as an ODE discretization again
%
\begin{align*}
	\weights^{(\lr)}(t) &:= \weights_{\lfloor t/\lr\rfloor}
	= \weights_0 - \lr\sum_{k=0}^{\lfloor t/\lr\rfloor-1}\nabla\Loss(\weights_k)
	+ \lr\sum_{k=1}^{\lfloor t/\lr\rfloor}\martIncr_k
\end{align*}
%
then for \(\lr_n := t/n\) we have
\begin{align*}
	\weights^{(\lr_n)}(t) &:= \weights_n
	= \weights_0 - \lr_n\sum_{k=0}^{n-1}\nabla\Loss(\weights_k)
	+\frac{t}{n}\martingale_n
\end{align*}
Now if we have some form of law of large numbers for martingales we get
\(t\martingale_n/n\to 0\) for \(n\to\infty\) which means that in the limit we
get our deterministic integral equation back back
\begin{align*}
	\lim_{n\to\infty}\weights^{(\lr_n)}(t) = \weights(t)
	= \weights_0 - \int_0^t \nabla\Loss(\weights(s))ds
\end{align*}

\textcite{nemirovskiRobustStochasticApproximation2009}
\textcite{simsekliTailIndexAnalysisStochastic2019}

\section{Batch Learning}

cf. \cite{hardtTrainFasterGeneralize2016} \cite{hofferTrainLongerGeneralize2018}

\section{Averaging}

cf. \cite{bachNonstronglyconvexSmoothStochastic2013}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
