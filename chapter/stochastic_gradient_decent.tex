% !TEX root = ../Masterthesis.tex

\chapter{Stochastic Gradient Decent (SGD)}

Until now we have always assumed we had access to the theoretical loss \(\Loss\) 
directly. In reality we almost never have. What we can do instead is use the
stochastic gradient
%
\begin{align*}
	\nabla_\weights\loss (\weights, X,Y) \qquad (X,Y)\sim\dist.
\end{align*}
%
Its expected value is the theoretical loss again\footnote{
	We need to be able to swap the expectation with differentiation. A sufficient 
	condition is continuity of \(\nabla\loss\) in \(\weights\), which allows us
	to use the mean value theorem
	\begin{align*}
		\frac{\partial}{\partial \weights_k}\E[\loss(\weights, X,Y)]
		&= \lim_{n\to\infty}
		\int\frac{\loss(\weights+\stdBasis_k/n, X,Y)-\loss(\weights,X,Y)}{1/n}d\Pr
		\\
		&=\lim_{n\to\infty} \int\frac{\partial}{\partial \weights_k}\loss(\xi_n, X,Y)d\Pr
		\qquad \xi_n \in [\weights, \weights + \stdBasis_k/n].
	\end{align*}
	Then we use the boundedness of a continuous function on the compact interval
	\([\weights, \weights + \stdBasis_k/n]\) to move the limit in using
	dominated convergence.
}. To shorten notation we will assume that the gradient is always meant
with respect to \(\weights\) unless otherwise stated.
Sampling an independent sequence \(((X_k,Y_k), k\ge 1)\) from the 
distribution \(\dist\) results in \emph{Stochastic Gradient Decent}
%
\begin{align*}
	\Weights_{n+1}
	&= \Weights_n - \lr_n\nabla\loss(\Weights_n, X_{n+1},Y_{n+1})\\
	&= \Weights_n - \lr_n\nabla\Loss(\Weights_n)
	+ \lr_n\underbrace{
		[\nabla\Loss(\Weights_n) - \nabla\loss(\Weights_n, X_{n+1}, Y_{n+1})]
	}_{=:\martIncr_{n+1}}
\end{align*}
Where \(\martIncr_n\) are martingale increments for the filtration
% \begin{align*}
% 	\martingale_n := \martingale_{n-1} + \martIncr_n \qquad \martingale_0:=0
% \end{align*}
\begin{align*}
	\filtration_n :=\sigma((X_k, Y_k): k\le n )
	\qquad (X_k,Y_k)\stackrel{\text{iid}}{\sim}\dist,
\end{align*}
since \(\Weights_n\) is \(\filtration_n\)-measurable which is independent of, i.e.
\((X_{n+1},Y_{n+1})\)
\begin{align}\label{eq: conditional independence of martingale increments}
	\E[\martIncr_{n+1}\mid \filtration_n]
	= \E[\nabla\Loss(\Weights_n) - \nabla\loss(\Weights_n, X_{n+1}, Y_{n+1})\mid \filtration_n]
	= 0.
\end{align}
%
\section{ODE View}

The first way to view SGD is through the lense of approximating the ODE with
integral equation
\begin{align*}
	\weights(t) = \phi^{t-t_0}\weights_0 =  \weights_0 -\int_{t_0}^{t}\nabla\Loss(\weights(s))ds.
\end{align*}
Its Euler discretization is gradient decent 
\begin{align*}
	\weights_{t_{n+1}}:=\weights_{n+1}
	= \psi^{\lr_n} \weights_n = \weights_n - \lr_n \nabla\Loss(\weights_n),
\end{align*}
where
\begin{align*}
	t_0 \le \dots \le t_N= T \quad \text{with} \quad t_{n+1}-t_n=\lr_n
	\quad \text{and}\quad \lr:=\max_n \lr_n
\end{align*}
and constant learning rate \(\lr\) represents an equidistant discretization.
Since \(\nabla\Loss\) is Lipschitz continuous, the local discretization error
\begin{align*}
	\|\phi^\lr \weights  - \psi^\lr \weights \|
	&= \|(\weights - \int_0^\lr\nabla\Loss(\phi^s \weights)ds) - (\weights-\lr\nabla\Loss(w))\|\\
	&\le  \int_0^\lr \underbrace{\|\nabla\Loss(w) - \nabla\Loss(\phi^s\weights)\|}_{
		\le \ubound \|\phi^s \weights - \weights\| \in O(s) \mathrlap{\quad\text{(Taylor approx., Lip. \(\nabla\Loss\))}}
	}ds 
\end{align*}
is of order \(O(\lr^2)\). 
Using the discrete Gr\"onwall's inequality and stability of the ODE (due to Lipschitz continuity of
\(\nabla\Loss\) and the continuous Gr\"onwall inequality) the global discretization
error is therefore of order 1, i.e.
\begin{align*}
	\max_{n} \|\weights(t_n) - \weights_n\| \in O(\lr),
\end{align*}
\fxnote{appendix?}{as is usually covered in ``Numerics of ODE's'' courses.}
Due to \((a+b)^2\le2(a^2+b^2)\), and therefore 
\begin{align*}
	\max_n\E[\|\weights(t_n)-\Weights_n\|^2]
	\le 2(\underbrace{\max_n \|\weights(t_n)-\weights_n\|^2}_{O(\lr^2)}
	+\max_{n}\E[\|\weights_n - W_n\|^2]),
\end{align*}
it is enough to bound the distance between SGD and GD.
\begin{theorem}\label{thm: distance SGD vs GD}
	In general we have
	\begin{align*}
		\E\|\weights(t_n)-\Weights_n\|^2
		\le \sum_{k=0}^{n-1}\lr_k^2\E\|\martIncr_{k+1}\|^2\exp\left(
			\sum_{j=k+1}^{n-1}\lr_j^2\ubound^2 + 2\lr_j\ubound
		\right)
	\end{align*}
	which can be simplified for constant learning rates \(\lr_n=\lr\) and bounded
	martingale increment variances \(\E\|\martIncr_n\|^2 \le K\) to
	\begin{align*}
		\max_{n}\E\|\weights(t_n)	-\Weights_n\|^2
		\le \lr TK\exp[T(\lr\ubound^2 + 2\ubound)] \in O(\lr)
	\end{align*}
\end{theorem}
Now before we get to the proof let us build some intuition why this result
is not surprising. Unrolling SGD we can rewrite it as
\begin{align*}
	\Weights_{t_n}
	= \weights_0 - \sum_{k=0}^{n-1} \lr_k \nabla\Loss(\Weights_k)
	+ \sum_{k=0}^{n-1} \lr_k\martIncr_{k+1}.
\end{align*}
Which is the same recursion as in GD except for the last term. Now for an
equidistance grid we have \(\lr=T/N\). Intuitively the last term in \(\Weights_T\)
should therefore disappear due to some law of large numbers for martingales
\begin{align*}
	\sum_{k=0}^{N-1}\lr_k\martIncr_{k+1} = \frac{T}{N}\sum_{k=1}^N\martIncr_k.
\end{align*}
And since the variance of a sum of martingale increments is the sum of variances
as mixed terms disappear due to conditional independence (\ref{eq: conditional
independence of martingale increments})
\begin{align*}
	\E\left\|\sum_{k=1}^N\martIncr_k\right\|^2 = \sum_{k=1}^N \E\|\martIncr_k\|^2,
\end{align*}
the variance of a mean decreases with rate \(O(1/N)=O(\lr)\), which is the rate
we get in Theorem~\ref{thm: distance SGD vs GD}.
\begin{proof}[Proof (Theorem~\ref{thm: distance SGD vs GD})]
	Using the conditional independence (\ref{eq: conditional independence of martingale increments})
	we can get rid of a all the mixed terms with
	\(\E[\langle \cdot, \martIncr_{n+1}\rangle\mid\filtration_n]=0\) as everything
	else is \(\filtration_n\) measurable:
	\begin{align*}
		&\E\|\Weights_{n+1}-\weights_{n+1}\|^2
		= \E\|\Weights_n - \weights_n
		+ \lr_n(\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n))
		+ \lr_n\martIncr_{n+1}\|^2\\
		&\lxeq{(\ref{eq: conditional independence of martingale increments})}
		\begin{aligned}[t]
			&\E\|\Weights_n-\weights_n\|^2 + \lr_n^2\E\|\martIncr_{n+1}\|^2\\
			&+\underbrace{
				\lr_n^2\E\|\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\|^2
				+ 2\lr_n\E\langle\Weights_n-\weights_n,
				\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\rangle
			}_{
				\le 0 \qquad
				\text{for convex \(\Loss\) (Lemma~\ref{lem: bermanDiv lower bound}) and }
				\lr_n<2/\ubound
			}
		\end{aligned}
	\end{align*}
	But since we do not want to demand convexity of \(\Loss\) just yet, we will
	have to get rid of these terms in a less elegant fashion using Lipschitz
	continuity and the Cauchy-Schwarz inequality
	\begin{align*}
		\|\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\|^2
		&\le \ubound^2\|\weights_n-\Weights_n\|^2\\
		\langle\Weights_n-\weights_n,
		\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\rangle
		&\le \ubound \|\weights_n-\Weights_n\|^2
	\end{align*}
	which leads us to
	\begin{align*}
		\E\|\Weights_{n+1}-\weights_{n+1}\|^2
		\le (1+\lr_n^2\ubound^2 + 2\lr_n\ubound)\E\|\Weights_n-\weights_n\|^2
		+ \lr_n^2\E\|\martIncr_{n+1}\|^2.
	\end{align*}
	Now we apply the discrete Gr\"onwall inequality\fxnote{appendix} to get
	\begin{align*}
		\E\|\Weights_n-\weights_n\|^2
		\le \sum_{k=0}^{n-1}\lr_k^2\E\|\martIncr_k\|^2
		\underbrace{
			\prod_{j=k+1}^{n-1}(1+\lr_j^2\ubound^2 + 2\lr_j\ubound)
		}_{
			\le \exp\left(\sum_{j=k+1}^{n-1}\lr_j^2\ubound^2 + 2\lr_j\ubound\right)
		}
	\end{align*}
	where we have used \(1+x\le\exp(x)\) to get our first claim.
\end{proof}

\subsection{Batch Learning}

cf. \cite{hardtTrainFasterGeneralize2016} \cite{hofferTrainLongerGeneralize2018}

\section{Expected Square Error Analysis}

\textcite{nemirovskiRobustStochasticApproximation2009}

\subsection{Averaging}

cf. \cite{bachNonstronglyconvexSmoothStochastic2013}

\section{SDE View}

\textcite{simsekliTailIndexAnalysisStochastic2019}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
