% !TEX root = ../Masterthesis.tex

\newcommand{\identity}{\mathbb{I}}
\newcommand{\lbound}{\mu_l}
\newcommand{\ubound}{\mu_u}
\newcommand{\diag}{\text{diag}}
\newcommand{\rate}{\text{rate}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\firstOrderMethod}{\mathcal{M}}
\newcommand{\linSpan}{\text{span}}
\newcommand{\dimension}{d}

\chapter{Gradient Decent (GD)}

A step up from zero order methods (such as grid search) are first order methods
using the first derivative (the gradient). This requires a "first order oracle"
which can provide us with \(L(\theta)\) and \(\nabla L(\theta)\) at any point
\(\theta\).

One immediately obvious way to utilize this information is to
incrementally move in the direction of steepest decent, i.e.
%
\begin{align*}
	\theta_{n+1} = \theta_n - \eta\nabla L(\theta_n).
\end{align*}
%
A useful way to look at this equation is to notice that it is the
discretization of an ordinary differential equation (ODE)
%
\begin{align*}
	\dot{\theta}_n \approx \frac{\theta_{n+1} - \theta_n}{\eta}
	= - \nabla L(\theta_n).
\end{align*}
%
Here the "learning rate" \(\eta\) is the time delta between discretizations
\(t_n\) and \(t_{n+1}\). This of course implies \(t_n = n\eta\). And for
\(\eta\to\infty\), we arrive at the ODE
%
\begin{align}\label{eq: velocity is gradient}
	\dot{\theta}(t) = -\nabla L(\theta(t)).
\end{align}
%
If you are familiar with Lyapunov functions you will not be surprised by the next
argument:
%
\begin{align}\label{eq: gradient integral}
	L(\theta(t_1)) - L(\theta(t_0))
	&= \int_{t_0}^{t_1} \nabla L(\theta(s)) \cdot \dot{\theta}(s) ds
	= \int_{t_0}^{t_1} -\|\nabla L(\theta(s))\|^2 ds
	\le 0
\end{align}
%
immediately implies
\begin{align*}
	L(\theta(t_0)) \ge L(\theta(t_1)) \ge \dots \ge \inf_\theta L(\theta) \ge 0
\end{align*}
which implies convergence. But not necessarily a convergent \(\theta(t)\) and
not necessarily convergence to \(\inf_\theta L(\theta)\).

\section{Visualizing the 2nd Taylor Approximation}\label{sec: visualize gd}

To build some intuition what leads to convergence of \(\theta(t)\) let us
consider more simple cases. If we assume our Hesse matrix \(\nabla^2
L(\theta)\) exists and is positive definite (all eigenvalues positive), then for
the second order Taylor approximation
%
\begin{align*}
	L(\theta+v) \approx T_2L(\theta+v)
	= L(\theta) + v^T \nabla L(\theta) + \tfrac12 v^T \nabla^2 L(\theta) v
\end{align*}
%
we can find the minimum 
\begin{align*}
	\hat{v} = -(\nabla^2 L(\theta))^{-1}\nabla L(\theta)
\end{align*}
by setting the first derivative to zero
%
\begin{align*}
	\nabla T_2L(\theta+v) = \nabla L(\theta) + \nabla^2L(\theta) v \xeq{!} 0.
\end{align*}
%
This minimum allows us not only to rewrite the taylor derivative
%
\begin{align*}
	\nabla T_2L(\theta+v) = \nabla^2 L(\theta)(v-\hat{v}),
\end{align*}
%
but also the original taylor approximation
%
\begin{align*}
	T_2L(\theta+v)
	&= L(\theta) - v^T \nabla^2 L(\theta) \hat{v} + \tfrac12 v^T \nabla^2 L(\theta) v \\
	&= \underbrace{L(\theta) - \tfrac12 \hat{v} \nabla^2 L(\theta) \hat{v}}_{=: c(\theta) \text{ (const.)}}
	+ \tfrac12 (v-\hat{v})^T \nabla^2 L(\theta)(v-\hat{v}).
\end{align*}
%
To get absolute instead of relative coordinates to \(\theta\) we set
%
\begin{align*}
	\hat{\theta} := \theta + \hat{v} = \theta -(\nabla^2 L(\theta))^{-1}\nabla L(\theta),
\end{align*}
%
and obtain the notion of a paraboloid centered around \(\hat{\theta}\)
%
\begin{align}\label{paraboloid approximation of L}
	L(y) = \tfrac12 (y- \hat{\theta}) \nabla^2 L(\theta) (y-\hat{\theta}) + c(\theta) + o(\|y-\theta\|^2)
\end{align}
%
\begin{wrapfigure}{O}{0.65\textwidth}
	\centering
	\def\svgwidth{0.65\textwidth}
	\input{media/contour.pdf_tex}
	\caption{Assuming \(\hat{\theta}=0\), \(\lambda_1=1, \lambda_2=2\), \(v_1=(\sin(1), \cos(1))\)}
	\label{fig: 2d paraboloid}
\end{wrapfigure}
%
To fully appreciate Figure~\ref{fig: 2d paraboloid}, we now only need to realize
that the diagonizability of \(\nabla^2 L(\theta)\)
%
\begin{align}\label{eq: diagnalization of the Hesse matrix}
	V \nabla^2 L(\theta) V^T
	= \diag(\lambda_1,\dots,\lambda_d), \qquad V=(v_1,\dots, v_d)
\end{align}
%
implies that once we have selected the center \(\hat{\theta}\) and the direction of
one eigenspace in two dimensions, the other eigenspace has to be
orthogonal to the first one which fully determines its direction at this point. 

Before we move on I want to briefly mention that we only really needed the
positive definiteness of \(\nabla^2 L(\theta)\) to make it invertible, so that
\(\hat{v}\) is well defined. If it was not positive definite but still invertible,
then \(\hat{v}\) would not be a minimum but all the other arguments would still
hold.
In that case the eigenvalues might be negative as well as positive, which would
represent a saddle point, or all negative which would represent a maximum.

Using the representation (\ref{paraboloid approximation of L}) of \(L\) we
can write the gradient at \(\theta\) as
%
\begin{align*}
	\nabla L(\theta)
	=  \nabla^2 L(\theta)(\theta-\hat{\theta})
	\ (= -\nabla^2 L(\theta)\hat{v})
\end{align*}
%
But note that \(\hat{\theta}\) depends on \(\theta\), so we need to index both
by \(n\) to rewrite gradient decent
%
\begin{align*}
	\theta_{n+1} &= \theta_n - \eta\nabla L(\theta_n)\\
	&= \theta_n - \eta\nabla^2 L(\theta_n)(\theta_n - \hat{\theta}_n).
\end{align*}
%
Subtracting \(\hat{\theta}_n\) from both sides we obtain the following
transformation 
%
\begin{align}\label{eq: Matrix GD Formulation}
	\theta_{n+1} - \hat{\theta}_n
	&= (\identity - \eta\nabla^2 L(\theta_n) ) (\theta_n - \hat{\theta}_n).
\end{align}
%
Taking a closer look at this transformation matrix we can use (\ref{eq:
diagnalization of the Hesse matrix}) to see
%
\begin{align*}
	\identity - \eta\nabla^2 L(\theta_n)
	&= V(\identity - \eta\cdot\diag(\lambda_1,\dots,\lambda_d) )V^T \\
	&= V\cdot\diag(1-\eta\lambda_1, \dots,1-\eta\lambda_d)V^T.
\end{align*}
%
Now if we assume like \textcite{gohWhyMomentumReally2017}, that the second
taylor approximation is accurate and thus that \(\nabla^2 L(\theta)=H\) is a
constant, then \(\hat{\theta}_n = \theta^*\) is the real minimum and we get
%
\begin{align}
	\theta_n - \theta^*
	= V\cdot\diag[(1-\eta\lambda_1)^n,\dots,(1-\eta\lambda_d)^n] V^T (\theta_0 - \theta^*)
\end{align}
%
by induction. Decomposing the difference into the eigenspaces of \(H\) we can 
see that each component scales exponentially on its own 
%
\begin{align*}
	\langle \theta_n -\theta^*, v_i\rangle
	= (1-\eta\lambda_i)^n \langle \theta_0 - \theta^*, v_i\rangle
\end{align*}
%
This is beautifully illustrated with interactive graphs in
\citetitle{gohWhyMomentumReally2017} by \citeauthor{gohWhyMomentumReally2017}.

\subsection{Negative Eigenvalues}\label{subsec: Negative Eigenvalues}

Now if \(\lambda_i<0\), then \(1-h\lambda_i\) would be greater
than one which would repel this component \(\langle \theta_0 - \theta^*,
v_i\rangle\) away from \(\theta^*\). This is a good thing, since \(\theta^*\)
is a maximum in this component. This means we will walk down local minima and
saddle points no matter the learning rate assuming  this component  was not
zero to begin with. In case of a maximum this would mean that we would start
right on top of the maximum, in case of a saddle point it implies starting on
the rim such that one can not roll down either side.

But since we are ultimately interested in stochastic loss functions, the
stochasticity would push us off these narrow equilibria so being right on
top of them is of little concern. But being close to zero in such a component
still means slow movement away, since we are multiplying by it. This is a
common explanation for the observation of temporary plateaus in machine learning
which "suddenly fall off" once the exponential factor ramps up and causes a
sharp drop in the loss (Figure~\ref{fig: visualize saddlepoint gd}).
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_gd.pdf_tex}
	\caption{Start=\(0.001v_1+4v_2\), \(\lambda_1=-1, \lambda_2=2\), learning rate\(=0.8\)}
	\label{fig: visualize saddlepoint gd}
\end{figure}

\subsection{Assuming Convexity}

Let us consider strictly positive eigenvalues now and assume that our
eigenvalues are already sorted
%
\begin{align}
	0 < \lambda_1 \le \dots \le \lambda_d.
\end{align}
%
Then for positive learning rates all exponentiation bases are smaller than one
%
\begin{align*}
	1-\eta\lambda_d \le \dots \le 1-\eta\lambda_1 \le 1.
\end{align*}
%
But to ensure convergence we also need that all of them are larger than \(-1\).
This leads us to the condition
\begin{align}\label{eq: learning rate restriction (eigenvalue)}
	0< \eta < 2/\lambda_d
\end{align}
%
Selecting \(\eta = 1/\lambda_d\) reduces the exponentiation base ("rate") of the
corresponding eigenspace to zero ensuring convergence in one step.
But if we want to maximize the convergence rate of \(\theta_n\), this is not
the best selection.

We can reduce the rates of the other eigenspaces if we
increase the learning rate further, getting them closer to zero. At this point
we are of course overshooting zero with the largest learning rate, so we only
continue with this until we get closer to \(-1\) with the largest eigenvalue
than we are to \(1\) with the smallest:
%
\begin{align*}
	\rate(\eta)=\max_{i} |1-\eta\lambda_i| = \max\{|1-\eta\lambda_1|, |1-\eta\lambda_d|\}.
\end{align*}
%
This is minimized when
%
\begin{align*}
	1-\eta\lambda_1 = \eta\lambda_d -1,
\end{align*}
%
implying
%
\begin{align*}
	\eta^* = \frac{2}{\lambda_1 + \lambda_d}.
\end{align*}
%
If \(\lambda_1\) is much smaller than \(\lambda_d\), this leaves \(\eta\)
at the upper end of the interval in (\ref{eq: learning rate restriction
(eigenvalue)}). And the optimal convergence rate
%
\begin{align*}
	\rate(\eta^*)
	% &= 1-\eta^* \lambda_1
	= 1 - \frac{2}{1+\kappa}
	\qquad \kappa:=\lambda_d/\lambda_1 \ge 1
\end{align*}
%
becomes close to one if the condition number \(\kappa\) is large.
If all the eigenvalues are the same on the other hand, the condition number
becomes one and the rate is zero, implying instant convergence. This is not
surprising if we recall our visualization in Figure~\ref{fig: 2d paraboloid}.
When the eigenvalues are the same, the contours are concentric circles and the
gradient points right at the center.

\fxnote{strong convexity and lipshitz continuity of gradient as lower and
upper bound, sketch convergence proof using that instead of eigenvalues}

\section{Generalizing Assumptions}

Let us try to get rid of the assumptions we made in \ref{sec: visualize gd}.
The most egregious assumption was a constant second derivative. We used this
constant second derivative to determine the (constant) condition number of
the (constant) eigenvalues. Since we used only the smallest and largest
eigenvalue, it is natural to guess that simply bounding the second derivative
from above and below should be sufficient:
%
\begin{align*}
	\lbound \identity \precsim \nabla^2 L(\theta) \precsim \ubound \identity.
\end{align*}
%
Here \(A \precsim B\) should be read as \(0\precsim B-A\) which we define to mean
\(B-A\) is positive definite.

\subsection{Convexity}

Assuming positive eigenvalues is assuming convexity, and there is not really a
way around that. As we have seen in Subsection~\ref{subsec: Negative
Eigenvalues}, negative eigenvalues act as a repelling force, preventing the
transformation (\ref{eq: Matrix GD Formulation}) from being a contraction.

For this reason authors generally assume either global convexity, or local
convexity together with the assumption that the starting point is in this
locally convex area. This is a reasonable assumption since the negative
eigenvalues mentioned above will force us out of regions where this is not
the case. And if we assume that our loss function \(L\) goes to infinity when
our parameters go to infinity, together with the fact that we are descending
down the loss, then we immediately get a bounded (compact) area 
%
\begin{align*}
	\{\theta : L(\theta) \le L(\theta_0)\} = L^{-1}([0, L(\theta_0)])
\end{align*}
%
in which we will stay. We can use that boundedness to argue that we will
eventually end up in a convex region if we are pushed out of the non-convex
regions.

That such a convex region exists follows from the existence of a minimum of
\(L\) in that compact region which necessitates non-negative
eigenvalues. And continuity of the second derivative allows us to extend that to 
a local ball around the minimum. Eigenvalues equal to zero
are a bit of an issue, but if they become negative in some epsilon ball
then moving in that direction would lead us further down. Which is a contradiction
to our assumption that we created a ball around the minimum of \(L\).

The statement that we are being pushed out of the non-convex regions is a bit
dubious as we could start right on top of local maxima or rims of saddle points.
But as we are ultimately interested in stochastic gradient decent, these zero
measure areas are of little concern. But as we already found out in
Subsection~\ref{subsec: Negative Eigenvalues}, starting close to such a feature
increases the amount of time it takes for us to escape that area which means
we can not really provide an upper bound on the time it takes to end up in
a locally convex area.

So the best we can do without actually getting into probability theory is to
hand-wave this starting phase away with the unlikelihood of it taking too long
due to the exponential repulsion from negative eigenvalues.

In the following we are therefore going to assume convexity. A formulation which
does not require the second derivative is
%
\begin{definition}[Convexity]
	A function \(f\) is called \emph{convex}, if for all \(x,y\), \(\lambda\in[0,1]\)
	\begin{align*}
		f(x + \lambda(y-x)) \le f(x) + \lambda (f(y)-f(x)).
	\end{align*}
	If \(f\) is differentiable this is equivalent to the tangent being below
	the function itself
	\begin{align*}
		f(x) + \langle\nabla f(x), y-x\rangle \le f(y).
	\end{align*}
	If \(f\) is twice differentiable then this is equivalent to
	\begin{align*}
		0\precsim\nabla^2 f.
	\end{align*}
 \end{definition}
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_convexity.pdf_tex}
	\caption{Visualize Convexity Definitions}
	\label{fig: visualize convexity definition}
\end{figure}
%
\begin{proof}
	For the equivalence of these convexity definitions see \textcite[Theorem
	2.1.2, 2.1.4]{nesterovLecturesConvexOptimization2018}\fxnote{appendix?}
 \end{proof}
%
\begin{remark}
	Now if you recall how the rate of convergence was related to the condition number
	\(\lambda_d/\lambda_1 =\kappa\), then you will not be surprised to see us 
	struggle to get a convergence rate with only non-negative eigenvalues in 
	the remaining section. Because eigenvalues close or equal to zero will blow up this
	condition number. We will therefore strengthen these assumptions in
	Section~\ref{sec: Strong Convexity} but we can actually make a few assertions
	with convexity only.
\end{remark}

\subsection{Lipschitz Gradient}

We still need a replacement for the upper bound of our second derivative.
Using the orthonormal basis of eigenvectors of
\(\nabla^2 L(\theta)\) to represent \(x\) in the definition of the operator norm
%
\begin{align*}
	\|A\| := \sup_{\|x\| =1} \|Ax\|
	\left(= \sup_{\|x\| =1} \sqrt{\langle Ax, Ax\rangle}\right)
\end{align*}
%
it becomes immediately clear that the operator norm is equal to the largest
absolute eigenvalue. This implies that for positive eigenvalues 
%
\begin{align}
	\label{eq: operator norm upper bound relation}
	\nabla^2 L(\theta) \precsim \ubound\identity
	\iff \|\nabla^2 L(\theta)\|\le \ubound,
\end{align}
%
and the following lemma allows us to get rid of the existence of the second
derivative for the upper bound entirely
%
\begin{lemma}\label{lem: lipschitz and bounded derivative}
	If \(f\) is differentiable, then the derivative \(\nabla f\) is
	bounded (w.r.t. the operator norm) by constant \(K\)
	\begin{align*}
		\|\nabla f\| \le K
	\end{align*}	
	if and only if the function
	\(f\) is Lipschitz continuous (Lip. constant \(K\))
	\begin{align*}
		\|f(x) - f(y)\| \le K \|x-y\|.
	\end{align*}
\end{lemma}
\begin{proof}
	See Appendix \ref{Appdx-lem: lipschitz and bounded derivative}.
\end{proof}
%
\noindent
So instead of the upper bound on the second derivative we can use Lipschitz
continuity of the derivative and convexity to formulate the upper and lower
bound without actually using the derivative.

Now we have to ask ourselves whether this  is a reasonable assumption. There
are two somewhat convincing reasons why there is probably not much more space
for generalization.

First, we are using our derivative to define an ODE (\ref{eq: velocity is
gradient}) and it is very common to use Lipschitz continuity to argue for
existence, uniqueness and stability of ODEs. While these properties might not
be needed for effective optimization, working without them will likely be very
cumbersome.

Second, we need at least uniform continuity of \(\nabla L\) so that the inequality
%
\begin{align}\label{eq: bounded gradient integral}
	\int_{t_0}^\infty \|\nabla L(\theta(s))\|^2 ds
	&\le L(\theta(t_0)) - \liminf_{t\to\infty} L(\theta(t)) \\
	&\le L(\theta(t_0)) - \inf_{\theta} L(\theta) < \infty \nonumber
\end{align}
%
derived from (\ref{eq: gradient integral}) is sufficient for a convergent
\(\|\nabla L(\theta(t))\|\). That still does not ensure that \(\theta(t)\)
converges, only that its derivative \(\dot{\theta}(t) = -\nabla L(\theta(t))\)
converges. The logarithm is an obvious example where this goes wrong. But
while it is not sufficient, the difference between \(\theta_{n+1}\) and
\(\theta_n\) is proportional to the gradient. Convergence of the gradient is
therefore necessary.
So let us see if we can get convergence of the discrete gradients as well.
%
\begin{lemma}[\citeauthor{nesterovLecturesConvexOptimization2018}]
	\label{lem: Lipschitz Gradient implies taylor inequality}
	Assume \(\nabla f\) is Lipschitz continuous with Lipschitz constant \(K\),
	then we have
	\begin{align*}
		| f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \le \tfrac{K}2 \|y-x\|^2
	\end{align*}
\end{lemma}
\begin{proof}
	See Appendix \ref{Appdx-lem: Lipschitz Gradient implies taylor inequality}.
\end{proof}
%
\noindent
Using the Lipschitz continuity of the gradient and Lemma~\ref{lem: Lipschitz
Gradient implies taylor inequality} we get
%
\begin{align}
	L(\theta_{n+1})
	&\le L(\theta_n) 
	+ \langle\nabla L(\theta_n),\theta_{n+1} - \theta_n\rangle
	+ \tfrac{\ubound}{2} \| \theta_{n+1} - \theta_n\|^2 
	\label{bound increment}\\
	&= L(\theta_n)
	+ \langle\nabla L(\theta_n), -\eta\nabla L(\theta_n)\rangle
	+ \tfrac{\ubound}{2}\eta^2\| \nabla L(\theta_n)\|^2
	\nonumber\\
	&=L(\theta_n) - \eta(1-\tfrac{\eta \ubound}{2})\|\nabla L(\theta_n)\|^2
	\nonumber
\end{align}
%
We can then parametrize all learning rates for which we can guarantee a positive
decrease by
\begin{align}\label{learning rate restrictions}
	0<\eta=\tfrac{2\alpha}{\ubound}<\tfrac2\ubound \qquad \alpha \in (0,1).
\end{align}
%
This is the same bound we have encountered in the simplified case (\ref{eq:
learning rate restriction (eigenvalue)}) which tells us that there are no more
gains to be made. Plugging the parametrization back into the learning rate we
get
%
\begin{align*}
	L(\theta_{n+1}) - L(\theta_n)
	\le - \tfrac{2}{\ubound}\alpha (1-\alpha)\|\nabla L(\theta_n)\|^2.
\end{align*}
%
The largest decrease is guaranteed by \(\eta=\tfrac{1}{\ubound}\) (\(\alpha=1/2\)).
\begin{remark}
	Recall that this sets the rate of the largest eigenvalue to zero eliminating
	the error in that eigenspace immediately, but it is not the optimal
	discretization for \(\theta\) to move towards a minimum in parameter space.
	But here we do not care about the distance in parameter space but the
	distance between the losses. And favouring the eigenspace with larger
	eigenvalues is results in greater reductions in the loss.
\end{remark}
This results in
%
\begin{align*}
	L(\theta_{n+1}) - L(\theta_n)
	\le - \tfrac{1}{2\ubound}\|\nabla L(\theta_n)\|^2.
\end{align*}
%
\subsubsection{Dead End: Convergence of the Gradient}

Summing over these increments results in a very similar equation to
(\ref{eq: bounded gradient integral})
%
\begin{align*}
	\frac{1}{2\ubound} \sum_{k=0}^{n-1}\|\nabla L(\theta_k)\|^2
	\le L(\theta_0) - L(\theta_n)
	\le L(\theta_0) - \inf_{\theta} L(\theta)
\end{align*}
%
Since \(\tfrac{1}{\ubound}\) is the time increment \(\eta\) between the \(\theta_k\)
we have only lost the factor \(1/2\) in our estimation (\ref{bound increment})
compared to the precise integral version.
In particular we get a convergent average of squared gradients
%
\begin{align*}
	\frac{1}{n} \sum_{k=0}^n \|\nabla L(\theta_k)\|^2
	\le \frac{2 \ubound}{n} (L(\theta_0) - \inf_\theta L(\theta)) \in O(1/n).
\end{align*}
%
It is important to note the "square" part. If the series of unsquared gradient
norms were finite as well, the \(\theta_n\) would be a cauchy sequence
%
\begin{align*}
	\|\theta_n - \theta_m \|
	\le \sum_{k=m}^{n-1} \|\theta_{k+1} - \theta_k\|
	\le \eta \sum_{k=m}^{n-1} \|\nabla L(\theta_k)\|.
\end{align*}
%
This provides us with some intuition how a situation might look like when the
gradient converges but not the sequence of \(\theta_n\). The gradient would
behave something like the harmonic series, as its squares converges and the
\(\theta_n\) would behave like the partial sums of the harmonic series which
behaves like the logarithm in the limit.

It is difficult to formulate an example in finite space, but
%
\begin{align*}
	L(\theta) &= \exp(-\theta) \\
	\dot{\theta} &= -\nabla L(\theta)
\end{align*}
%
implies
%
\begin{align*}
	\theta(t) &= \log(t)\\
	\nabla L(t) &= -\tfrac1t
\end{align*}
%
which provides intuition how "flat" a minima has to be to cause such behavior.
The minimum at \(\infty\) has an infinitely wide basin which flattens out
more and more. If we wanted such an example in a bounded space we would have
to try and coil up such an infinite slope into a spiral, which spirals outwards
to avoid convergence.

Since our example is one dimensional we can also immediately see the "eigenvalue"
%
\begin{align*}
	\nabla^2 L(\theta(t)) = \exp(-\theta(t)) = 1/t
\end{align*}
%
which decreases towards zero, stalling the movement towards the minimum.

\section{Convex Convergence Theorems}

While we do need lower bounds on the second derivative to achieve convergence
of \(\theta(t)\) as motivated in the previous section we can get convergence
of the loss

\begin{theorem}[\citeauthor{nesterovLecturesConvexOptimization2018}]
	Let \(L\) be convex, differentiable with Lipschitz gradient, then Gradient
	Decent with learning rate \(0 < \eta < 2/\ubound\) results in
	\begin{align*}
		L(\theta_n) - \inf_\theta L(\theta)
		\le \frac{2\ubound\|\theta_0 - \theta^*\|}{4 + n\ubound\eta(2-\ubound\eta)}
		\in O(1/n)
	\end{align*}
	with the optimal rate of convergence being achieved for \(\eta=1/\ubound\).
\end{theorem}
\begin{proof}
	See \textcite[Theorem 2.1.14, Corollary
	2.1.2]{nesterovLecturesConvexOptimization2018}
\end{proof}

\fxnote{More extensive treatment?}{
And if we can not have convergence of the parameters \(\theta\) we might not
care as much about the convergence of the gradient either. In that case it
turns out that we can even get convergence of the loss if we only assume
Lipschitz continuity of \(L\) itself. But this form of "Subgradient Decent"
requires a bounded, convex parameter set we project back into if we leave and
a decreasing learning rate. And since we can not guarantee a monotonic decrease
with subgradients, the name "decent" is generally avoided and we have to keep
a running minimum. See \textcite[Section 3.2.3]{nesterovLecturesConvexOptimization2018}
or \textcite[Section 3.1]{bubeckConvexOptimizationAlgorithms2015} for a proper
treatment. The convergence rate is not only dependent on the size of the
parameter set \(B\) but it also decreases to \(O(B/\sqrt{n})\).
}

\subsection{Complexity Bound: The Colorization Problem}

Now that we found convergence rates for gradient decent on convex problems, can
we find a lower bound for the complexity of the problem? I.e. how fast could
an algorithm we could possibly think of be? To tackle this problem let us first
make an assumption about what this algorithm can do.

Unrolling our gradient decent algorithm
%
\begin{align*}
	\theta_n = \theta_0 - \eta\sum_{k=0}^{n-1} \nabla L(\theta_k)
\end{align*}
%
we can see that even if we allowed custom learning rates for every iteration
%
\begin{align*}
	\theta_n = \theta_0 - \sum_{k=0}^{n-1} \eta_k \nabla L(\theta_k)
\end{align*}
%
we would still end up in the linear span of all gradients, shifted by \(\theta_0\).
An obvious assumption for a class of optimization methods would thus be
%
\begin{assumption}[\citeauthor{nesterovLecturesConvexOptimization2018}]
	\label{assmpt: parameter in linear hull of gradients}
	The \(n\)-th iterate of the optimization method is contained in the span of all
	previous gradients shifted by the initial starting point
	\begin{align*}
		\theta_n \in \linSpan\{\nabla L(\theta_k) : 0\le k \le n-1\} + \theta_0
	\end{align*}
\end{assumption}

Now the question becomes: How can we utilize this assumption to construct a
function which is difficult to optimize?

\textcite{gohWhyMomentumReally2017} provides an intuitive interpretation for a loss
function taken from \textcite[Section 2.1.2]{nesterovLecturesConvexOptimization2018}
which I will try to reproduce here.

\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_coloring_problem.pdf_tex}
	\caption{Even after \(\dimension\) steps (when all tiles have a non-zero value)
	gradient decent is still far off from the solution even using the optimal
	learning rate \(1/\ubound\)}
	\label{fig: visualize coloring problem}
\end{figure}
%
Consider an empty elongated swimming pool with a running tap on one side. Then
the second meter of the swimming pool will only start to get wet once the first
meter of it contains water. Similarly the \(n\)-th meter will only get wet
once the \((n-1)\)-th meter contains water. If the flow rate would only depend
on the current water level difference, one could thus express a simplified
water level recursion as follows\footnote{In the limit this would become the
well known differential equation for heat.}: 
%
\begin{align*}
	\theta_{n+1}^{(i)}
	&= \theta_{n}^{(i)}
	+ \eta 
	\begin{cases}
		[\theta_{n}^{(i-1)} - \theta_{n}^{(i)}] + [\theta_{n}^{(i+1)} - \theta_{n}^{(i)}]
		&  1 < i < \dimension \\
		[1 - \theta_{n}^{(1)}] + [\theta_{n}^{(2)} - \theta_{n}^{(1)}]
		& i = 1 \\
		[\theta_{n}^{(i-1)} - \theta_{n}^{(i)}]
		& i = \dimension
	\end{cases}
\end{align*}
%
For the \fxnote{appendix?}{(convex)} loss function
%
\begin{align*}
	L(\theta) = \frac12 (\theta^{(1)}-1)^2  + \frac12 \sum_{k=1}^{\dimension-1} (\theta^{(k)}-\theta^{(k+1)})^2
\end{align*}
%
this is just the gradient decent recursion. Now here is the crucial insight:

Since any weight (parameter component) is only affected once its neighbors are
affected, the second weight can not be affected before the second step since
the first weight is only filled in the first step. And since the second weight
will be unaffected until the second step, the third weight will be unaffected
until the third step, etc.

Therefore the \(\dimension - n\) last components will still be zero in
step \(n\), because the linear span of all the gradients so far is still
contained in the subspace \(\reals^n\) of \(\reals^\dimension\). Formalizing
this argument results in the following theorem.

\begin{theorem}[\citeauthor{nesterovLecturesConvexOptimization2018}]
	For any \(\theta_0\in\reals^d\), any \(k\) such that \(0\le k\le \tfrac12 (d-1)\),
	there exists a convex, smooth function \(f\) with Lipschitz continuous
	derivative such that for any first order method \(\firstOrderMethod\)
	satisfying Assumption~\ref{assmpt: parameter in linear hull of gradients}
	we have
	\begin{subequations}
	\begin{align}
		f(\theta_k) - \inf_\theta f(\theta)
		&\ge \frac{3\ubound \|\theta_0 - \theta^*\|^2}{32(k+1)^2} \\
		\|\theta_k -\theta^*\|^2 
		&\ge \frac18 \|\theta_0 - \theta^*\|^2
	\end{align}
	\end{subequations}
	where \(\theta^* = \arg\min_\theta f(\theta)\).
\end{theorem}
\begin{proof}
	See \textcite[Theorem 2.1.7]{nesterovLecturesConvexOptimization2018}
\end{proof}

\begin{remark}
	This type of loss function is not unlikely to be encountered in real
	problems. E.g. in reinforcement learning with sparse rewards the estimation
	of the value function over states requires the back-propagation of this
	reward through the states that lead up to it. "Eligibility traces"
	\parencite[Chapter 12]{suttonReinforcementLearningIntroduction2018}
	are an attempt to speed this process up.
	\end{remark}

\section{Strong Convexity}\label{sec: Strong Convexity}

To get convergence of the iterate we need to reintroduce a lower bound on
the second derivative. As you might recall from Lemma~\ref{lem: Lipschitz
Gradient implies taylor inequality}, a Lipschitz continuous gradient implies
a quadratic function provides an upper bound for our loss function
%
\begin{align*}
	L(y) \le L(x) + \langle \nabla L(x), y-x\rangle + \frac\ubound{2}\|y-x\|^2.
\end{align*}
%
And convexity was defined as a lower bound
%
\begin{align*}
	L(x) + \langle \nabla L(x), y-x\rangle \le L(y)
\end{align*}
%
which we will now sharpen this to a quadratic lower bound as well.
%
\begin{definition}
	A function \(L\) is \emph{strongly convex}, if for all \(x,y\)
	\begin{align*}
		L(x) + \langle \nabla L(x), y-x\rangle + \frac{\lbound}{2}\|y-x\|^2 \le L(y)
	\end{align*}
\end{definition}
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_strong_convexity.pdf_tex}
	\caption{Strong Convexity provides a tighter lower bound}
	\label{fig: visualize strong convexity}
\end{figure}
%

\fxnote{If we achieve a contraction towards our minimum, the
function necessarily has to be convex, cf. https://youtu.be/6WeyTUnbwQQ?t=1020}


\section{Heuristics}

\subsection{Adagrad}

\subsection{Adadelta}

\subsection{RMSProp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput