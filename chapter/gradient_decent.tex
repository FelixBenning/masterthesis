% !TEX root = ../Masterthesis.tex

\chapter{Gradient Decent (GD)}

A step up from zero order methods (such as grid search) are first order methods
using the first derivative (the gradient). This requires a "first order oracle"
which can provide us with \(\Loss(\weights)\) and \(\nabla \Loss(\weights)\) at any point
\(\weights\).

One immediately obvious way to utilize this information is to
incrementally move in the direction of steepest decent, i.e.
%
\begin{align*}
	\weights_{n+1} = \weights_n - \lr\nabla \Loss(\weights_n).
\end{align*}
%
A useful way to look at this equation is to notice that it is the
discretization of an ordinary differential equation (ODE)
%
\begin{align*}
	\dot{\weights}_n \approx \frac{\weights_{n+1} - \weights_n}{\lr}
	= - \nabla \Loss(\weights_n).
\end{align*}
%
Here the "learning rate" \(\lr\) is the time delta between discretizations
\(t_n\) and \(t_{n+1}\). This of course implies \(t_n = n\lr\). And for
\(\lr\to\infty\), we arrive at the ODE
%
\begin{align}\label{eq: velocity is gradient}
	\dot{\weights}(t) = -\nabla \Loss(\weights(t)).
\end{align}
%
If you are familiar with Lyapunov functions you will not be surprised by the next
argument:
%
\begin{align}\label{eq: gradient integral}
	\Loss(\weights(t_1)) - \Loss(\weights(t_0))
	&= \int_{t_0}^{t_1} \nabla \Loss(\weights(s)) \cdot \dot{\weights}(s) ds
	= \int_{t_0}^{t_1} -\|\nabla \Loss(\weights(s))\|^2 ds
	\le 0
\end{align}
%
immediately implies
\begin{align*}
	\Loss(\weights(t_0)) \ge \Loss(\weights(t_1)) \ge \dots \ge \inf_\weights \Loss(\weights) \ge 0
\end{align*}
which implies convergence. But not necessarily a convergent \(\weights(t)\) and
not necessarily convergence to \(\inf_\weights \Loss(\weights)\).

\section{Visualizing the 2nd Taylor Approximation}\label{sec: visualize gd}

To build some intuition what leads to convergence of \(\weights(t)\) let us
consider more simple cases. If we assume our Hesse matrix \(\nabla^2
\Loss(\weights)\) exists and is positive definite (all eigenvalues positive), then for
the second order Taylor approximation
%
\begin{align*}
	\Loss(\weights+v) \approx T_2\Loss(\weights+v)
	= \Loss(\weights) + v^T \nabla \Loss(\weights) + \tfrac12 v^T \nabla^2 \Loss(\weights) v
\end{align*}
%
we can find the minimum 
\begin{align*}
	\hat{x} = -(\nabla^2 \Loss(\weights))^{-1}\nabla \Loss(\weights)
\end{align*}
by setting the first derivative to zero
%
\begin{align*}
	\nabla T_2\Loss(\weights+x) = \nabla \Loss(\weights) + \nabla^2\Loss(\weights) x \xeq{!} 0.
\end{align*}
%
This minimum allows us not only to rewrite the taylor derivative
%
\begin{align*}
	\nabla T_2\Loss(\weights+x) = \nabla^2 \Loss(\weights)(x-\hat{x}),
\end{align*}
%
but also the original taylor approximation
%
\begin{align*}
	T_2\Loss(\weights+x)
	&= \Loss(\weights) - x^T \nabla^2 \Loss(\weights) \hat{x} + \tfrac12 x^T \nabla^2 \Loss(\weights) x \\
	&= \underbrace{
		\Loss(\weights) - \tfrac12 \hat{x} \nabla^2 \Loss(\weights) \hat{x}
	}_{
		=: c(\weights) \text{ (const.)}
	} + \tfrac12 (x-\hat{x})^T \nabla^2 \Loss(\weights)(x-\hat{x}).
\end{align*}
%
To get absolute instead of relative coordinates to \(\weights\) we set
%
\begin{align}\label{eq: newton minimum approx}
	\hat{\weights} := \weights + \hat{x} = \weights -(\nabla^2 \Loss(\weights))^{-1}\nabla \Loss(\weights),
\end{align}
%
and obtain the notion of a paraboloid centered around \(\hat{\weights}\)
%
\begin{align}\label{paraboloid approximation of L}
	\Loss(y)
	= \tfrac12 (y- \hat{\weights}) \nabla^2 \Loss(\weights) (y-\hat{\weights})
	+ c(\weights) + o(\|y-\weights\|^2)
\end{align}
%
\begin{wrapfigure}{O}{0.65\textwidth}
	\centering
	\def\svgwidth{0.65\textwidth}
	\input{media/contour.pdf_tex}
	\caption{Assuming \(\hat{\weights}=0\), \(\lambda_1=1, \lambda_2=2\), \(v_1=(\sin(1), \cos(1))\)}
	\label{fig: 2d paraboloid}
\end{wrapfigure}
%
To fully appreciate Figure~\ref{fig: 2d paraboloid}, we now only need to realize
that the diagonizability of \(\nabla^2 \Loss(\weights)\)
%
\begin{align}\label{eq: diagnalization of the Hesse matrix}
	V \nabla^2 \Loss(\weights) V^T
	= \diag(\lambda_1,\dots,\lambda_d), \qquad V=(v_1,\dots, v_d)
\end{align}
%
implies that once we have selected the center \(\hat{\weights}\) and the direction of
one eigenspace in two dimensions, the other eigenspace has to be
orthogonal to the first one which fully determines its direction at this point. 

Before we move on I want to briefly mention that we only really needed the
positive definiteness of \(\nabla^2 \Loss(\weights)\) to make it invertible, so that
\(\hat{x}\) is well defined. If it was not positive definite but still invertible,
then \(\hat{x}\) would not be a minimum but all the other arguments would still
hold.
In that case the eigenvalues might be negative as well as positive, which would
represent a saddle point, or all negative which would represent a maximum.

Using the representation (\ref{paraboloid approximation of L}) of \(\Loss\) we
can write the gradient at \(\weights\) as
%
\begin{align}\label{eq: hesse representation of gradient}
	\nabla \Loss(\weights)
	=  \nabla^2 \Loss(\weights)(\weights-\hat{\weights})
	\ (= -\nabla^2 \Loss(\weights)\hat{x})
\end{align}
%
But note that \(\hat{\weights}\) depends on \(\weights\), so we need to index both
by \(n\) to rewrite gradient decent
%
\begin{align*}
	\weights_{n+1} &= \weights_n - \lr\nabla \Loss(\weights_n)\\
	&= \weights_n - \lr\nabla^2 \Loss(\weights_n)(\weights_n - \hat{\weights}_n).
\end{align*}
%
Subtracting \(\hat{\weights}_n\) from both sides we obtain the following
transformation 
%
\begin{align}\label{eq: Matrix GD Formulation}
	\weights_{n+1} - \hat{\weights}_n
	&= (\identity - \lr\nabla^2 \Loss(\weights_n) ) (\weights_n - \hat{\weights}_n).
\end{align}
%
Taking a closer look at this transformation matrix we can use (\ref{eq:
diagnalization of the Hesse matrix}) to see
%
\begin{align*}
	\identity - \lr\nabla^2 \Loss(\weights_n)
	&= V(\identity - \lr\cdot\diag(\lambda_1,\dots,\lambda_d) )V^T \\
	&= V\cdot\diag(1-\lr\lambda_1, \dots,1-\lr\lambda_d)V^T.
\end{align*}
%
Now if we assume like \textcite{gohWhyMomentumReally2017}, that the second
taylor approximation is accurate and thus that \(\nabla^2 \Loss(\weights)=H\) is a
constant, then \(\hat{\weights}_n = \minimum\) is the real minimum and we get
%
\begin{align}
	\weights_n - \minimum
	= V\cdot\diag[(1-\lr\lambda_1)^n,\dots,(1-\lr\lambda_d)^n] V^T (\weights_0 - \minimum)
\end{align}
%
by induction. Decomposing the difference into the eigenspaces of \(H\) we can 
see that each component scales exponentially on its own 
%
\begin{align*}
	\langle \weights_n -\minimum, v_i\rangle
	= (1-\lr\lambda_i)^n \langle \weights_0 - \minimum, v_i\rangle
\end{align*}
%
This is beautifully illustrated with interactive graphs in
\citetitle{gohWhyMomentumReally2017} by \citeauthor{gohWhyMomentumReally2017}.

\subsection{Negative Eigenvalues}\label{subsec: Negative Eigenvalues}

Now if \(\lambda_i<0\), then \(1-\lr\lambda_i\) would be greater
than one which would repel this component \(\langle \weights_0 - \minimum,
v_i\rangle\) away from \(\minimum\). This is a good thing, since \(\minimum\)
is a maximum in this component. This means we will walk down local minima and
saddle points no matter the learning rate assuming  this component  was not
zero to begin with. In case of a maximum this would mean that we would start
right on top of the maximum, in case of a saddle point it implies starting on
the rim such that one can not roll down either side.

But since we are ultimately interested in stochastic loss functions, the
stochasticity would push us off these narrow equilibria so being right on
top of them is of little concern. But being close to zero in such a component
still means slow movement away, since we are multiplying by it. This is a
common explanation for the observation of temporary plateaus in machine learning
which "suddenly fall off" once the exponential factor ramps up and causes a
sharp drop in the loss (Figure~\ref{fig: visualize saddlepoint gd}).
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_gd.pdf_tex}
	\caption{Start=\(0.001v_1+4v_2\), \(\lambda_1=-1, \lambda_2=2\), learning rate\(=0.8\)}
	\label{fig: visualize saddlepoint gd}
\end{figure}

\subsection{Assuming Convexity}

Let us consider strictly positive eigenvalues now and assume that our
eigenvalues are already sorted
%
\begin{align}
	0 < \lambda_1 \le \dots \le \lambda_d.
\end{align}
%
Then for positive learning rates all exponentiation bases are smaller than one
%
\begin{align*}
	1-\lr\lambda_d \le \dots \le 1-\lr\lambda_1 \le 1.
\end{align*}
%
But to ensure convergence we also need that all of them are larger than \(-1\).
This leads us to the condition
\begin{align}\label{eq: learning rate restriction (eigenvalue)}
	0< \lr < 2/\lambda_d
\end{align}
%
Selecting \(\lr = 1/\lambda_d\) reduces the exponentiation base ("rate") of the
corresponding eigenspace to zero ensuring convergence in one step.
But if we want to maximize the convergence rate of \(\weights_n\), this is not
the best selection.

We can reduce the rates of the other eigenspaces if we
increase the learning rate further, getting them closer to zero. At this point
we are of course overshooting zero with the largest learning rate, so we only
continue with this until we get closer to \(-1\) with the largest eigenvalue
than we are to \(1\) with the smallest:
%
\begin{align*}
	\rate(\lr)=\max_{i} |1-\lr\lambda_i| = \max\{|1-\lr\lambda_1|, |1-\lr\lambda_d|\}.
\end{align*}
%
This is minimized when
%
\begin{align*}
	1-\lr\lambda_1 = \lr\lambda_d -1,
\end{align*}
%
implying
%
\begin{align}\label{eq: optimal SGD lr eigenvalue representation}
	\lr^* = \frac{2}{\lambda_1 + \lambda_d}.
\end{align}
%
If \(\lambda_1\) is much smaller than \(\lambda_d\), this leaves \(\lr\)
at the upper end of the interval in (\ref{eq: learning rate restriction
(eigenvalue)}). And the optimal convergence rate
%
\begin{align*}
	\rate(\lr^*)
	% &= 1-\lr^* \lambda_1
	= 1 - \frac{2}{1+\condition}
	\qquad \condition:=\lambda_d/\lambda_1 \ge 1
\end{align*}
%
becomes close to one if the condition number \(\condition\) is large.
If all the eigenvalues are the same on the other hand, the condition number
becomes one and the rate is zero, implying instant convergence. This is not
surprising if we recall our visualization in Figure~\ref{fig: 2d paraboloid}.
When the eigenvalues are the same, the contours are concentric circles and the
gradient points right at the center.

\fxnote{strong convexity and lipshitz continuity of gradient as lower and
upper bound, sketch convergence proof using that instead of eigenvalues}

\section{Generalizing Assumptions}

Let us try to get rid of the assumptions we made in \ref{sec: visualize gd}.
The most egregious assumption was a constant second derivative. We used this
constant second derivative to determine the (constant) condition number of
the (constant) eigenvalues. Since we used only the smallest and largest
eigenvalue, it is natural to guess that simply bounding the second derivative
from above and below should be sufficient:
%
\begin{align*}
	\lbound \identity \precsim \nabla^2 \Loss(\weights) \precsim \ubound \identity.
\end{align*}
%
Here \(A \precsim B\) should be read as \(0\precsim B-A\) which we define to mean
\(B-A\) is positive definite.

\subsection{Convexity}

Assuming positive eigenvalues is assuming convexity, and there is not really a
way around that. As we have seen in Subsection~\ref{subsec: Negative
Eigenvalues}, negative eigenvalues act as a repelling force, preventing the
transformation (\ref{eq: Matrix GD Formulation}) from being a contraction.

For this reason authors generally assume either global convexity, or local
convexity together with the assumption that the starting point is in this
locally convex area. This is a reasonable assumption since the negative
eigenvalues mentioned above will force us out of regions where this is not
the case. And if we assume that our loss function \(\Loss\) goes to infinity when
our parameters go to infinity, together with the fact that we are descending
down the loss, then we immediately get a bounded (compact) area 
%
\begin{align*}
	\{\weights : \Loss(\weights) \le \Loss(\weights_0)\} = \Loss^{-1}([0, \Loss(\weights_0)])
\end{align*}
%
in which we will stay. We can use that boundedness to argue that we will
eventually end up in a convex region if we are pushed out of the non-convex
regions.

That such a convex region exists follows from the existence of a minimum of
\(\Loss\) in that compact region which necessitates non-negative
eigenvalues. And continuity of the second derivative allows us to extend that to 
a local ball around the minimum. Eigenvalues equal to zero
are a bit of an issue, but if they become negative in some epsilon ball
then moving in that direction would lead us further down. Which is a contradiction
to our assumption that we created a ball around the minimum of \(\Loss\).

The statement that we are being pushed out of the non-convex regions is a bit
dubious as we could start right on top of local maxima or rims of saddle points.
But as we are ultimately interested in stochastic gradient decent, these zero
measure areas are of little concern. But as we already found out in
Subsection~\ref{subsec: Negative Eigenvalues}, starting close to such a feature
increases the amount of time it takes for us to escape that area which means
we can not really provide an upper bound on the time it takes to end up in
a locally convex area.

So the best we can do without actually getting into probability theory is to
hand-wave this starting phase away with the unlikelihood of it taking too long
due to the exponential repulsion from negative eigenvalues.

In the following we are therefore going to assume convexity. A formulation which
does not require the second derivative is
%
\begin{definition}[Convexity]\label{def: convexity}
	A function \(f\) is called \emph{convex}, if for all \(x,y\), \(\lambda\in[0,1]\)
	\begin{align*}
		f(x + \lambda(y-x)) \le f(x) + \lambda (f(y)-f(x)).
	\end{align*}
	If \(f\) is differentiable this is equivalent to the tangent being below
	the function itself
	\begin{align*}
		f(x) + \langle\nabla f(x), y-x\rangle \le f(y).
	\end{align*}
	If \(f\) is twice differentiable then this is equivalent to
	\begin{align*}
		0\precsim\nabla^2 f.
	\end{align*}
 \end{definition}
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_convexity.pdf_tex}
	\caption{Visualize Convexity Definitions}
	\label{fig: visualize convexity definition}
\end{figure}
%
\begin{proof}
	For the equivalence of these convexity definitions see \textcite[Theorem
	2.1.2, 2.1.4]{nesterovLecturesConvexOptimization2018}\fxnote{appendix?}
 \end{proof}
%
\begin{remark}
	Now if you recall how the rate of convergence was related to the condition number
	\(\lambda_d/\lambda_1 =\condition\), then you will not be surprised to see us 
	struggle to get a convergence rate with only non-negative eigenvalues in 
	the remaining section. Because eigenvalues close or equal to zero will blow up this
	condition number. We will therefore strengthen these assumptions in
	Section~\ref{sec: Strong Convexity} but we can actually make a few assertions
	with convexity only.
\end{remark}

\subsection{Lipschitz Gradient}

We still need a replacement for the upper bound of our second derivative.
Using the orthonormal basis of eigenvectors of
\(\nabla^2 \Loss(\weights)\) to represent \(x\) in the definition of the operator norm
%
\begin{align*}
	\|A\| := \sup_{\|x\| =1} \|Ax\|
	\left(= \sup_{\|x\| =1} \sqrt{\langle Ax, Ax\rangle}\right)
\end{align*}
%
it becomes immediately clear that the operator norm is equal to the largest
absolute eigenvalue. This implies that for positive eigenvalues 
%
\begin{align}
	\label{eq: operator norm upper bound relation}
	\nabla^2 \Loss(\weights) \precsim \ubound\identity
	\iff \|\nabla^2 \Loss(\weights)\|\le \ubound,
\end{align}
%
and the following lemma allows us to get rid of the existence of the second
derivative for the upper bound entirely
%
\begin{lemma}\label{lem: lipschitz and bounded derivative}
	If \(f\) is differentiable, then the derivative \(\nabla f\) is
	bounded (w.r.t. the operator norm) by constant \(\lipConst\)
	\begin{align*}
		\|\nabla f\| \le\lipConst 
	\end{align*}	
	if and only if the function
	\(f\) is \(\lipConst\)-Lipschitz continuous
	\begin{align*}
		\|f(x) - f(y)\| \le \lipConst \|x-y\|.
	\end{align*}
\end{lemma}
\begin{proof}
	See Appendix \ref{Appdx-lem: lipschitz and bounded derivative}.
\end{proof}
%
\noindent
So instead of the upper bound on the second derivative we can use Lipschitz
continuity of the derivative and convexity to formulate the upper and lower
bound without actually using the derivative.

Now we have to ask ourselves whether this  is a reasonable assumption. There
are two somewhat convincing reasons why there is probably not much more space
for generalization.

First, we are using our derivative to define an ODE (\ref{eq: velocity is
gradient}) and it is very common to use Lipschitz continuity to argue for
existence, uniqueness and stability of ODEs. While these properties might not
be needed for effective optimization, working without them will likely be very
cumbersome.

Second, we need at least uniform continuity of \(\nabla \Loss\) so that the inequality
%
\begin{align}\label{eq: bounded gradient integral}
	\int_{t_0}^\infty \|\nabla \Loss(\weights(s))\|^2 ds
	&\le \Loss(\weights(t_0)) - \liminf_{t\to\infty} \Loss(\weights(t)) \\
	&\le \Loss(\weights(t_0)) - \inf_{\weights} \Loss(\weights) < \infty \nonumber
\end{align}
%
derived from (\ref{eq: gradient integral}) is sufficient for a convergent
\(\|\nabla \Loss(\weights(t))\|\). That still does not ensure that \(\weights(t)\)
converges, only that its derivative \(\dot{\weights}(t) = -\nabla \Loss(\weights(t))\)
converges. The logarithm is an obvious example where this goes wrong. But
while it is not sufficient, the difference between \(\weights_{n+1}\) and
\(\weights_n\) is proportional to the gradient. Convergence of the gradient is
therefore necessary.
So let us see if we can get convergence of the discrete gradients as well.
%
\begin{lemma}[\cite{nesterovLecturesConvexOptimization2018}]
	\label{lem: Lipschitz Gradient implies taylor inequality}
	If \(\nabla f\) is \(\ubound\)-Lipschitz continuous, then
	\begin{align*}
		| f(y) - f(x) - \langle \nabla f(x), y-x\rangle |
		\le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}
	If \(f\) is convex, then the opposite direction is also true.
\end{lemma}
\begin{proof}
	See Appendix \ref{Appdx-lem: Lipschitz Gradient implies taylor inequality}.
\end{proof}
%
\noindent
Using the Lipschitz continuity of the gradient and Lemma~\ref{lem: Lipschitz
Gradient implies taylor inequality} we get
%
\begin{align}
	\Loss(\weights_{n+1})
	&\le \Loss(\weights_n) 
	+ \langle\nabla \Loss(\weights_n),\weights_{n+1} - \weights_n\rangle
	+ \tfrac{\ubound}{2} \| \weights_{n+1} - \weights_n\|^2 
	\label{eq: bound increment}\\
	&= \Loss(\weights_n)
	+ \langle\nabla \Loss(\weights_n), -\lr\nabla \Loss(\weights_n)\rangle
	+ \tfrac{\ubound}{2}\lr^2\| \nabla \Loss(\weights_n)\|^2
	\nonumber\\
	&=\Loss(\weights_n) - \lr(1-\tfrac{\lr \ubound}{2})\|\nabla \Loss(\weights_n)\|^2
	\nonumber
\end{align}
%
We can then parametrize all learning rates for which we can guarantee a positive
decrease by
\begin{align}\label{learning rate restrictions}
	0<\lr=\tfrac{2\alpha}{\ubound}<\tfrac2\ubound \qquad \alpha \in (0,1).
\end{align}
%
This is the same bound we have encountered in the simplified case (\ref{eq:
learning rate restriction (eigenvalue)}) which tells us that there are no more
gains to be made. Plugging the parametrization back into the learning rate we
get
%
\begin{align*}
	\Loss(\weights_{n+1}) - \Loss(\weights_n)
	\le - \tfrac{2}{\ubound}\alpha (1-\alpha)\|\nabla \Loss(\weights_n)\|^2.
\end{align*}
%
The largest decrease is guaranteed by \(\lr=\tfrac{1}{\ubound}\) (\(\alpha=1/2\)).
This results in
%
\begin{align}\label{eq: optimal loss decrease step}
	\Loss(\weights_{n+1}) - \Loss(\weights_n)
	\le - \tfrac{1}{2\ubound}\|\nabla \Loss(\weights_n)\|^2.
\end{align}
\begin{remark}
	Recall that this sets the rate of the largest eigenvalue to zero eliminating
	the error in that eigenspace immediately, but it is not the optimal
	discretization for \(\weights\) to move towards a minimum in parameter space.
	But here we do not care about the distance in parameter space but the
	distance between the losses. And favouring the eigenspace with larger
	eigenvalues is results in greater reductions in the loss.
\end{remark}
%
\subsubsection{Dead End: Convergence of the Gradient}

Summing over these increments results in a very similar equation to
(\ref{eq: bounded gradient integral})
%
\begin{align*}
	\frac{1}{2\ubound} \sum_{k=0}^{n-1}\|\nabla \Loss(\weights_k)\|^2
	\le \Loss(\weights_0) - \Loss(\weights_n)
	\le \Loss(\weights_0) - \inf_{\weights} \Loss(\weights)
\end{align*}
%
Since \(\tfrac{1}{\ubound}\) is the time increment \(\lr\) between the \(\weights_k\)
we have only lost the factor \(1/2\) in our estimation (\ref{eq: bound increment})
compared to the precise integral version.
In particular we get a convergent average of squared gradients
%
\begin{align*}
	\frac{1}{n} \sum_{k=0}^n \|\nabla \Loss(\weights_k)\|^2
	\le \frac{2 \ubound}{n} (\Loss(\weights_0) - \inf_\weights \Loss(\weights)) \in O(1/n).
\end{align*}
%
It is important to note the "square" part. If the series of unsquared gradient
norms were finite as well, the \(\weights_n\) would be a cauchy sequence
%
\begin{align*}
	\|\weights_n - \weights_m \|
	\le \sum_{k=m}^{n-1} \|\weights_{k+1} - \weights_k\|
	= \lr \sum_{k=m}^{n-1} \|\nabla \Loss(\weights_k)\|.
\end{align*}
%
This provides us with some intuition how a situation might look like when the
gradient converges but not the sequence of \(\weights_n\). The gradient would
behave something like the harmonic series, as its squares converges and the
\(\weights_n\) would behave like the partial sums of the harmonic series which
behaves like the logarithm in the limit.

It is difficult to formulate an example in finite space, but
%
\begin{align*}
	\Loss(\weights) &= \exp(-\weights) \\
	\dot{\weights} &= -\nabla \Loss(\weights)
\end{align*}
%
implies
%
\begin{align*}
	\weights(t) &= \log(t)\\
	\nabla \Loss(t) &= -\tfrac1t
\end{align*}
%
which provides intuition how "flat" a minima has to be to cause such behavior.
The minimum at \(\infty\) has an infinitely wide basin which flattens out
more and more. If we wanted such an example in a bounded space we would have
to try and coil up such an infinite slope into a spiral, which spirals outwards
to avoid convergence.

Since our example is one dimensional we can also immediately see the "eigenvalue"
%
\begin{align*}
	\nabla^2 \Loss(\weights(t)) = \exp(-\weights(t)) = 1/t
\end{align*}
%
which decreases towards zero, stalling the movement towards the minimum.

\section{Convergence with Convexity}\label{sec: convex convergence theorems}

While we do need lower bounds on the second derivative to achieve convergence
of \(\weights(t)\) as motivated in Section~\ref{sec: visualize gd} we can get
convergence of the loss without them.

To understand the convergence proof it is helpful to take a closer look at
what we have done in (\ref{eq: bound increment}). First we used the Lipschitz
continuity of the gradient (i.e. Lemma~\ref{lem: Lipschitz Gradient implies taylor inequality})
to bound our function
%
\begin{align}\label{eq: lin approx + distance penalty notion}
	\Loss(y)
	\le \underbrace{
		\Loss(\weights_n)+ \langle\nabla\Loss(\weights_n), y-\weights_n\rangle 
	}_{\text{linear approximation}}
	+ \underbrace{\tfrac\ubound{2} \|y-\weights_n\|^2}_{\text{distance penalty}}.
\end{align}
%
The distance penalty can be understood in view of Lemma~\ref{lem: lipschitz and bounded derivative}
which states that the change of gradient (third derivative) is bounded by \(\ubound\).
In other words: The gradient changes at most at rate \(\ubound\) so if we
add this upper bound as a penalty we can be sure to have a real upper bound.

Gradient Decent with optimal learning rate minimizes this upper bound:
%
\begin{lemma}\label{lem: smallest upper bound}
	Let \(\nabla f\) be \(\ubound\)-Lipschitz continuous, then for any \(x\in\reals^d\)
	\begin{subequations}
	\begin{align}
		x - \tfrac{1}{\ubound}\nabla f(x) 
		&= \arg\min_{y}\{
			f(x)+ \langle\nabla f(x), y-x\rangle + \tfrac{\ubound}{2}\|y-x\|^2 
		\}\\
		\label{eq: min approximation}
		\min_{y} f(y)	
		&\lxle{\ref{lem: Lipschitz Gradient implies taylor inequality}} \min_{y} \{
			f(x)+ \langle\nabla f(x), y-x\rangle + \tfrac{\ubound}{2}\|y-x\|^2 
		\} \\ \nonumber
		&= f(x) - \tfrac{1}{2\ubound} \|\nabla f(x)\|^2
	\end{align}
	\end{subequations}
\end{lemma}
\begin{proof}
	The direction is irrelevant for the distance penalty term \(\|y-x\|^2\), and
	\begin{align*}
		y-x = -\tfrac{r}{\|\nabla f(x)\|}\nabla f(x)
	\end{align*}	
	solves
	\begin{align*}
		\min_{x,y} \{\langle \nabla f(x), y-x \rangle : \|y-x\|=r \}
	\end{align*}
	as can be seen via the Cauchy-Schwarz inequality. We can  therefore
	reformulate our minimization problem into
	\begin{align*}
		\min_{r} \{ f(x) + \langle \nabla f(x), -r \nabla f(x)\rangle
		+ \tfrac{\ubound}{2}\|r\nabla f(x)\|^2\}
	\end{align*}
	which we have already seen in (\ref{eq: bound increment}) and solved
	by (\ref{eq: optimal loss decrease step}).
\end{proof}

Now you might have noticed that we have not used convexity in this section yet.
Well that is not entirely true, we assumed that \(\inf_\weights\Loss(\weights)\)
is a real number which assumes a lower bound and rules out things like
non-convex polynomials.

But since getting stuck in a local minimum would prevent convergence towards the
minimum entirely we can not formulate a convergence theorem without that. So
this changes now.

While Lipschitz continuity of the gradient can be viewed as an upper
bound on the distance between the function and its linear approximation
(Lemma~\ref{lem: Lipschitz Gradient implies taylor inequality}),
convexity can be viewed as a lower bound on this "distance", called the 
the Bergman divergence
%
\begin{align*}
	\bergmanDiv{f}(y,x):= f(y)-f(x)-\langle \nabla f(x), y-x\rangle \ge 0
\end{align*}
%
This lower bound can be sharpened using the Lipschitz continuity of
the gradient.
Borrowing \citeauthor{nesterovLecturesConvexOptimization2018}'s notation
we are going to denote  the set of \(q\) times differentiable convex functions
with an \(\ubound\)-Lipschitz continuous \(p\)-th derivative by
\(\lipGradientSet[q,p]{\ubound}\).
%
\begin{lemma}\label{lem: bermanDiv lower bound}
	Let \(f\in\lipGradientSet{\ubound}\)
	\begin{subequations}
	\begin{align}
		\bergmanDiv{f}(y,x)
		&\ge \tfrac{1}{2\ubound}\|\nabla f(x) - \nabla f(y)\|^2\\
		\label{eq: bergmanDiv lower bound b}
		\langle \nabla f(x) - \nabla f(y), x-y\rangle
		&\ge \tfrac{1}{\ubound}\|\nabla f(x) - \nabla f(y)\|^2
	\end{align}	
	\end{subequations}	
\end{lemma}
\begin{proof}
	Since \(y\mapsto \langle\nabla f(x), y-x\rangle\) is linear, we know that
	%
	\begin{align*}
		\phi(y):=\bergmanDiv{f}(y,x) = f(y)-f(x)-\langle \nabla f(x), y-x\rangle 
	\end{align*}
	%
	still has \(\ubound\)-Lipschitz gradient
	%
	\begin{align}\label{eq: bergman divergence gradient}
		\nabla\phi(y) = \nabla f(y) - \nabla f(x).
	\end{align}
	%
	This gradient is equal to the change of gradient from \(x\) to \(y\) and 
	therefore represents the error "rate" we are making assuming a constant gradient
	and linearly approximating \(f\), which is by definition the derivative of the size of
	\(\bergmanDiv{f}(y,x)\). Now this error "rate" can be translated
 	back into a real error using the stickiness of the gradient due to
	its Lipschitz continuity. I.e. by the previous lemma we know that
	%
	\begin{align*}
		0 = \bergmanDiv{f}(x,x)
		\xeq{f\text{ convex}} \min_z\phi(z)
		\xle{(\ref{eq: min approximation})} \phi(y) - \tfrac{1}{2\ubound}\|\nabla \phi(y)\|^2.
	\end{align*}
	%
	Now (\ref{eq: bergman divergence gradient}) implies our first result
	%
	\begin{align*}
		\tfrac{1}{2\ubound}\|\nabla f(y) - \nabla f(x)\|^2
		\le \phi(y) =\bergmanDiv{f}(y,x).
	\end{align*}
	The second inequality follows from adding \(\bergmanDiv{f}(x,y)\) to
	\(\bergmanDiv{f}(y,x)\).
\end{proof}
\fxnote{
	Should equivalence of these results to \(f\in\lipGradientSet{\ubound}\)
	be mentioned?
}

Now we can state the main convergence result

\begin{theorem}[\cite{nesterovLecturesConvexOptimization2018}]
	\label{thm: convex function GD loss upper bound}
	Let \(\Loss\in\lipGradientSet{\ubound}\), let \(\minimum\) be the unique minimum.
	Then Gradient Decent with learning rate \(0 < \lr < 2/\ubound\) results in
	\begin{align}\label{eq: convex function loss upper bound}
		\Loss(\weights_n) - \Loss(\minimum)
		\le \frac{2\ubound\|\weights_0 - \minimum\|}{4 + n\ubound\lr(2-\ubound\lr)}
	\end{align}
	The optimal rate of convergence 
	\begin{align*}
		\Loss(\weights_n) - \Loss(\minimum)
		\le \frac{2\ubound\|\weights_0-\minimum\|}{4+n}
		\in O\Big(\frac{\ubound\|\weights_0-\minimum\|}{n}\Big)
	\end{align*}
	is achieved for \(\lr=1/\ubound\).
\end{theorem}
\begin{proof}
	While we can not ensure convergence of \(\|\weights_n - \minimum\|\) we can
	ensure it is decreasing using Lemma~\ref{lem: bermanDiv lower bound} and
	\(\nabla \Loss(\minimum)=0\)
	\begin{align}
		\nonumber
		\|\weights_{n+1} - \minimum\|^2
		&= \|\weights_n - \minimum\|^2
		- 2\lr\underbrace{
			\langle \nabla\Loss(\weights_n), \weights_n - \minimum\rangle
		}_{
			\ge \tfrac{1}\ubound \|\nabla \Loss (\weights_n) - \nabla\Loss(\minimum)\|^2
			\mathrlap{\quad \text{using (\ref{eq: bergmanDiv lower bound b})}}
		} + \lr^2\|\nabla \Loss(\weights_n)\|^2
		\\
		&\le \|\weights_n - \minimum\|^2 - 
		\underbrace{\lr}_{>0}\underbrace{(\tfrac{2}{\ubound}-\lr)}_{>0}
		\|\nabla\Loss(\weights_n)\|^2
		\label{eq: decreasing weight difference}
	\end{align}
	Now we can use convexity of the loss function, Cauchy-Schwarz and the
	decreasing weight difference
	%
	\begin{align*}
		\Loss(\weights_n)-\Loss(\minimum)
		\le -\langle \Loss(\weights_n), \minimum-\weights_n\rangle
		\xle{\text{C.S.}} \underbrace{\|\weights_n - \minimum\|}_{\le \|\weights_0 - \minimum\|}
		\|\nabla\Loss(\weights_n)\|
	\end{align*}
	%
	to express our gradient norm in terms of the loss distance
	\begin{align}\label{eq: lower bound gradient size}
		\|\nabla\Loss(\weights)\|
		\ge \frac{\Loss(\weights_n)-\Loss(\minimum)}{\|\weights_0-\minimum\|}.
	\end{align}
	%
	In contrast to (\ref{eq: optimal loss decrease step}) this is a lower bound
	on the gradient which is going to allow us to force a rate of decrease in
	the recursion derived in (\ref{eq: bound increment})
	%
	\begin{align*}
		\overbrace{\Loss(\weights_{n+1}) - \Loss(\minimum)}^{=:\Delta_{n+1}}
		&\le \overbrace{\Loss(\weights_n)-\Loss(\minimum)}^{=:\Delta_n}
		- \overbrace{\lr(1-\lr\ubound/2)}^{=:\omega} \|\nabla \Loss(\weights_n)\|^2\\
		&\lxle{(\ref{eq: lower bound gradient size})} \Delta_n - \tfrac{\omega}{\|\weights_0-\minimum\|^2}\Delta_n^2
	\end{align*}
	%
	To get a simpler recursion we are going to use the fact that the loss delta
	decreases, by dividing both sides by \(\Delta_n\Delta_{n+1}\) which results in
	%
	\begin{align*}
		\frac{1}{\Delta_{n+1}}
		&\ge \frac{1}{\Delta_n}
		+ \frac{\omega}{\|\weights_0-\minimum\|^2}
		\underbrace{\frac{\Delta_n}{\Delta_{n+1}}}_{\ge 1}
		\xge{\text{Induction}} \frac{1}{\Delta_0}
		+ \tfrac{\omega}{\|\weights_0-\minimum\|^2}(n+1)
	\end{align*}
	%
	This can now be transformed into an upper bound
	%
	\begin{align*}
		\Delta_n
		&\le \frac{1}{
			\frac{1}{\Delta_0} + \tfrac{\omega}{\|\weights_0-\minimum\|^2}n
		}
		= \frac{\Delta_0\|\weights_0-\minimum\|^2}{\|\weights_0-\minimum\|^2 + n\omega\Delta_0 }.
	\end{align*}
	%
	The first representation shows that our upper bound is increasing in \(\Delta_0\)
	so an upper bound on \(\Delta_0\)
	%
	\begin{align*}
		\Delta_0 + \Loss(\minimum) = \Loss(\weights_0)
		&\xle{\ref{lem: Lipschitz Gradient implies taylor inequality}}
		\Loss(\minimum) + \langle\nabla\Loss(\minimum), \weights_0 - \minimum\rangle
		+\frac{\ubound}{2}\|\weights_0-\minimum\|^2 \\
		&= \Loss(\minimum) + \tfrac{\ubound}{2}\|\weights_0-\minimum\|^2
	\end{align*}
	%
	immediately yields the upper bound (\ref{eq: convex function loss upper bound}) using the
	definition of \(\omega\)
	%
	\begin{align*}
		\Delta_n
		&\le \frac{\tfrac\ubound{2} \|\weights_0-\minimum\|^4}{
			\|\weights_0-\minimum\|^2
			+ n\omega\tfrac\ubound{2} \|\weights_0-\minimum\|^2
		}
		= \frac{2\ubound \|\weights_0-\minimum\|^2}{
			4 +   n 2\ubound\omega
		}.
		\qedhere
	\end{align*}
\end{proof}

\subsection{Subgradient "Decent"}
\fxnote{More extensive treatment?}{
If we can not have convergence of the parameters \(\weights\) we might not
care as much about the convergence of the gradient either. In that case it
turns out that we can even get convergence of the loss if we only assume
convexity and \(\lipConst\)-Lipschitz continuity of \(\Loss\) itself, i.e.
\begin{align*}
	\Loss \in \lipGradientSet[0,0]{\lipConst}
\end{align*}
%
But this form of Subgradient "Decent"
requires a bounded, convex parameter set we project back into if we leave, and
a decreasing learning rate. And since we can not guarantee a monotonic decrease
with subgradients, the name "decent" is generally avoided and we have to keep
a running minimum. See \textcite[Section 3.2.3]{nesterovLecturesConvexOptimization2018}
or \textcite[Section 3.1]{bubeckConvexOptimizationAlgorithms2015} for a proper
treatment. The convergence rate is not only dependent on the size of the
parameter set \(B\) but it also decreases to
\begin{align*}
	O\Big(\frac{B\lipConst}{\sqrt{n}}\Big).
\end{align*}
}

\subsection{Mirror Decent}

Lipschitz continuity represents an upper bound on the Bergman divergence
\begin{align*}
	\bergmanDiv{f}(y,x)
	= f(y) - f(x) - \langle\nabla f(x), y-x\rangle
	\lxle{\ref{lem: Lipschitz Gradient implies taylor inequality}}
	\tfrac{\ubound}{2} \|y-x\|^2.
\end{align*}
In order to get close to the minimum we use this bound in
(\ref{eq: min approximation}) to get an upper bound
\begin{align*}
	\min_y f(y)
	&= \min_y \{f(x) + \langle f(x), y-x\rangle +\bergmanDiv{f}(y,x)\} \\
	&\le \min_y\{f(x) + \langle f(x), y-x\rangle +\tfrac{\ubound}{2} \|y-x\|^2\}
\end{align*}
As we have shown in Lemma~\ref{lem: smallest upper bound}, gradient decent
minimizes this upper bound. The basic idea of ``Mirror Decent'' is: If we had a
better upper bound on the Bergman divergence, then we could also improve on
gradient decent by taking the minimum of this improved upper bound.


\section{Strong Convexity}\label{sec: Strong Convexity}

To get convergence of the iterate we need to reintroduce a lower bound on
the second derivative. As you might recall from Lemma~\ref{lem: Lipschitz
Gradient implies taylor inequality}, a Lipschitz continuous gradient implies
a quadratic function provides an upper bound for our loss function
%
\begin{align}\label{eq: upper bound}
	\Loss(y) \le \Loss(x) + \langle \nabla \Loss(x), y-x\rangle + \frac\ubound{2}\|y-x\|^2.
\end{align}
%
And convexity was defined as a lower bound
%
\begin{align*}
	\Loss(x) + \langle \nabla \Loss(x), y-x\rangle \le \Loss(y)
\end{align*}
%
which we will now sharpen this to a quadratic lower bound as well.
%
\begin{definition}\label{def: strong convexity}
	A function \(\Loss\) is \emph{strongly convex}, if for all \(x,y\)
	\begin{align*}
		\Loss(x) + \langle \nabla \Loss(x), y-x\rangle + \frac{\lbound}{2}\|y-x\|^2 \le \Loss(y)
	\end{align*}
	And let \(\strongConvex{\lbound}{\ubound}\) denote the \(\lbound\)-strongly convex
	functions form \(\lipGradientSet{\ubound}\).
\end{definition}
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_strong_convexity.pdf_tex}
	\caption{Strong Convexity provides a tighter lower bound}
	\label{fig: visualize strong convexity}
\end{figure}
%
Using this new assumption we get our linear\footnote{
	In the Numerical Analysis, exponential rate of
	convergence is called "linear convergence" because the relationship
	\[
		\|\weights_{n+1} - \minimum\| \le c \|\weights_n - \minimum\|
	\]
	is linear. In
	general convergence order \(q\) implies 
	\[
		\|\weights_{n+1} -\minimum\| \le c\|\weights_n - \minimum\|^q.
	\]
	This is because quadratic convergence (\(q=2\)) already results in an actual
	rate of convergence of \(c^{\left(2^n\right)}\) which is double exponential and
	repeated application of \(\exp\) is annoying to write so writers prefer the
	recursive notation above which motivates this seemingly peculiar naming.
	The convergence rate we obtained in Section~\ref{sec: convex convergence theorems}
	is therefore referred to as "sub-linear" convergence.
}
rate of convergence back, which we had motivated in Section~\ref{sec: visualize gd}. 
%
\begin{theorem}[\cite{nesterovLecturesConvexOptimization2018}]
	If \(\Loss\in\strongConvex{\lbound}{\ubound}\), then for learning rate
	\[0 \le \lr \le \tfrac{2}{\ubound+\lbound}\]
	Gradient Decent generates a sequence \(\weights_n\) satisfying
	\begin{subequations}
	\begin{align}
		\label{eq: gd strong convexity convergence rate 1}
		\|\weights_n - \minimum\|
		&\le \left(
			1- 2\lr\frac{\ubound\lbound}{\ubound+\lbound}
		\right)^{n/2}
		\|\weights_0 - \minimum\| \\
		\label{eq: gd strong convexity convergence rate 2}
		\Loss(\weights_n) - \Loss(\minimum)
		&\le \frac\ubound{2} \left(
			1- 2\lr\frac{\ubound\lbound}{\ubound+\lbound}
		\right)^{n}
		\|\weights_0 - \minimum\|^2
	\end{align}
	\end{subequations}
	In particular for \(\lr=\tfrac2{\ubound+\lbound}\) we achieve the optimal
	rate of convergence
	\begin{subequations}\label{eq: gd strong convexity optimal rate}
	\begin{align}
		\|\weights_n - \minimum\|
		&\le \left(
			1- \frac{2}{1+\condition}
		\right)^n
		\|\weights_0 - \minimum\| \\
		\Loss(\weights_n) - \Loss(\minimum)
		&\le \frac\ubound{2} \left(
			1- \frac{2}{1+\condition}
		\right)^{2n}
		\|\weights_0 - \minimum\|^2
	\end{align}
	\end{subequations}
	where \(\condition=\ubound/\lbound\) is the condition number.
\end{theorem}
\begin{proof}[Proof {\parencite[Theorem 2.1.15]{nesterovLecturesConvexOptimization2018}}]
	This proof starts like the proof for convex functions, cf. (\ref{eq:
	decreasing weight difference}):
 \begin{align*}
		\|\weights_{n+1} - \minimum\|^2
		&= \| \weights_n -\minimum - \lr\nabla\Loss(\weights_n)\|^2\\
		&= \| \weights_n - \minimum\|^2
		- 2\lr\langle \nabla\Loss(\weights_n), \weights_n - \minimum\rangle
		+ \lr^2 \| \nabla\Loss(\weights_n)\|^2
	\end{align*}
	But now we need to find a better lower bound for the scalar product in the
	middle to obtain a significant decrease. Previously we used the lower bound
	on the Bergman Divergence from Lemma~\ref{lem: bermanDiv lower bound}, now
	we have a lower bound from the strong convexity property. But just as using
	convexity directly in the previous theorem was too weak and had to be
	improved with Lemma~\ref{lem: bermanDiv lower bound}, we are going to use
	an improved lower bound again:
 	\begin{align*}
		&\langle \nabla\Loss(\weights_n) -\nabla\Loss(\minimum), \weights_n - \minimum\rangle	\\
		&\ge \tfrac{\ubound\lbound}{\ubound+\lbound}\|\weights_n - \minimum\|^2
		+ \tfrac{1}{\ubound+\lbound}
		\|\nabla\Loss(\weights_n) - \nabla\Loss(\minimum)\|^2
	\end{align*}
	Using this lower bound, which we will prove in Lemma~\ref{lem: bermanDiv
	lower bound (strongly convex)}, we get
 \begin{align*}
		\|\weights_{n+1} - \minimum\|^2
		\le \left(1- 2\lr \tfrac{\ubound\lbound}{\ubound+\lbound}\right)
		\|\weights_n - \minimum\|^2
		+ \underbrace{\lr}_{\ge 0}
		\underbrace{\left(\lr-\tfrac{2}{\ubound+\lbound}\right)}_{\le0}
		\|\nabla\Loss(\weights_n)\|^2
	\end{align*}
	which immediately proves (\ref{eq: gd strong convexity convergence rate
	1}) using induction. (\ref{eq: gd strong convexity convergence rate 2}) follows from our
	upper bound (\ref{eq: upper bound}), i.e. Lipschitz continuity of the gradient:
	\begin{align*}
		\Loss(\weights_n) - \Loss(\minimum)
		\le \langle \underbrace{\nabla\Loss(\minimum)}_{=0}, \weights_n - \minimum\rangle
		+ \tfrac\ubound{2} \| \weights_n - \minimum\|^2
	\end{align*}
	To get (\ref{eq: gd strong convexity optimal rate}) we simply have to plug
	in the presumed optimal learning rate \(\lr=2/(\ubound+\lbound)\) into
	our general rate and show that the rate is still positive which means that
	we have not overshot zero, which means that the upper bound on our learning
	rate which we presume to be optimal is binding:
	\begin{align*}
		\left(1- 4 \frac{\ubound\lbound}{(\ubound+\lbound)^2}\right)
		= \frac{(\ubound^2 + 2\ubound\lbound + \lbound^2) - 4 \ubound\lbound}{(\ubound+\lbound)^2}
		= \left(\frac{\ubound-\lbound}{\ubound+\lbound}\right)^2
	\end{align*}
	To get the representation in (\ref{eq: gd strong convexity optimal rate}) we
	only have to add and subtract one inside the bracket and cancel out
	\(\lbound\) from enumerator and denominator.
\end{proof}

Now as promised we still have to show the improved lower bound.

\begin{lemma}
	\label{lem: bermanDiv lower bound (strongly convex)}
	If \(f\in\strongConvex{\lbound}{\ubound}\), then for any
	\(x,y\in\reals^\dimension\) we have
	\begin{align*}
		\langle \nabla f(x) - \nabla f(y), x-y\rangle 
		\ge \tfrac{\lbound\ubound}{\lbound+\ubound} \| x-y\|^2
		+ \tfrac{1}{\lbound+\ubound}\|\nabla f(x) -\nabla f(y)\|^2
	\end{align*}
\end{lemma}
\begin{proof}
	Similarly to Lemma~\ref{lem: bermanDiv lower bound} we want to
	add \(\bergmanDiv{f}(x,y)\) and \(\bergmanDiv{f}(y,x)\) together to
	get a lower bound for the scalar product equal to this sum. But since we
	already have a lower bound due to strong convexity
	\begin{align}\label{eq: strong convexity implies bergmanDiv lower bound}
		\bergmanDiv{f}(y,x) = f(y) - f(x) -\langle\nabla f(x), y-x\rangle
		\ge \tfrac{\lbound}2 \|y-x\|^2,
	\end{align}
	we first have to "remove" this lower bound from \(f\) to apply our other
	lower bound. So we define
	\begin{align*}
		g_x(y) := f(y) - \tfrac{\lbound}2\|y-x\|^2 \qquad
		\nabla g_x(y) = \nabla f(y) - \lbound(y-x)
	\end{align*}
	which still has positive Bergman Convergence, i.e. is convex
	\begin{align*}
		\bergmanDiv{g_x}(y,z)
		&= g_x(y) - g_x(z) - \langle\nabla g_x(z), y-z\rangle \\
		&= 
		\begin{aligned}[t]
			& f(y) - f(z) -\langle\nabla f(z), y-z\rangle \\
			&- \tfrac\lbound{2}\underbrace{\|y-x\|^2}_{
				=\|y-z\|^2 + \mathrlap{2\langle y-z, z-x\rangle + \|z-x\|^2}
			}
			+ \tfrac\lbound{2}\|z-x\|^2
			+ \lbound\langle z-x, y-z\rangle
		\end{aligned}\\
		&= \bergmanDiv{f}(y,z) - \tfrac{\lbound}2\|y-z\|^2
		\xge{(\ref{eq: strong convexity implies bergmanDiv lower bound})} 0
	\end{align*}
	and has \((\ubound-\lbound)\)-Lipschitz continuous gradient by
	Lemma~\ref{lem: Lipschitz Gradient implies taylor inequality} and 
	\begin{align}
		\label{eq: bergmanDiv upper bound}
		\bergmanDiv{f}(y,z) \le \tfrac{\ubound}2\|y-z\|^2.
	\end{align}
	The inequality above also follows from Lemma~\ref{lem: Lipschitz Gradient
	implies taylor inequality}. Therefore
	\(g_x\in\lipGradientSet{\ubound-\lbound}\) and if \(\ubound -\lbound>0\) we
	can now apply Lemma~\ref{lem: bermanDiv lower bound} to get
	\begin{align}
		\bergmanDiv{f}(y,x)
		&= \bergmanDiv{g_x}(y,x) + \tfrac{\lbound}2 \|y-x\|^2
		\nonumber \\
		&\ge \tfrac{1}{2(\ubound-\lbound)} \|\nabla g_x(x) - \nabla g_x(y)\|^2
		+ \tfrac{\lbound}2 \|y-x\|^2 
		\nonumber \\
		\label{eq: improved lower bound strong convex bergman Divergence}
		&\ge \tfrac{1}{2(\ubound-\lbound)}
		\|\nabla f(x) - \lbound x - (\nabla f(y)-\lbound y)\|^2
		+ \tfrac{\lbound}2 \|y-x\|^2.
	\end{align}
	Since the \(\|y-x\|^2\) lower bound improves the convergence rate in
	our convergence theorem, it is helpful to view it as the "good" part of
	our lower bound and view the application of the gradient lower bound as
	a fallback.

	The case \(\ubound=\lbound\) which we have to cover separately offers some
	more insight into that. In this case our lower bound  (\ref{eq:
	strong convexity implies bergmanDiv lower bound}) makes (\ref{eq: bergmanDiv
	upper bound}) an equality. This means we could get
	\begin{align*}
		\langle \nabla f(x) - \nabla f(y), x-y\rangle
		= \bergmanDiv{f}(x,y) + \bergmanDiv{f}(y,x) = \lbound \|y-x\|^2.
	\end{align*}
	But instead we apply Lemma~\ref{lem: bermanDiv lower bound} to half of it
	which results in the statement of this lemma.

	Now we just have to finish the case \(\ubound>\lbound\). In equation
	(\ref{eq: improved lower bound strong convex bergman Divergence}) we have
	already removed the dependence on our helper function \(g_x\) and will now
	use its symmetry to add together the mirrored Bergman Divergences to get
	\begin{align*}
		&\langle \nabla f(x) - \nabla f(y), x-y\rangle
		= \bergmanDiv{f}(x,y) + \bergmanDiv{f}(y,x) \\
		&\ge \tfrac{1}{\ubound-\lbound}
		\underbrace{\|\nabla f(x) - \lbound x - (\nabla f(y)-\lbound y)\|^2}_{
			=\|\nabla f(x) - \nabla f(y)\|^2
			- 2\lbound \langle \nabla f(x) - \nabla f(y), x-y\rangle
			\mathrlap{+ \lbound^2 \| x-y\|^2}
		}
		+ \lbound \|y-x\|^2.
	\end{align*}
	Moving the scalar product to the left we get
	\begin{align*}
		\overbrace{(\ubound+\lbound)/(\ubound-\lbound)}^{
			= (1+\tfrac{2\lbound}{\ubound-\lbound})
		}
		&\langle \nabla f(x) - \nabla f(y), x-y\rangle \\
		&\ge \tfrac{1}{\ubound-\lbound}\|\nabla f(x) - \nabla f(y)\|^2
		+ \underbrace{(\tfrac{\lbound^2}{\ubound-\lbound}-\lbound)}_{
			=\ubound\lbound/(\ubound-\lbound)
		} \|y-x\|^2.
	\end{align*}
	Dividing by the factor on the left finishes this proof.
 \end{proof}

While it is not too surprising that we recovered the learning rate from
Section~\ref{sec: visualize gd} as we simply provided lower and upper bounds on
the eigenvalues, it is still reassuring. 

\section{Complexity Bounds}

Now that we found convergence rates for gradient decent on (strongly) convex
problems, the question is can we do better? I.e. how fast could an algorithm we
could possibly think of be? To tackle this problem let us first make an
assumption about what this algorithm can do.
Unrolling our gradient decent algorithm
%
\begin{align*}
	\weights_n = \weights_0 - \lr\sum_{k=0}^{n-1} \nabla \Loss(\weights_k)
\end{align*}
%
we can see that even if we allowed custom learning rates for every iteration
%
\begin{align*}
	\weights_n = \weights_0 - \sum_{k=0}^{n-1} \lr_k \nabla \Loss(\weights_k)
\end{align*}
%
we would still end up in the linear span of all gradients, shifted by \(\weights_0\).
And since we are in the class of first order optimization methods where we are
only provided with the function evaluation itself and the gradient, an obvious
assumption for a class of optimization methods would be
%
\begin{assumption}[\citeauthor{nesterovLecturesConvexOptimization2018}]
	\label{assmpt: parameter in linear hull of gradients}
	The \(n\)-th iterate of the optimization method is contained in the span of all
	previous gradients shifted by the initial starting point
	\begin{align*}
		\weights_n \in \linSpan\{\nabla \Loss(\weights_k) : 0\le k \le n-1\} + \weights_0
	\end{align*}
\end{assumption}
%
Now the question becomes: How can we utilize this assumption to construct a
function which is difficult to optimize?
\textcite{gohWhyMomentumReally2017} provides an intuitive interpretation for a loss
function taken from \textcite[Section 2.1.2]{nesterovLecturesConvexOptimization2018}
which I further modify into the following example.

\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_coloring_problem.pdf_tex}
	\caption{Even after \(\dimension\) steps (when all tiles have a non-zero value)
	gradient decent is still far off from the solution even using the optimal
	learning rate \(1/\ubound\)}
	\label{fig: visualize coloring problem}
\end{figure}
%
Consider a cold, zero temperature, 1-dimensional rod which is heated from one side
by an infinite heat source at temperature one. Then the second segment of the rod
will only start to get hot once the first segment of it has heated up. Similarly
the \(n\)-th segment will only increase in temperature once the \((n-1)\)-th segment
is no longer at zero temperature. If the heat transfer would only depend on the
current heat level difference, one could thus express a simplified heat level
recursion as follows\footnote{
	Taking first the learning rate to zero and afterwards the space discretization, would
	result in the well known differential equation for heat with boundary
	conditions.
}: 
%
\begin{align*}
	\weights_{n+1}^{(i)}
	&= \weights_{n}^{(i)}
	+ \lr 
	\begin{cases}
		[\weights_{n}^{(i-1)} - \weights_{n}^{(i)}] + [\weights_{n}^{(i+1)} - \weights_{n}^{(i)}]
		&  1 < i < \dimension \\
		[1 - \weights_{n}^{(1)}] + [\weights_{n}^{(2)} - \weights_{n}^{(1)}]
		& i = 1 \\
		[\weights_{n}^{(i-1)} - \weights_{n}^{(i)}]
		& i = \dimension
	\end{cases}
\end{align*}
%
For the loss function
%
\begin{align}\label{eq: naive complexity counterexample loss}
	\tilde{\Loss}(\weights)
	= \frac12 (\weights^{(1)}-1)^2
	+ \frac12 \sum_{k=1}^{\dimension-1} (\weights^{(k)}-\weights^{(k+1)})^2
\end{align}
%
this is just the gradient decent recursion. Now here is the crucial insight:

Since any weight (parameter component) is only affected once its neighbors are
affected, the second weight can not be affected before the second step since
the first weight is only heated in the first step. And since the second weight
will be unaffected until the second step, the third weight will be unaffected
until the third step, etc.

Therefore the \(\dimension - n\) last components will still be zero in
step \(n\), because the linear span of all the gradients so far is still
contained in the subspace \(\reals^n\) of \(\reals^\dimension\). Formalizing
this argument results in the following theorem inspired by \textcite[Theorem
2.1.7]{nesterovLecturesConvexOptimization2018} and \textcite{gohWhyMomentumReally2017}.

\begin{remark}
	This type of loss function is not unlikely to be encountered in real
	problems. E.g. in reinforcement learning with sparse rewards the estimation
	of the value function over states requires the back-propagation of this
	reward through the states that lead up to it. "Eligibility traces"
	\parencite[Chapter 12]{suttonReinforcementLearningIntroduction2018}
	are an attempt to speed this process up.
\end{remark}
\begin{theorem}\label{thm: convex function complexity bound}
	For any \(\weights_0\in\reals^d\), any \(n\) such that 
	\[0\le n\le \tfrac12 (d-1),\]
	there exists \(\Loss\in\lipGradientSet[\infty,1]{\ubound}\)
	s.t. for any first order method \(\firstOrderMethod\)
	satisfying Assumption~\ref{assmpt: parameter in linear hull of gradients}
	we have
	\begin{subequations}
	\begin{align}
		\Loss(\weights_n) - \Loss(\minimum)
		&\ge \frac{\ubound \|\weights_0 - \minimum\|^2}{16(n+1)^2} \\
		\|\weights_k -\minimum\|^2 
		&\ge \frac13 \|\weights_0 - \minimum\|^2
	\end{align}
	\end{subequations}
	where \(\minimum = \arg\min_\weights \Loss(\weights)\) is the unique minimum.
\end{theorem}
\begin{proof}
	First, since we could always define
	\(\Loss(\weights):=\tilde{\Loss}(\weights-\weights_0)\)
	we can assume without loss of generality that \(\weights_0=\mathbf{0}\). 
	
	Second, we can define \(\tilde{\dimension} := 2n +1 \le \dimension	\)	
	and interpret \(\weights_0 = \mathbf{0} \in\reals^\dimension\) as an element of
	\(\reals^{\tilde{\dimension}}\). If our constructed loss simply
	ignores the dimensions between \(\tilde{\dimension}\) and \(\dimension\) then
	this part of the gradients will then be zero keeping all \(\weights_n\) in
	\(\reals^{\tilde{\dimension}}\). We can therefore assume without loss of
	generality that 
	\begin{align}\label{eq: w.l.o.g dim=2n+1}
		\dimension=2n+1.
	\end{align}

	Third, we want to make sure that our loss function is actually convex. To
	see this notice that we can write the gradient as
	%
	\begin{align}\label{eq: naive complexity example loss gradient}
		\nabla \tilde{\Loss}(\weights)
		= \graphLaplacian\weights + (\weights^{(1)} -1) \stdBasis_1
		= (\graphLaplacian + \stdBasis_1 \stdBasis_1^T)\weights - \stdBasis_1
	\end{align}
	%
	where \(\stdBasis_1=(1,0,\dots,0)^T\) is the first standard basis
	vector and \(\graphLaplacian\) is the "Graph Laplacian"\footnote{
		It might seem a bit excessive to introduce the concept of graphs when
		we could have simply calculated the derivative by hand to get the same
		result. But this graph view is very suggestive how one would generalize
		this "heat spread" problem to more complicated structures than a
		1-dimensional rod. In particular the connectedness of graphs is related
		to its condition number \parencite{gohWhyMomentumReally2017}. And this
		provides some more intuition for how "worst case problems" look like.
	} for the 
	\fxnote{tikz graph}{undirected graph} \(\graph=(\vertices, \edges)\) with 
	\begin{align*}
		\vertices = \{1,\dots, \dimension\}
		\qquad
		\edges = \{(i, i+1) : 1\le i \le \dimension-1\}
	\end{align*}
	%
	\begin{align*}
		\graphLaplacian^{(i,j)} 
		&= 
		\begin{cases}
			[\text{degree of vertex i}] & i=j\\
			-1 & (i,j)\in\edges \text{ or } (j,i)\in\edges\\
			0 & \text{ else}
		\end{cases}
		\\
		&=
		\begin{pmatrix*}[r]
			1 & -1 & 0  & \cdots & \cdots & 0 \\
			-1 & 2 & -1 & \ddots &  &  \vdots \\ 
			0 & -1 & 2 & \ddots & \ddots & \vdots \\
			\vdots & \ddots & \ddots & \ddots & -1 & 0 \\
			\vdots &  & \ddots & -1 & 2 & -1 \\
			0 & \cdots & \cdots & 0  & -1 & 1
		\end{pmatrix*}
	\end{align*}
	%
	This means our loss function is quadratic which means that our
	hesse matrix \(\nabla \tilde{\Loss}(\weights)\) is a constant \(H\). And it is
	positive definite because
	%
	\begin{align*}
		\langle \weights , H \weights\rangle
		&= \langle \weights, (A_G + \stdBasis_1 \stdBasis_1^T) \weights \rangle
		= \langle \weights, \stdBasis_1\rangle^2
		+ \langle \weights, A_G \weights\rangle
		\\
		&= (\weights^{(1)})^2
		+ \sum_{k=1}^{\dimension-1}(\weights^{(k)}-\weights^{(k+1)})^2
		\ge 0.
	\end{align*}
	%
	The last equality can be verified by taking the derivative of both and
	realizing they are the same, which is sufficient because they are both zero	
	in the origin. We simply used the fact that our original loss function
	was essentially already a quadratic function except for the constant influx
	at the first tile. And that constant in the derivative has no bearing on the
	hesse matrix.


	To calculate the operator norm of \(H\) we can use the equality above and	
	\((a-b)^2 \le 2(a^2 + b^2)\) to get
	\begin{align*}
		\langle \weights, H\weights\rangle
		\le (\weights^{(1)})^2 + 2\sum_{k=1}^{d-1}(\weights^{(k)})^2 + (\weights^{(k+1)})^2 
		\le 4 \sum_{k=1}^{d-1} (\weights^{(k)})^2
		= 4 \|\weights\|^2
	\end{align*}
	Which immediately implies that the largest eigenvalue is smaller than 4. To
	obtain a convex function with a Lipschitz continuos gradient with constant
	\(\ubound\), we can now simply define
	\begin{align*}
		\Loss(\weights):= \frac{\ubound}{4}\tilde{\Loss}(\weights)
		= \frac\ubound{8}\left[
			(\weights^{(1)}-1)^2
			+ \sum_{k=1}^{\dimension-1} (\weights^{(k)}-\weights^{(k+1)})^2
		\right].
	\end{align*}
	Now we know that \(\minimum = (1,\dots,1)^T\) achieves a loss of zero which is
	the unique minimum as the loss function is positive and convex. Since we
	assumed w.l.o.g. that \(\weights_0=\mathbf{0}\) we get
	%
	\begin{align}\label{eq: initial distance}
		\|\minimum - \weights_0\|^2 = \dimension
	\end{align}
	%
	We also know
	that Assumption~\ref{assmpt: parameter in linear hull of gradients} implies
	%
	\begin{align*}
		\weights_n \in \linSpan\{\nabla\Loss(\weights_k): 0 \le k \le n-1\}
		\subseteq \reals^n \subseteq \reals^\dimension
	\end{align*}
	%
	which immediately results in the second claim of the theorem
	\begin{align*}
		\|\minimum - \weights_n\|^2
		&\ge \sum_{k=n+1}^d (\minimum^{(k)}- \weights_n^{(k)})^2
		= d-(n+1) \\
		&= n = \tfrac{n}{2n+1} d
		\ge \tfrac13 \|\minimum - \weights_0\|^2. 
	\end{align*}
	Where we have assumed without loss of generality that \(n\ge1\) in the last
	inequality. The case \(n=0\) is obvious.

	To get the first bound on the loss function we have to be a bit more subtle.
	On \(\reals^n\subseteq \reals^\dimension\) the modified loss function
	%
	\begin{align}\label{eq: sink loss}
		\Loss_n(\weights) := \frac\ubound{8}\left[
			(\weights^{(1)} -1)^2
			+ (\weights^{(n)} - 0)^2
			+ \sum_{k=1}^{n-1} (\weights^{(k)} - \weights^{(k+1)})^2
		\right]
	\end{align}
	%
	is equal to \(\Loss(\weights)\) because \(\weights^{(i)}=0\) for \(i>n\).
	Therefore
	%
	\begin{align*}
		\Loss(\weights_n) - \inf_{\weights}\Loss(\weights)
		\ge \Loss\restriction_{\reals^n}(\weights_n) = \Loss_n(\weights_n)
		\ge \inf_{\weights}\Loss_n(\weights).
	\end{align*}
	%
	Having a closer look at \(\Loss_n\) will therefore give us a lower bound
	without having to know anything about \(\firstOrderMethod\). Since \(\Loss_n\)
	is similarly convex, setting its derivative equal to zero will actually provide us
	with its minimum. And similarly to \(\Loss\) itself, \(\Loss_n\) is a
	quadratic function, so the gradient is just an affine function with a
	constant hesse matrix. Therefore finding the minimum only requires solving
	a linear equation. One can just go through the same motions as we did
	before to obtain the equation:
	%
	\begin{align}\label{eq: optimality condition for sink loss}
		A_n \weights - \stdBasis_1 \xeq{!} 0 \qquad 
		A_n =\begin{cases}
			2 & i=j\le n \\
			-1 & i=j+1\le n \text{ or } j=i+1\le n \\
			0 & \text{else}
		\end{cases} 
	\end{align}
	%
	But at least for solving it, it helps to have an intuition what \(\Loss_n\)
	actually represents. Recall \(\Loss\) represents just
	a single heat source at interval \(\weights^{(1)}\) at constant
	temperature one which slowly heats up our rod to this level. Now
	\(\Loss_n\) not only has a source, it also has a sink with constant
	temperature zero at \(n\) which cools down the rod from this other side.

	It is therefore quite intuitive that the equilibrium solution
	\(\hat{\weights}_n\) (optimal solution for \(\Loss_n\)) should be a linearly
	decreasing slope
	%
	\begin{align*}
		\hat{\weights}_n^{(i)} = \begin{cases}
			1 - \tfrac{i}{n+1} & i \le n+1\\
			0	& i \ge n+1
		\end{cases}
	\end{align*}	
	%
	And this is in fact the solution of our linear equation (\ref{eq: optimality
	condition for sink loss}) as can be verified by calculation. Now we just
	have to plug this into our loss function
	%
	\begin{align*}
		&\Loss(\weights_n)-\inf_\weights\Loss(\weights)
		\ge \Loss(\hat{\weights}_n)\\
		&= \frac{\ubound}{8}\left[
			\left(-\tfrac1{n+1}\right)^2 + \left(1-\tfrac{n}{n+1}\right)^2
			+ \sum_{k=1}^{n-1}\left(\tfrac{k+1}{n+1}-\tfrac{k}{n+1}\right)^2
		\right]\\
		&= \frac{\ubound}{8}\sum_{k=0}^n \tfrac{1}{(n+1)^2}
		=\frac{\ubound}{8(n+1)}
		\xeq{(\ref{eq: initial distance})} \frac{L\|\weights_0 - \minimum\|^2}{8(n+1)d}\\
		&\lxge{(\ref{eq: w.l.o.g dim=2n+1})}
		\frac{L\|\weights_0 - \minimum\|^2}{16(n+1)^2}
		\qedhere
	\end{align*}
\end{proof}

\subsection{Complexity Bounds for Strongly Convex Losses}

For a strongly convex loss we simply add a regularization term:
%
\begin{align*}
	\Loss(\weights)
	&= \frac{\ubound -\lbound}{8} \left[
		(\weights^{(1)}-1)^2
		+ \sum_{k=1}^{\dimension-1} (\weights^{(k)}-\weights^{(k+1)})^2
	\right]
	+ \frac\lbound{2} \| \weights \|^2\\
	&= \frac{\ubound - \lbound}{4} \tilde{\Loss}(\weights)
	+ \frac{\lbound}{2}\| \weights \|^2
\end{align*}
%
From the proof of Theorem~\ref{thm: convex function complexity bound} we know
about \(\tilde{\Loss}\) that
%
\begin{align*}
	0 \precsim \nabla^2\tilde{\Loss} \precsim 4\identity.
\end{align*}
%
Therefore we know that \(\Loss\in\strongConvex[\infty,1]{\lbound}{\ubound}\) because
%
\begin{align*}
	\lbound\identity
	\precsim \nabla^2\Loss
	&= \frac{\ubound - \lbound}{4} \nabla^2 \tilde{\Loss} + \lbound\identity\\
	&\precsim (\ubound - \lbound + \lbound)\identity = \ubound \identity.
\end{align*}
%
Using \(\lbound(\condition -1)=\ubound-\lbound\) and (\ref{eq: naive complexity
example loss gradient}) we can set the gradient to zero
\begin{align*}
	0 &\lxeq{!} \nabla \Loss(\weights)
	=  \frac{\lbound(\condition -1)}{4} \nabla\tilde{\Loss}(\weights) - \lbound \weights\\
	&= \left[
		\frac{\lbound(\condition-1)}{4}(A_G + \stdBasis_1\stdBasis_1^T) - \lbound\identity
	\right]\weights - \frac{\lbound(\condition-1)}{4}\stdBasis_1.
\end{align*}
%
Assuming \(\condition>1\) (\(\condition=1\) implies \(\Loss=\lbound\|\cdot\|^2\) and
\(\minimum=\mathbf{0}\)),
this condition can be rewritten as
%
\begin{align*}
	0 =  \left[
		A_G + \stdBasis_1\stdBasis_1^T + \frac{4}{\condition-1}\identity
	\right] - \stdBasis_1
	\weights.
\end{align*}
%
All entries on the diagonal except for the last dimension (which only has one
connection and no source or sink) are equal to
%
\begin{align*}
	2+\frac{4}{\condition-1} = 2\frac{\condition +1}{\condition-1}
\end{align*}
%
This results in the system of equations
%
\begin{subequations}
\label{eq: solution to the strongly convex coloring problem}
\begin{align*}
	0&=2\frac{\condition+1}{\condition -1}\weights^{(1)} - \weights^{(2)} -1 \\
	0&=2\frac{\condition+1}{\condition -1}\weights^{(i)}
	- \weights^{(i+1)} - \weights^{(i-1)}  && 2\le i <d \\
	0&= \left(2\frac{\condition+1}{\condition -1} -1 \right)\weights^{(d)}
	- \weights^{(d-1)}
\end{align*}
\end{subequations}
Defining \(\weights^{(0)} =1\) we can unify the first two equations (flipping
the sign)
\begin{align*}
	0&= 
	\frac{\weights^{(i+1)}}{\weights^{(i)}}\frac{\weights^{(i)}}{\weights^{(i-1)}}
	- 2\frac{\condition+1}{\condition -1}\frac{\weights^{(i)}}{\weights^{(i-1)}}
	+ 1  && i <d
\end{align*}
%
Which means that for a solution \(\) of the quadratic equation
\begin{align}\label{eq: coloring solution quadratic equation}
	0&= x^2 - 2\frac{\condition+1}{\condition -1}x + 1,
\end{align}
%
\(\weights^{(i)} = x^i\) will be a solution to the first \(d-1\) equations.
Unfortunately neither of the solutions
%
\begin{align*}
	x_{1/2} &= \frac{\condition+1}{\condition -1} \pm 
	\sqrt{\left(\tfrac{\condition+1}{\condition-1}\right)^2 -1}
	=\frac{\condition +1 \pm \sqrt{4\condition}}{\condition -1}\\
	&= \frac{(\sqrt{\condition}\pm 1)^2}{(\sqrt{\condition}-1)(\sqrt{\condition}+1)}
\end{align*}
%
is a solution to the last equation. Which is why we are going to formulate the
following theorem only for \(\dimension=\infty\)\footnote{
	For large dimensions the real solution seems to be almost
	indistinguishable from the limiting case (e.g. \(\dimension=40,
	\condition=10\) results in a maximal error of the order \(10^{-11}\). The
	error in the first 35 dimensions is even of the order \(10^{-13}\))
}.
We formalize \(\dimension=\infty\) as the sequence space with euclidean norm
\begin{align*}
	\sequenceSpace := \Big\{
		f:\naturals \to \reals : \| f\|^2 = \sum_{k\in\naturals} f(k)^2< \infty
	\Big\}.
\end{align*}
%
\begin{theorem}[{\cite[Theorem 2.1.13]{nesterovLecturesConvexOptimization2018}}]
	\label{thm: strong convexity complexity bound}
	For any \(\weights_0\in\sequenceSpace\) there exists
	\(\Loss\in\strongConvex[\infty,1]{\lbound}{\ubound}\) such that for any
	first order method \(\firstOrderMethod\) satisfying Assumption~\ref{assmpt:
	parameter in linear hull of gradients}, we have
	\begin{subequations}
	\begin{align}
		\|\weights_n - \minimum\|
		&\ge \left(1-\frac2{1+\condition}\right)^n \|\weights_0 - \minimum\| \\
		\Loss(\weights_n) - \Loss(\minimum)
		&\ge \tfrac{\lbound}{2}
		\left(1-\frac2{1+\condition}\right)^{2n} \|\weights_0 - \minimum\|^2
	\end{align}
	\end{subequations}
	where \(\minimum\) is the unique minimum of \(\Loss\).
\end{theorem}
\begin{proof}
	We can again assume without loss of generality that \(\weights_0=\mathbf{0}\).
	Of the two solutions to the linear equations (\ref{eq: solution to the
	strongly convex coloring problem}) induced by the two solutions \(q_{1/2}\)
	of (\ref{eq: coloring solution quadratic equation}) only
	\begin{align*}
		\minimum^{(i)} = \left(\frac{\sqrt{\condition}-1}{\sqrt{\condition}+1}\right)^i
		= \left(1 - \frac{2}{1+\sqrt{\condition}}\right)^i =: q^i
	\end{align*}
	is an element of \(\sequenceSpace\) and therefore the unique minimum. Since 
	\(\weights_n\) stays in the subspace \(\reals^n\) by assumption we know
	\begin{align*}
		\|\weights_n -\minimum\|^2
		\ge \sum_{k=n+1}^\infty (\minimum^{(k)})^2
		= q^{2n} \sum_{k=0}^\infty q^{2k}
		= q^{2n} \| \minimum\|^2
		= q^{2n} \|\weights_0 - \minimum\|^2.
	\end{align*}
	Taking the root results in the first claim. The second claim immediately
	follows from the definition of strong convexity (Def~\ref{def: strong convexity})
	and \(\nabla\Loss(\minimum) = 0\):
	\begin{align*}
		\Loss(\weights_n) - \Loss(\minimum)
		&\ge \langle \nabla\Loss(\minimum), \weights_n -\minimum\rangle
		+\tfrac\lbound{2} \|\weights_n -\minimum\|^2
		\qedhere
	\end{align*}
\end{proof}

In view of Lemma~\ref{lem: smallest upper bound} which states that gradient
decent is in some sense optimal (minimizes the upper bound induced by
Lipschitz continuity of the gradient), it might be surprising how far away
Gradient Decent is from these lower bounds. Maybe these lower bounds are not
tight enough? This is not the case. In the next chapter we will get to know
a family of methods which do achieve these rates of convergence.

So can we
explain the Lemma? Well, first of all it is of course just an upper bound we
are minimizing. But the more important issue is that we are taking a very
local perspective. We only care about minimizing the next loss, using 
nothing but the current gradient. In Assumption~\ref{assmpt: parameter in
linear hull of gradients} on the other hand, we consider the linear span of all
gradients so far. This distinction is important as it will make the
"momentum method" we will discuss next seem much more obvious.

Lastly since this generalization of assumptions improved our rate of convergence
we might ask if we could generalize our assumption further. Heuristics like
the well known Adagrad, Adadelta and RMSProp use different learning rates for
different components of the gradients for example. Their iterates can thus
be expressed by
%
\begin{align*}
	\weights_n = \weights_0 + \sum_{k=0}^{n-1} H_k \nabla\Loss(\weights_k)
\end{align*}
%
where \(H_k\) are diagonal matrices. But at least upper triangle matrices do
not change the fact that \(\weights_n\) is contained in subspace \(\reals^n\).
Since we only use this fact, one could thus generalize Assumption~\ref{assmpt:
parameter in linear hull of gradients} to
%
\begin{assumption}\label{assmpt: parameter in generalized linear hull of gradients}
	The \(n\)-th iterate of the optimization method can be expressed as
	\begin{align*}
		\weights_n = \weights_0 + \sum_{k=0}^{n-1} H_k \nabla\Loss(\weights_k)
	\end{align*}
	where \(H_k\) are upper triangular matrices.
\end{assumption}

Both Complexity Bounds continue to hold with this Assumption in place of
Assumption~\ref{assmpt: parameter in linear hull of gradients}. And since one
could just switch the relation \((\weights_n-\weights_{n+1})^2\) to
\((\weights_n+\weights_{n+1})^2\) to force a flip from positive to negative
at this point, it is unclear how one should extrapolate to components not
available in any gradient. But making further assumptions like continuity of
the underlying function \(\model\) inducing \(\Loss(\weights)\) it might
be possible to find better bounds.

\section{Heuristics}

\subsection{Adagrad}

\subsection{Adadelta}

\subsection{RMSProp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput