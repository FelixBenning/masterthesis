% !TEX root = ../Masterthesis.tex

\newcommand{\momentum}{p}
\newcommand{\friction}{\mu}

\chapter{Momentum}

To understand why the convergence rate is poor when the condition
number is high, we can visualize a high ratio of the lowest to the highest
eigenvalue as a narrow ravine. The gradient points in the direction of the
strongest decent which is mostly opposite of the ravine and only slightly
along its length. This causes our iterate to bounce back and forth between
the walls of the ravine.
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_bad_contitioning.pdf_tex}
	\caption{Momentum reduces fluctuations and converges faster}
	\label{fig: visualize bad conditioning}
\end{figure}

As a fix it seems appropriate to average the gradients in some sense, to
cancel out the opposing jumps and go straight down the ravine. In other words
we want to build momentum. Now if we move according to the sum (integral) of
the gradients, then our velocity stops being equal to the gradient but instead
becomes the antiderivative of the gradient.

So instead of setting our gradient equal to the velocity like in (\ref{eq:
velocity is gradient}), we want to set the acceleration equal to our gradient
%
\begin{align*}
	\ddot{\theta} = -\nabla L(\theta).
\end{align*}
%
But without friction we are going to massively overshoot the minimum, so we are
also going to add a "friction force" inversely proportional to our current
velocity
%
\begin{align}\label{eq: acceleration is gradient + friction}
	\ddot{\theta} = -\nabla L(\theta) - \friction \dot{\theta}.
\end{align}
%
The standard way to discretize a second order ODE is to convert it into a first
order ODE
%
\begin{align*}
	\dot{y} := \begin{pmatrix}
		\dot{\theta}\\
		\ddot{\theta}
	\end{pmatrix}
	= \begin{pmatrix}
		\dot{\theta} \\
		-\nabla L(\theta) - \friction \dot{\theta}
	\end{pmatrix}
	=: g\Big(\begin{pmatrix}
		\theta \\
		\dot{\theta}
	\end{pmatrix}\Big)
	= g(y).
\end{align*}
%
Which allows us to naively discretize our ODE with the Euler discretization
\fxnote{whitespace issue with subequations}
%
\begin{subequations}
\begin{align}
	\theta_{n+1} &= \theta_n + \eta \momentum_n \label{eq: naive momentum move}\\
	\momentum_{n+1} &= \momentum_n + \eta [-\nabla L(\theta_n) - \friction \momentum_n]
	\label{eq: naive momentum}\\ \nonumber
	&= (1-\eta\friction)\momentum_n - \eta\nabla L(\theta_n).
\end{align}
\end{subequations}
%
Here we use \(\momentum\) to denote the momentum (velocity \(\dot{\theta}\)
assuming unit mass).
If we plug the second equation (\ref{eq: naive momentum}) into the first
equation (\ref{eq: naive momentum move}) we get
%
\begin{align*}
	\theta_{n+1}
	&= \theta_n + \eta [(1-\eta\friction)\momentum_{n-1} - \eta\nabla L(\theta_{n-1})].
\end{align*}
%
This means we are using gradient information from \(\theta_{n-1}\) to update
\(\theta_{n+1}\). If we instead use the most up to date information
\(\momentum_{n+1}\) instead of \(\momentum_n\) for the \(\theta_{n+1}\) update,
we get the well known \emph{heavy ball method} (momentum method) first proposed
by \textcite{polyakMethodsSpeedingConvergence1964} and wonderfully illustrated
by \textcite{gohWhyMomentumReally2017}.

\begin{definition}[Momentum Method]
	\begin{subequations}
	\begin{align}
		\theta_{n+1} &= \theta_n + \eta \momentum_{n+1} \label{eq: momentum move}\\
		\momentum_{n+1} &= (1-\eta\friction)\momentum_n - \eta\nabla L(\theta_n)
		\label{eq: momentum}
	\end{align}
	\end{subequations}
	%
	An equivalent formulation obtained by plugging (\ref{eq: momentum}) into
	(\ref{eq: momentum move}) is
	%
	\begin{align}
		\theta_{n+1}
		&= \theta_n + (1-\eta\friction)(\theta_n - \theta_{n-1}) - \eta^2\nabla L(\theta_n)
	\end{align}
\end{definition}
%
Nesterov's momentum is even more aggressive: Instead of using \(p_{n+1}\) for
the \(\theta_{n+1}\) update, it considers the certain "momentum move"
to calculate an intermediate position \(y_{n+1}\)
%
\begin{align*}
	\theta_{n+1}
	&= \theta_n + \overbrace{\eta [(1-\eta\friction)\momentum_n}^{\text{"momentum move"}}
	- \eta\nabla L(\theta_n)] \\
	&= y_{n+1} - \eta^2 \nabla L(\theta_n).
\end{align*}
%
It then uses that intermediate position to calculate the gradient instead of the
previous position \(\theta_n\).
%
\fxnote{Illustrate Nesterov's Momentum}
\begin{definition}[Nesterov's Momentum]
	\begin{subequations}
	\begin{align}
		\theta_{n+1} &= \theta_n + \eta \momentum_{n+1} \label{eq: nesterov momentum move}\\
		\momentum_{n+1}
		&= (1-\eta\friction)\momentum_n
		- \eta\nabla L[\theta_n + \eta(1-\eta\friction)\momentum_n]
		\label{eq: nesterov momentum}
		\\ \nonumber
		&= (1-\eta\friction)\momentum_n
		- \eta\nabla L[\underbrace{
			\theta_n + (1-\eta\friction)(\theta_n - \theta_{n-1})
		}_{= y_{n+1}}]
	\end{align}
	\end{subequations}
	%
	Discarding the momentum term and solely using the intermediate position
	results in the simplified version
	%
	\begin{subequations} \label{eq: nesterov intermediate position version}
	\begin{align}
		\theta_{n+1} &= y_{n+1} - \eta^2 \nabla L(y_{n+1})\\
		y_{n+1}&= \theta_n + (1-\eta\friction)(\theta_n - \theta_{n-1})
	\end{align}
	\end{subequations}
\end{definition}
%
\begin{remark}\fxwarning{too informal?}
	This method is said to date back to \textcite{nesterovMethodSolvingConvex1983}.
	But not only is the original text in Russian, it also seems to be impossible
	to find on the internet. Fortunately, \citeauthor{nesterovMethodSolvingConvex1983}
	wrote textbooks, \citetitle{nesterovLecturesConvexOptimization2018}
	(\citeyear{nesterovLecturesConvexOptimization2018}) being the most recent.
	Unfortunately, the chapter 2.2 on optimal methods provides barely any
	intuition at all and it is advisable to consult other sources (e.g.
	\textcite{dontlooWhatDifferenceMomentum2016}). If you
	want to recognize this scheme in \textcite{nesterovLecturesConvexOptimization2018}
	compare (\ref{eq: nesterov intermediate position version}) with Nesterov's
	Constant Step Scheme III (2.2.22).
 \end{remark}



\section{Convergence Rate}

\section{Complexity Bounds}

\section{Adam \& Nadam}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
