% !TEX root = ../Masterthesis.tex

\chapter{Momentum}

To understand why the convergence rate is poor when the condition
number is high, we can visualize a high ratio of the lowest to the highest
eigenvalue as a narrow ravine. The gradient points in the direction of the
strongest decent which is mostly to the opposite side of the ravine and only slightly
along its length. This causes our iterate to bounce back and forth between
the walls of the ravine.
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_bad_contitioning.pdf_tex}
	\caption{Momentum reduces fluctuations and converges faster}
	\label{fig: visualize bad conditioning}
\end{figure}

As a fix it seems appropriate to average the gradients in some sense, to
cancel out the opposing jumps and go straight down the ravine. In other words
we want to build momentum. Now if we move according to the sum (integral) of
the gradients, then our velocity stops being equal to the gradient but instead
becomes the antiderivative of the gradient.

So instead of setting our gradient equal to the velocity like in (\ref{eq:
velocity is gradient}), we want to set the acceleration equal to our gradient
%
\begin{align*}
	\ddot{\weights} = -\nabla \Loss(\weights).
\end{align*}
%
But without friction we are going to massively overshoot the minimum, so we are
also going to add a "friction force" inversely proportional to our current
velocity
%
\begin{align}\label{eq: acceleration is gradient + friction}
	\ddot{\weights} = -\nabla \Loss(\weights) - \friction \dot{\weights}.
\end{align}
%
The standard way to discretize a second order ODE is to convert it into a first
order ODE
%
\begin{align*}
	\dot{y} := \begin{pmatrix}
		\dot{\weights}\\
		\ddot{\weights}
	\end{pmatrix}
	= \begin{pmatrix}
		\dot{\weights} \\
		-\nabla \Loss(\weights) - \friction \dot{\weights}
	\end{pmatrix}
	=: g\Big(\begin{pmatrix}
		\weights \\
		\dot{\weights}
	\end{pmatrix}\Big)
	= g(y).
\end{align*}
%
Which allows us to naively discretize our ODE with the Euler discretization
\fxnote{whitespace issue with subequations}
%
\begin{subequations}
\begin{align}
	\weights_{n+1} &= \weights_n + \lr \momentum_n \label{eq: naive momentum move}\\
	\momentum_{n+1} &= \momentum_n + \lr [-\nabla \Loss(\weights_n) - \friction \momentum_n]
	\label{eq: naive momentum}\\ \nonumber
	&= (1-\lr\friction)\momentum_n - \lr\nabla \Loss(\weights_n).
\end{align}
\end{subequations}
%
Here we use \(\momentum\) to denote the momentum (velocity \(\dot{\weights}\)
assuming unit mass).
If we plug the second equation (\ref{eq: naive momentum}) into the first
equation (\ref{eq: naive momentum move}) we get
%
\begin{align*}
	\weights_{n+1}
	&= \weights_n + \lr [(1-\lr\friction)\momentum_{n-1} - \lr\nabla \Loss(\weights_{n-1})].
\end{align*}
%
This means we are using gradient information from \(\weights_{n-1}\) to update
\(\weights_{n+1}\). If we instead use the most up to date information
\(\momentum_{n+1}\) instead of \(\momentum_n\) for the \(\weights_{n+1}\) update,
we get the well known \emph{heavy ball method} (momentum method) first proposed
by \textcite{polyakMethodsSpeedingConvergence1964} and wonderfully illustrated
by \textcite{gohWhyMomentumReally2017}.

\begin{definition}[Heavy Ball Method]
	\begin{subequations}
	\begin{align}
		\weights_{n+1} &= \weights_n + \lr \momentum_{n+1} \label{eq: momentum move}\\
		\momentum_{n+1} &= (1-\lr\friction)\momentum_n - \lr\nabla \Loss(\weights_n)
		\label{eq: momentum}
	\end{align}
	\end{subequations}
	%
	An equivalent formulation obtained by plugging (\ref{eq: momentum}) into
	(\ref{eq: momentum move}) but using (\ref{eq: momentum move}) for
	\(\momentum_n\) is
	%
	\begin{align}\label{eq: flat momentum}
		\weights_{n+1}
		&= \weights_n
		+ \underbrace{(1-\lr\friction)}_{
			=:\momCoeff
		}(\weights_n - \weights_{n-1})
		- \underbrace{\lr^2}_{=:\lrSq}\nabla \Loss(\weights_n)
	\end{align}
	In particular we can set "the momentum coefficient" \(\momCoeff\) to zero to
	obtain gradient decent again. This is of course an artifact of our
	discretization since \(\lr\to0\) would never allow \(\momCoeff\) to be zero
	in the limit. But actual implementations often use this
	(\(\momCoeff,\lrSq\))-parametrization and thus treat gradient decent as a
	special case.
\end{definition}
%
Nesterov's momentum is even more aggressive: Instead of using \(p_{n+1}\) for
the \(\weights_{n+1}\) update, it considers the certain "momentum move"
to calculate an intermediate position \(y_{n+1}\)
%
\begin{align*}
	\weights_{n+1}
	&= \weights_n + \overbrace{\lr [(1-\lr\friction)\momentum_n}^{\text{"momentum move"}}
	- \lr\nabla \Loss(\weights_n)] \\
	&= y_{n+1} - \lrSq \nabla \Loss(\weights_n).
\end{align*}
%
It then uses that intermediate position to calculate the gradient instead of the
previous position \(\weights_n\).
%
\fxnote{Illustrate Nesterov's Momentum}
\begin{definition}[Nesterov's Momentum]
	\begin{subequations}
	\begin{align}
		\weights_{n+1} &= \weights_n + \lr \momentum_{n+1} \label{eq: nesterov momentum move}\\
		\momentum_{n+1}
		&= (1-\lr\friction)\momentum_n
		- \lr\nabla L[\weights_n + \lr(1-\lr\friction)\momentum_n]
		\label{eq: nesterov momentum}
		\\ \nonumber
		&= \momCoeff\momentum_n
		- \lr\nabla L[\underbrace{
			\weights_n + \momCoeff(\weights_n - \weights_{n-1})
		}_{= y_{n+1}}]
	\end{align}
	\end{subequations}
	%
	Discarding the momentum term and solely using the intermediate position
	results in the simplified version
	%
	\begin{subequations} \label{eq: nesterov intermediate position version}
	\begin{align}
		\weights_{n+1} &= y_{n+1} - \lrSq \nabla \Loss(y_{n+1})\\
		y_{n+1}&= \weights_n + \momCoeff(\weights_n - \weights_{n-1})
	\end{align}
	\end{subequations}
	dropping the intermediate position as well results in the analog to (\ref{eq:
	flat momentum})
	\begin{align}
		\weights_{n+1} &= \weights_n + \momCoeff(\weights_n - \weights_{n-1})
		- \lrSq \nabla \Loss(\weights_n + \momCoeff(\weights_n - \weights_{n-1}))
	\end{align}
\end{definition}
%
\begin{remark}\fxwarning{too informal?}
	This method also known as "Nesterov's Accelerated Momentmum" is said to date
	back to \textcite{nesterovMethodSolvingConvex1983}.  But not only is the
	original text in Russian, it also difficult to find on the internet.
	Fortunately, \citeauthor{nesterovMethodSolvingConvex1983} wrote textbooks,
	\citetitle{nesterovLecturesConvexOptimization2018}
	(\citeyear{nesterovLecturesConvexOptimization2018}) being the most recent.
	Unfortunately, the chapter 2.2 on optimal methods provides barely any
	intuition at all and it is advisable to consult other sources (e.g.
	\textcite{dontlooWhatDifferenceMomentum2016}). If you
	want to recognize this scheme in \textcite{nesterovLecturesConvexOptimization2018}
	compare (\ref{eq: nesterov intermediate position version}) with Nesterov's
	Constant Step Scheme III (2.2.22).
 \end{remark}

\section{Convergence Rates}

\subsection{Heavy Ball Convergence on Quadratic Functions}

Following the arguments from Section~\ref{sec: visualize gd} in particular
(\ref{eq: hesse representation of gradient}) we can rewrite the momentum
method as
\begin{align*}
	\begin{pmatrix}
		\weights_n - \hat{\weights}_n \\
		\weights_{n+1} - \hat{\weights}_n
	\end{pmatrix}
	&=
	\begin{pmatrix}
		\weights_n - \hat{\weights}_n \\
		\weights_n - \hat{\weights}_n + \momCoeff (\weights_n - \weights_{n-1})
		- \lrSq \nabla\Loss^2(\weights_n)(\weights_n -\hat{\weights}_n)
	\end{pmatrix}\\
	&=
	\begin{pmatrix}
		0\identity_\dimension & \identity_\dimension \\
		-\momCoeff\identity_\dimension
		& (1+\momCoeff)\identity_\dimension -\lrSq \nabla^2\Loss(\weights_n)
	\end{pmatrix}
	\begin{pmatrix}
		\weights_{n-1} - \hat{\weights}_n \\
		\weights_n - \hat{\weights}_n
	\end{pmatrix}
\end{align*}
%
For readability I will now omit the identity matrix \(\identity\) from the
block matrices which are just a constant multiplied by an identity matrix.
Using the digitalization of the hesse matrix (\ref{eq: diagnalization of the
Hesse matrix}) again, we get
%
\begin{align*}
	&\begin{pmatrix}
		0 & 1 \\
		-\momCoeff & 1+\momCoeff -\lrSq \nabla^2\Loss(\weights_n)
	\end{pmatrix}\\
	&=
	\begin{pmatrix}
		V & 0 \\
		0 & V
	\end{pmatrix}	
	\begin{pmatrix}
		0 & 1 \\
		-\momCoeff &
		\diag(1+\momCoeff -\lrSq\hesseEV_1, \dots, 
		1+\momCoeff -\lrSq\hesseEV_\dimension)
	\end{pmatrix}
	\begin{pmatrix}
		V & 0 \\
		0 & V
	\end{pmatrix}^T
\end{align*}
%
Reordering the eigenvalues to
%
\begin{align*}
	\begin{pmatrix}
		v_1 & 0 & \cdots & v_\dimension & 0 \\
		0 & v_1 & \cdots & 0 & v_\dimension
	\end{pmatrix}
\end{align*}
%
reorders the transformation matrix of the eigenspace to
%
\begin{align}\label{eq: eigenspace momentum transformation}
	\Sigma := \begin{pmatrix}
		0 & 1 \\
		-\momCoeff & 1+\momCoeff - \lrSq\hesseEV_1 & \\
		& & \ddots & \\
		& & & 0 & 1 \\
		& & & -\momCoeff & 1+\momCoeff - \lrSq\hesseEV_\dimension \\
	\end{pmatrix}
	= \begin{pmatrix}
		\Sigma_1 \\
		& \ddots\\
		&& \Sigma_\dimension
	\end{pmatrix}
\end{align}
%
So in contrast to Section~\ref{sec: visualize gd} we do not get a diagonal
matrix immediately. To achieve a similar eigenvalue analysis as in
Section~\ref{sec: visualize gd} we first have to determine the eigenvalues
\(\sigma_{i1},\sigma_{i2}\) of each \(\Sigma_i\) and then ensure that
\begin{align}
	\max_{i=1,\dots, \dimension} \max\{|\sigma_{i1}|,|\sigma_{i2}|\} < 1.
\end{align}
Using the p-q formula on the characteristic polynomial of \(\Sigma_i\)
results in 
\begin{align*}
	\sigma_{i1/2}
	= \tfrac12 \left(
		1+\momCoeff-\lrSq\hesseEV_i
		\pm \sqrt{(1+\momCoeff-\lrSq\hesseEV_i)^2 - 4\momCoeff}
	\right).
\end{align*}
%
The analysis of these eigenvalues is very technical and can be found in the
appendix. We will only cover the result here.

\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/annotated_heavy_ball_rates.pdf_tex}
	\caption{
		Heat plot of \(\max\{|\sigma_1|,|\sigma_2|\}\).
		Strictly speaking the "Monotonic" and "Oscillation" area should not
		extend into negative \(\momCoeff\) since \(\sigma_{1/2}\)
		have opposite signs there, but it still describes the behavior of the
		dominating eigenvalue. Also recall that \(\momCoeff=0\) represents SGD
		without momentum.
	}
	\label{fig: annotated heavy ball rates}
\end{figure}

\begin{theorem}[\cite{qianMomentumTermGradient1999}]
	\label{thm: momentum - stable set of parameters}
	Let
	\begin{align*}
		\sigma_{1/2}
		= \tfrac12 \left(
			1+\momCoeff-\lrSq\hesseEV \pm \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\right)
	\end{align*}
	then 
	\begin{enumerate}
		\item \(\max\{|\sigma_1|,|\sigma_2|\}<1\) if and only if
		\begin{align*}
			0<\lrSq\hesseEV < 2(1+\momCoeff) \qquad \text{and} \qquad |\momCoeff|<1
		\end{align*}
		\item The complex case can be characterized by either
		\begin{align*}
			0<(1-\sqrt{\lrSq\hesseEV})^2 < \momCoeff < 1
		\end{align*}		
		or alternatively \(\momCoeff>0\) and
		\begin{align*}
			(1-\sqrt{\momCoeff})^2 < \lrSq\hesseEV < (1+\sqrt{\momCoeff})^2,
		\end{align*}
		for which we have \(|\sigma_1|=|\sigma_2|=\sqrt{\momCoeff}\).
		
		As complex
		eigenvalues imply a rotation, this can be viewed as rotating the distance
		to the minimum of the quadratic function into the momentum and back like 
		a pendulum where the friction causes it to eventually end up in the
		minimum. Looking at the distance to the minimum only, we will therefore
		observe a sinus wave ("Ripples").
		\item In the real case we have \(\sigma_1>\sigma_2\) and
		\begin{align}
			\max\{|\sigma_1|, |\sigma_2|\} = \begin{cases}
				|\sigma_1|=\sigma_1 & \lrSq\hesseEV < 1+\momCoeff \\
				|\sigma_2|=-\sigma_2 & \lrSq\hesseEV \ge 1+\momCoeff.
			\end{cases}
		\end{align}
		Restricted to \(1>\momCoeff>0\) this results in two different	
		behaviors. For
		\begin{align*}
			0<\lrSq\hesseEV \le (1-\sqrt{\momCoeff})^2 < 1+\momCoeff
		\end{align*}
		we have \(1>\sigma_1 > \sigma_2 > 0\) which results in a monotonic
		linear convergence ("Monotonic" case). For
		\begin{align*}
			1+\momCoeff < (1+\sqrt{\momCoeff})^2\le \lrSq\hesseEV < 2(1+\momCoeff)
		\end{align*}
		on the other hand, we get \(-1 < \sigma_2 < \sigma_1 < 0\) which implies 
		an oscillating convergence ("Oscillation"). In contrast to the "Ripples"
		we switch the side of the distance to the minimum in every iteration.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Appendix Theorem~\ref{thm-appdx: momentum - stable set of parameters}
\end{proof}

In particular (positive) momentum extends the available range for convergent
learning rate/eigenvalue products which allows for larger learning rates. As
can be seen very well in Figure~\ref{fig: annotated heavy ball rates}, the
wider attraction area allows for moving out the largest eigenvalues to the
right with larger eigenvalues shifting the smaller eigenvalues towards the
right as well into more acceptable convergence rates. If the smallest and
largest eigenvalue are far apart (i.e. the condition number \(\condition\) is
high) this should be very useful.

So how much do we gain? In other words what are the optimal rates of
convergence? Since the plot looks quite symmetric it is quite intuitive that
we only have to consider the largest and smallest eigenvalue. For now we
can consider it as a lower bound:
\begin{align}\label{eq: lower bound best heavy ball rates}
	\min_{\lrSq,\momCoeff}\max_{i=1,\dots,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
	\ge \min_{\lrSq}\min_{\momCoeff}
	\max_{i=1,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
\end{align}
Now let us consider the inner minimum first. Let us inspect the claim
\begin{align}
	\arg\min_{\momCoeff}\max_{i=1,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
	= \max\{(1-\sqrt{\lrSq\hesseEV_1})^2, (1-\sqrt{\lrSq\hesseEV_\dimension})^2\}
\end{align}\label{eq: optimal heavy ball momentum - ev representation}
In that case we are still (barely) in the complex case, therefore the rate
of convergence is equal to \(\sqrt{\momCoeff}\), i.e.
\begin{align*}
	\min_{\momCoeff}\max_{i=1,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
	= \max\{(1-\sqrt{\lrSq\hesseEV_1}), (1-\sqrt{\lrSq\hesseEV_\dimension})\}
\end{align*}
Now let us wiggle around \(\momCoeff\) a bit. If we would increase it, then
we are obviously increasing our rate since it is then still equal to
\(\sqrt{\momCoeff}\) as we just expanded the size of the complex case. Now the
question is what happens if we decrease it. Now by our selection of \(\momCoeff\)
either the smallest or largest eigenvalue sits right at the edge of the real 
case. If we can show that our absolute eigenvalue maximum is increasing if
we move further into the real case (decrease \(\momCoeff\)), we would have
proven that we can only become worse off doing so. This is annoyingly technical
so it is covered in Lemma~\ref{lem-appdx: just at the border of complex case is
best beta} in the appendix.

Now since both the largest and smallest eigenvalue are included in the complex
case, we know that all eigenvalues in between must also be in the complex case
which means that (\ref{eq: lower bound best heavy ball rates}) is an equality
and
\begin{align*}
	\min_{\lrSq,\momCoeff}\max_{i=1,\dots,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
	= \min_{\lrSq}\max\{(1-\sqrt{\lrSq\hesseEV_1}), (1-\sqrt{\lrSq\hesseEV_\dimension})\}.
\end{align*}
Now we are just doing our balancing act  again, only with 
\(\sqrt{\hesseEV_1},\sqrt{\hesseEV_\dimension}\) instead of
\(\hesseEV_1,\hesseEV_\dimension\). But we know the solution to that already
from (\ref{eq: optimal SGD lr eigenvalue representation}),
which is
\begin{align*}
	\lr_* = \sqrt{\lrSq_*} = \frac{2}{\sqrt{\hesseEV_1}+\sqrt{\hesseEV_\dimension}}
\end{align*}
%
Since this makes both of them equally bad, we can plug this learning rate into
either equation from (\ref{eq: optimal heavy ball momentum - ev representation})
to get
%
\begin{align*}
	\momCoeff_*=\left(1-\frac{2}{1+\sqrt{\kappa}}\right)^2
\end{align*}
%
which results in the optimal rate of convergence of
%
\begin{align*}
	\sqrt{\momCoeff_*} = 1-\frac{2}{1+\sqrt{\kappa}}.
\end{align*}
%
And this is in fact \emph{equal} to our lower bound (not even up to a constant!) in
Theorem~\ref{thm: strong convexity complexity bound}. So on quadratic functions
we can not do any better without violating our assumptions about possible
optimizing algorithms (Assumption~ \ref{assmpt: parameter in generalized linear
hull of gradients}).

\subsection{Why Consider Alternatives?}

Now while Nesterov's Momentum breaks this assumption as it uses gradients at a
different point than the iterates, it does not break it fundamentally. The
assumption applies to the intermediate states \(y_n\) which are not too far
from our actual iterates and would thus allow us to make similar statements
about them with a bit of work.

So why should we even considering Nesterov's Momentum then?

\subsubsection{Weakest Link Contraction Factor}

The first issue you
might have noticed is: Since \emph{all eigenspaces} are in the complex case,
all of them have the worst contraction factor \(\sqrt{\momCoeff_*}\). In stochastic
gradient decent on the other hand the contraction factor is \(|1-\lr_*\hesseEV_i|\)
for the eigenspace \(i\), which can be considerably better than the worst
contraction factor 
\[|1-\lr_*\hesseEV_\dimension|=|1-\lr_*\hesseEV_1|.\]
To improve the convergence rate of the worst eigenspaces we have therefore
sacrificed the (possibly excellent) convergence rate of all other eigenspaces.

\subsubsection{No General Convergence at these Rates}

While
\textcite[pp. 65-67]{polyakIntroductionOptimization1987} used the eigenvalue
analysis of \(\nabla\Loss^2(\weights_*)\) to extend this rate of convergence
to a local area around \(\weights_*\), \textcite[pp. 78-79]{lessardAnalysisDesignOptimization2016}
gave an example of an \(\strongConvex{\lbound}{\ubound}\) function where
the heavy ball method with these "optimal" parameters is trapped in an attractive
cycle instead of converging. 
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/hb_counterexample.pdf_tex}
	\caption{
		\citeauthor{lessardAnalysisDesignOptimization2016}'s counterexample.
		Notice that the y-axis is much coarser than the x-axis and gradients are
		therefore much steeper than apparent. At point \(-1.8\) the gradient is
		quite steep and the next parameter is flung across the minimum to
		\(2.12\). Due to the less convex segment on this side, the gradient is
		not quite as steep here, so the iterate does not make it to the other
		side again which would then cause a bouncing back and forth to the
		minimum. Instead it stays on this side, which causes the next gradient
		at \(0.65\) together with the built momentum in this direction to move the
		iterate back out to our first point.
	}
	\label{fig: heavy ball counterexample}
\end{figure}

Since we know \(\lbound\) and \(\ubound\) of their counterexample function from
Figure~\ref{fig: heavy ball counterexample}, we know the parameters \(\lrSq_*\)
and \(\momCoeff_*\) which they used to explicitly write out the momentum recursion
with \(\weights_{n+1}, \weights_{n}, \weights_{n-1}\). Assuming a three-cycle
this left them with just three linear equations which results in the three-cycle
displayed in the Figure. Since we start with zero momentum, one now just has to
find a point to enter this cycle. Additional to solving this initialization
problem, \textcite[pp. 93-94]{lessardAnalysisDesignOptimization2016} also show
that this cycle is stable under noise (attractive).

\textcite{ghadimiGlobalConvergenceHeavyball2015} salvages some of this wreckage
by proving general convergence results with tighter restrictions on hyper
parameters. In particular for \(\Loss\in\strongConvex{\lbound}{\ubound}\) and
\begin{align*}
	\momCoeff\in[0,1),
	\qquad \lrSq\in \left(0, \frac{2(1-\momCoeff)}{\ubound+\lbound}\right)
\end{align*}
they can guarantee linear convergence with an explicit, but complicated
\fxnote{take a closer look?}{contraction factor}. Notice that this is
considerably weaker than the condition
\begin{align*}
	0<\lrSq\lbound \le \dots \le \lrSq\ubound < 2(1+\momCoeff)
\end{align*}
from Theorem~\ref{thm: momentum - stable set of parameters}. In particular
momentum stops increasing the selection space for learning rates, just like
\(\lbound\) which rules out our optimal parameters immediately. 

Similarly they prove that for \(\Loss\in\lipGradientSet{\ubound}\) and
\begin{align*}
	\momCoeff\in[0,1),
	\qquad \lrSq\in \left(0, \frac{(1-\momCoeff)}{\ubound}\right]
\end{align*}
that the running minimum of loss evaluation converges to the minimal loss,
with their best guarantee achieved for \(\lrSq =(1-\momCoeff)/\ubound\)
resulting in
\begin{align*}
	\min_{k=0,\dots,n}\Loss(\weights_k) -\Loss(\weights_*)
	\le \frac{1}{1-\momCoeff}\frac{\ubound\|\weights_0-\weights_*\|^2}{2(n+1)}
\end{align*}
which provides a tighter convergence bound than 
Theorem~\ref{thm: convex function GD loss upper bound} for gradient decent 
without momentum \(\momCoeff=0\), but is actually harmed by momentum.
\fxnote{Check out improvements by \cite{sunNonErgodicConvergenceAnalysis2019}}

\subsection{Nesterov's Momentum}

The Eigenvalue analysis of Nesterov's Momentum starts out very similar and we
easily obtain the analog eigenspace transformation to (\ref{eq: eigenspace
momentum transformation})
%
\begin{align*}
	\Sigma_i = \begin{pmatrix}
		0 & 1\\
		-\momCoeff(1-\lrSq\hesseEV_i) & (1+\momCoeff)(1-\lrSq\hesseEV_i).
	\end{pmatrix}	
\end{align*}
%
which curiously reduces to the heavy ball version if we use
\begin{align*}
	\tilde{\momCoeff}_i = \momCoeff(1-\lrSq\hesseEV_i)
\end{align*}

\begin{figure}[h]
	\centering
	\begin{minipage}{1\textwidth}
		\centering
		\def\svgwidth{1\textwidth}
		\input{media/heavy_ball_rates.pdf_tex}
	\end{minipage}
	\begin{minipage}{1\textwidth}
		\centering
		\def\svgwidth{1\textwidth}
		\input{media/nesterov_rates.pdf_tex}
	\end{minipage}
	\caption{
		Heat plot of \(\max\{|\sigma_1|,|\sigma_2|\}\) of the Heavy Ball
		Momentum and Nesterov's Momentum in direct comparison.
	}
	\label{fig: compare rates}
\end{figure}


\section{Complexity Bounds}

\section{Adam \& Nadam}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
