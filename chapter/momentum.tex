% !TEX root = ../Masterthesis.tex

\chapter{Momentum}

To understand why the convergence rate is poor when the condition
number is high, we can visualize a high ratio of the lowest to the highest
eigenvalue as a narrow ravine. The gradient points in the direction of the
strongest decent which is mostly to the opposite side of the ravine and only slightly
along its length. This causes our iterate to bounce back and forth between
the walls of the ravine.
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/visualize_bad_contitioning.pdf_tex}
	\caption{Momentum reduces fluctuations and converges faster}
	\label{fig: visualize bad conditioning}
\end{figure}

As a fix it seems appropriate to average the gradients in some sense, to
cancel out the opposing jumps and go straight down the ravine. In other words
we want to build momentum. Now if we move according to the sum (integral) of
the gradients, then our velocity stops being equal to the gradient but instead
becomes the antiderivative of the gradient.

So instead of setting our gradient equal to the velocity like in (\ref{eq:
velocity is gradient}), we want to set the acceleration equal to our gradient
%
\begin{align*}
	\ddot{\weights} = -\nabla \Loss(\weights).
\end{align*}
%
But without friction we are going to massively overshoot the minimum, so we are
also going to add a ``friction force'' inversely proportional to our current
velocity
%
\begin{align}\label{eq: acceleration is gradient + friction}
	\ddot{\weights} = -\nabla \Loss(\weights) - \friction \dot{\weights}.
\end{align}
%
The standard way to discretize a second order ODE is to convert it into a first
order ODE
%
\begin{align*}
	\dot{y} := \begin{pmatrix}
		\dot{\weights}\\
		\ddot{\weights}
	\end{pmatrix}
	= \begin{pmatrix}
		\dot{\weights} \\
		-\nabla \Loss(\weights) - \friction \dot{\weights}
	\end{pmatrix}
	=: g\Big(\begin{pmatrix}
		\weights \\
		\dot{\weights}
	\end{pmatrix}\Big)
	= g(y).
\end{align*}
%
Which allows us to naively discretize our ODE with the Euler discretization
\fxnote{whitespace issue with subequations}
%
\begin{subequations}
\begin{align}
	\weights_{n+1} &= \weights_n + \lr \momentum_n \label{eq: naive momentum move}\\
	\momentum_{n+1} &= \momentum_n + \lr [-\nabla \Loss(\weights_n) - \friction \momentum_n]
	\label{eq: naive momentum}\\ \nonumber
	&= (1-\lr\friction)\momentum_n - \lr\nabla \Loss(\weights_n).
\end{align}
\end{subequations}
%
Here we use \(\momentum\) to denote the momentum (velocity \(\dot{\weights}\)
assuming unit mass).
If we plug the second equation (\ref{eq: naive momentum}) into the first
equation (\ref{eq: naive momentum move}) we get
%
\begin{align*}
	\weights_{n+1}
	&= \weights_n + \lr [(1-\lr\friction)\momentum_{n-1} - \lr\nabla \Loss(\weights_{n-1})].
\end{align*}
%
This means we are using gradient information from \(\weights_{n-1}\) to update
\(\weights_{n+1}\). If we instead use the most up to date information
\(\momentum_{n+1}\) instead of \(\momentum_n\) for the \(\weights_{n+1}\) update,
we get the well known \emph{heavy ball method} (momentum method) first proposed
by \textcite{polyakMethodsSpeedingConvergence1964} and wonderfully illustrated
by \textcite{gohWhyMomentumReally2017}.

\begin{definition}[Heavy Ball Method]
	\begin{subequations}
	\begin{align}
		\weights_{n+1} &= \weights_n + \lr \momentum_{n+1} \label{eq: momentum move}\\
		\momentum_{n+1} &= (1-\lr\friction)\momentum_n - \lr\nabla \Loss(\weights_n)
		\label{eq: momentum}
	\end{align}
	\end{subequations}
	%
	An equivalent formulation obtained by plugging (\ref{eq: momentum}) into
	(\ref{eq: momentum move}) but using (\ref{eq: momentum move}) for
	\(\momentum_n\) is
	%
	\begin{align}\label{eq: flat momentum}
		\weights_{n+1}
		&= \weights_n
		+ \underbrace{(1-\lr\friction)}_{
			=:\momCoeff
		}(\weights_n - \weights_{n-1})
		- \underbrace{\lr^2}_{=:\lrSq}\nabla \Loss(\weights_n)
	\end{align}
	In particular we can set ``the momentum coefficient'' \(\momCoeff\) to zero to
	obtain gradient decent again. This is of course an artifact of our
	discretization since \(\lr\to0\) would never allow \(\momCoeff\) to be zero
	in the limit. But actual implementations often use this
	(\(\momCoeff,\lrSq\))-parametrization and thus treat gradient decent as a
	special case.
\end{definition}
%
Nesterov's momentum is even more aggressive: Instead of using \(p_{n+1}\) for
the \(\weights_{n+1}\) update, it considers the certain ``momentum move''
to calculate an intermediate position \(y_{n+1}\)
%
\begin{align*}
	\weights_{n+1}
	&= \weights_n + \overbrace{\lr [(1-\lr\friction)\momentum_n}^{\text{``momentum move''}}
	- \lr\nabla \Loss(\weights_n)] \\
	&= y_{n+1} - \lrSq \nabla \Loss(\weights_n).
\end{align*}
%
It then uses that intermediate position to calculate the gradient instead of the
previous position \(\weights_n\).
%
\fxnote{Illustrate Nesterov's Momentum}
\begin{definition}[Nesterov's Momentum]
	\label{def: nesterov's momentum}
	\begin{subequations}
	\begin{align}
		\weights_{n+1} &= \weights_n + \lr \momentum_{n+1} \label{eq: nesterov momentum move}\\
		\momentum_{n+1}
		&= (1-\lr\friction)\momentum_n
		- \lr\nabla L[\weights_n + \lr(1-\lr\friction)\momentum_n]
		\label{eq: nesterov momentum}
		\\ \nonumber
		&= \momCoeff\momentum_n
		- \lr\nabla L[\underbrace{
			\weights_n + \momCoeff(\weights_n - \weights_{n-1})
		}_{= y_{n+1}}]
	\end{align}
	\end{subequations}
	%
	Discarding the momentum term and solely using the intermediate position
	results in the simplified version
	%
	\begin{subequations} \label{eq: nesterov intermediate position version}
	\begin{align}
		\weights_{n+1} &= y_{n+1} - \lrSq \nabla \Loss(y_{n+1})\\
		y_{n+1}&= \weights_n + \momCoeff(\weights_n - \weights_{n-1})
	\end{align}
	\end{subequations}
	dropping the intermediate position as well results in the analog to (\ref{eq:
	flat momentum})
	\begin{align}
		\weights_{n+1} &= \weights_n + \momCoeff(\weights_n - \weights_{n-1})
		- \lrSq \nabla \Loss(\weights_n + \momCoeff(\weights_n - \weights_{n-1}))
	\end{align}
\end{definition}
%
\begin{remark}\fxwarning{too informal?}
	This method also known as ``Nesterov's Accelerated Momentmum'' is said to date
	back to \textcite{nesterovMethodSolvingConvex1983}.  But not only is the
	original text in Russian, it also difficult to find on the internet.
	Fortunately, \citeauthor{nesterovMethodSolvingConvex1983} wrote textbooks,
	\citetitle{nesterovLecturesConvexOptimization2018}
	(\citeyear{nesterovLecturesConvexOptimization2018}) being the most recent.
	Unfortunately, the chapter 2.2 on optimal methods provides barely any
	intuition at all and it is advisable to consult other sources (e.g.
	\textcite{dontlooWhatDifferenceMomentum2016}). If you
	want to recognize this scheme in \textcite{nesterovLecturesConvexOptimization2018}
	compare (\ref{eq: nesterov intermediate position version}) with Nesterov's
	Constant Step Scheme III (2.2.22).
 \end{remark}

\section{Heavy Ball Convergence}

Following the arguments from Section~\ref{sec: visualize gd} in particular
(\ref{eq: hesse representation of gradient}) we can rewrite the momentum
method as
\begin{align*}
	\begin{pmatrix}
		\weights_n - \hat{\weights}_n \\
		\weights_{n+1} - \hat{\weights}_n
	\end{pmatrix}
	&=
	\begin{pmatrix}
		\weights_n - \hat{\weights}_n \\
		\weights_n - \hat{\weights}_n + \momCoeff (\weights_n - \weights_{n-1})
		- \lrSq \nabla\Loss^2(\weights_n)(\weights_n -\hat{\weights}_n)
	\end{pmatrix}\\
	&=
	\begin{pmatrix}
		0\identity_\dimension & \identity_\dimension \\
		-\momCoeff\identity_\dimension
		& (1+\momCoeff)\identity_\dimension -\lrSq \nabla^2\Loss(\weights_n)
	\end{pmatrix}
	\begin{pmatrix}
		\weights_{n-1} - \hat{\weights}_n \\
		\weights_n - \hat{\weights}_n
	\end{pmatrix}
\end{align*}
%
For readability I will now omit the identity matrix \(\identity\) from the
block matrices which are just a constant multiplied by an identity matrix.
Using the digitalization of the hesse matrix (\ref{eq: diagnalization of the
Hesse matrix}) again, we get
%
\begin{align*}
	&\begin{pmatrix}
		0 & 1 \\
		-\momCoeff & 1+\momCoeff -\lrSq \nabla^2\Loss(\weights_n)
	\end{pmatrix}\\
	&=
	\begin{pmatrix}
		V & 0 \\
		0 & V
	\end{pmatrix}	
	\begin{pmatrix}
		0 & 1 \\
		-\momCoeff &
		\diag(1+\momCoeff -\lrSq\hesseEV_1, \dots, 
		1+\momCoeff -\lrSq\hesseEV_\dimension)
	\end{pmatrix}
	\begin{pmatrix}
		V & 0 \\
		0 & V
	\end{pmatrix}^T
\end{align*}
%
Reordering the eigenvalues to
%
\begin{align*}
	\begin{pmatrix}
		v_1 & 0 & \cdots & v_\dimension & 0 \\
		0 & v_1 & \cdots & 0 & v_\dimension
	\end{pmatrix}
\end{align*}
%
reorders the transformation matrix of the eigenspace to
%
\begin{align}\label{eq: eigenspace momentum transformation}
	\Sigma := \begin{pmatrix}
		0 & 1 \\
		-\momCoeff & 1+\momCoeff - \lrSq\hesseEV_1 & \\
		& & \ddots & \\
		& & & 0 & 1 \\
		& & & -\momCoeff & 1+\momCoeff - \lrSq\hesseEV_\dimension \\
	\end{pmatrix}
	= \begin{pmatrix}
		\Sigma_1 \\
		& \ddots\\
		&& \Sigma_\dimension
	\end{pmatrix}
\end{align}
%
So in contrast to Section~\ref{sec: visualize gd} we do not get a diagonal
matrix immediately. To achieve a similar eigenvalue analysis as in
Section~\ref{sec: visualize gd} we first have to determine the eigenvalues
\(\sigma_{i1},\sigma_{i2}\) of each \(\Sigma_i\) and then ensure that
\begin{align}
	\max_{i=1,\dots, \dimension} \max\{|\sigma_{i1}|,|\sigma_{i2}|\} < 1.
\end{align}
Using the p-q formula on the characteristic polynomial of \(\Sigma_i\)
results in 
\begin{align*}
	\sigma_{i1/2}
	= \tfrac12 \left(
		1+\momCoeff-\lrSq\hesseEV_i
		\pm \sqrt{(1+\momCoeff-\lrSq\hesseEV_i)^2 - 4\momCoeff}
	\right).
\end{align*}
%
The analysis of these eigenvalues is very technical and can be found in the
appendix. We will only cover the result here.

\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/annotated_heavy_ball_rates.pdf_tex}
	\caption{
		Heat plot of \(\max\{|\sigma_1|,|\sigma_2|\}\) for the Heavy Ball Momentum.
		Strictly speaking the ``Monotonic'' (M) and ``Oscillation'' (O) areas should not
		extend into negative \(\momCoeff\) since \(\sigma_{1/2}\)
		have opposite signs there, but it still describes the behavior of the
		dominating eigenvalue. The third label in the legend corresponds to the
		remarkably sharp border of the ``Ripples'' area which I did not want to
		draw over. Also recall that \(\momCoeff=0\) represents SGD
		without momentum.
	}
	\label{fig: annotated heavy ball rates}
\end{figure}

\begin{theorem}[\cite{qianMomentumTermGradient1999}]
	\label{thm: momentum - stable set of parameters}
	Let
	\begin{align*}
		\sigma_{1/2}
		= \tfrac12 \left(
			1+\momCoeff-\lrSq\hesseEV \pm \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\right)
	\end{align*}
	then 
	\begin{enumerate}
		\item \(\max\{|\sigma_1|,|\sigma_2|\}<1\) if and only if
		\begin{align*}
			0<\lrSq\hesseEV < 2(1+\momCoeff) \qquad \text{and} \qquad |\momCoeff|<1
		\end{align*}
		\item The complex case can be characterized by either
		\begin{align*}
			0<(1-\sqrt{\lrSq\hesseEV})^2 < \momCoeff < 1
		\end{align*}		
		or alternatively \(\momCoeff>0\) and
		\begin{align*}
			(1-\sqrt{\momCoeff})^2 < \lrSq\hesseEV < (1+\sqrt{\momCoeff})^2,
		\end{align*}
		for which we have \(|\sigma_1|=|\sigma_2|=\sqrt{\momCoeff}\).
		
		As complex
		eigenvalues imply a rotation, this can be viewed as rotating the distance
		to the minimum of the quadratic function into the momentum and back like 
		a pendulum where the friction causes it to eventually end up in the
		minimum. Looking at the distance to the minimum only, we will therefore
		observe a sinus wave (``Ripples'').
		\item In the real case we have \(\sigma_1>\sigma_2\) and
		\begin{align}
			\max\{|\sigma_1|, |\sigma_2|\} = \begin{cases}
				|\sigma_1|=\sigma_1 & \lrSq\hesseEV < 1+\momCoeff \\
				|\sigma_2|=-\sigma_2 & \lrSq\hesseEV \ge 1+\momCoeff.
			\end{cases}
		\end{align}
		Restricted to \(1>\momCoeff>0\) this results in two different	
		behaviors. For
		\begin{align*}
			0<\lrSq\hesseEV \le (1-\sqrt{\momCoeff})^2 < 1+\momCoeff
		\end{align*}
		we have \(1>\sigma_1 > \sigma_2 > 0\) which results in a monotonic
		linear convergence (``Monotonic'' case). For
		\begin{align*}
			1+\momCoeff < (1+\sqrt{\momCoeff})^2\le \lrSq\hesseEV < 2(1+\momCoeff)
		\end{align*}
		on the other hand, we get \(-1 < \sigma_2 < \sigma_1 < 0\) which implies 
		an oscillating convergence (``Oscillation''). In contrast to the ``Ripples''
		we switch the side of the distance to the minimum in every iteration.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Appendix Theorem~\ref{thm-appdx: momentum - stable set of parameters}
\end{proof}

In particular (positive) momentum extends the available range for convergent
learning rate/eigenvalue products which allows for larger learning rates. As
can be seen very well in Figure~\ref{fig: annotated heavy ball rates}, the
wider attraction area allows for moving out the largest eigenvalues to the
right with larger eigenvalues shifting the smaller eigenvalues towards the
right as well into more acceptable convergence rates. If the smallest and
largest eigenvalue are far apart (i.e. the condition number \(\condition\) is
high) this should be very useful.

So how much do we gain? In other words what are the optimal rates of
convergence? Since the plot looks quite symmetric it is quite intuitive that
we only have to consider the largest and smallest eigenvalue. For now we
can consider it as a lower bound:
\begin{align}\label{eq: lower bound best heavy ball rates}
	\min_{\lrSq,\momCoeff}\max_{i=1,\dots,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
	\ge \min_{\lrSq}\min_{\momCoeff}
	\max_{i=1,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
\end{align}
Now let us consider the inner minimum first. Let us inspect the claim
\begin{align}
	\arg\min_{\momCoeff}\max_{i=1,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
	= \max\{(1-\sqrt{\lrSq\hesseEV_1})^2, (1-\sqrt{\lrSq\hesseEV_\dimension})^2\}
\end{align}\label{eq: optimal heavy ball momentum - ev representation}
In that case we are still (barely) in the complex case, therefore the rate
of convergence is equal to \(\sqrt{\momCoeff}\), i.e.
\begin{align*}
	\min_{\momCoeff}\max_{i=1,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
	= \max\{(1-\sqrt{\lrSq\hesseEV_1}), (1-\sqrt{\lrSq\hesseEV_\dimension})\}
\end{align*}
Now let us wiggle around \(\momCoeff\) a bit. If we would increase it, then
we are obviously increasing our rate since it is then still equal to
\(\sqrt{\momCoeff}\) as we just expanded the size of the complex case. Now the
question is what happens if we decrease it. Now by our selection of \(\momCoeff\)
either the smallest or largest eigenvalue sits right at the edge of the real 
case. If we can show that our absolute eigenvalue maximum is increasing if
we move further into the real case (decrease \(\momCoeff\)), we would have
proven that we can only become worse off doing so. This is annoyingly technical
so it is covered in Lemma~\ref{lem-appdx: just at the border of complex case is
best beta} in the appendix.

Now since both the largest and smallest eigenvalue are included in the complex
case, we know that all eigenvalues in between must also be in the complex case
which means that (\ref{eq: lower bound best heavy ball rates}) is an equality
and
\begin{align*}
	\min_{\lrSq,\momCoeff}\max_{i=1,\dots,\dimension}\{|\sigma_{i1}|,|\sigma_{i2}|\}
	= \min_{\lrSq}\max\{(1-\sqrt{\lrSq\hesseEV_1}), (1-\sqrt{\lrSq\hesseEV_\dimension})\}.
\end{align*}
Now we are just doing our balancing act  again, only with 
\(\sqrt{\hesseEV_1},\sqrt{\hesseEV_\dimension}\) instead of
\(\hesseEV_1,\hesseEV_\dimension\). But we know the solution to that already
from (\ref{eq: optimal SGD lr eigenvalue representation}),
which is
\begin{align*}
	\lr_* = \sqrt{\lrSq_*} = \frac{2}{\sqrt{\hesseEV_1}+\sqrt{\hesseEV_\dimension}}
\end{align*}
%
Since this makes both of them equally bad, we can plug this learning rate into
either equation from (\ref{eq: optimal heavy ball momentum - ev representation})
to get
%
\begin{align*}
	\momCoeff_*=\left(1-\frac{2}{1+\sqrt{\kappa}}\right)^2
\end{align*}
%
which results in the optimal rate of convergence of
%
\begin{align*}
	\sqrt{\momCoeff_*} = 1-\frac{2}{1+\sqrt{\kappa}}.
\end{align*}
%
And this is in fact \emph{equal} to our lower bound (not even up to a constant!) in
Theorem~\ref{thm: strong convexity complexity bound}. So on quadratic functions
we can not do any better without violating our assumptions about possible
optimizing algorithms (Assumption~ \ref{assmpt: parameter in generalized linear
hull of gradients}).

\subsection{Why Consider Alternatives?}

Now while Nesterov's Momentum breaks this assumption as it uses gradients at a
different point than the iterates, it does not break it fundamentally. The
assumption applies to the intermediate states \(y_n\) which are not too far
from our actual iterates and would thus allow us to make similar statements
about them with a bit of work.

So why should we even considering Nesterov's Momentum then?

\subsubsection{Weakest Link Contraction Factor}

The first issue you
might have noticed is: Since \emph{all eigenspaces} are in the complex case,
all of them have the worst contraction factor \(\sqrt{\momCoeff_*}\). In stochastic
gradient decent on the other hand the contraction factor is \(|1-\lr_*\hesseEV_i|\)
for the eigenspace \(i\), which can be considerably better than the worst
contraction factor 
\[|1-\lr_*\hesseEV_\dimension|=|1-\lr_*\hesseEV_1|.\]
To improve the convergence rate of the worst eigenspaces we have therefore
sacrificed the (possibly excellent) convergence rate of all other eigenspaces.

\subsubsection{No General Convergence at these Rates}

While
\textcite[pp. 65-67]{polyakIntroductionOptimization1987} used the eigenvalue
analysis of \(\nabla\Loss^2(\weights_*)\) to extend this rate of convergence
to a local area around \(\weights_*\), \textcite[pp. 78-79]{lessardAnalysisDesignOptimization2016}
gave an example of an \(\strongConvex{\lbound}{\ubound}\) function where
the heavy ball method with these ``optimal'' parameters is trapped in an attractive
cycle instead of converging. 
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/hb_counterexample.pdf_tex}
	\caption{
		\citeauthor{lessardAnalysisDesignOptimization2016}'s counterexample.
		Notice that the y-axis is much coarser than the x-axis and gradients are
		therefore much steeper than apparent. At point \(-1.8\) the gradient is
		quite steep and the next parameter is flung across the minimum to
		\(2.12\). Due to the less convex segment on this side, the gradient is
		not quite as steep here, so the iterate does not make it to the other
		side again which would then cause a bouncing back and forth to the
		minimum. Instead it stays on this side, which causes the next gradient
		at \(0.65\) together with the built momentum in this direction to move the
		iterate back out to our first point.
	}
	\label{fig: heavy ball counterexample}
\end{figure}

Since we know \(\lbound\) and \(\ubound\) of their counterexample function from
Figure~\ref{fig: heavy ball counterexample}, we know the parameters \(\lrSq_*\)
and \(\momCoeff_*\) which they used to explicitly write out the momentum recursion
with \(\weights_{n+1}, \weights_{n}, \weights_{n-1}\). Assuming a three-cycle
this left them with just three linear equations which results in the three-cycle
displayed in the Figure. Since we start with zero momentum, one now just has to
find a point to enter this cycle. Additional to solving this initialization
problem, \textcite[pp. 93-94]{lessardAnalysisDesignOptimization2016} also show
that this cycle is stable under noise (attractive).

\textcite{ghadimiGlobalConvergenceHeavyball2015} salvages some of this wreckage
by proving general convergence results with tighter restrictions on hyper
parameters. In particular for \(\Loss\in\strongConvex{\lbound}{\ubound}\) and
\begin{align*}
	\momCoeff\in[0,1),
	\qquad \lrSq\in \left(0, \frac{2(1-\momCoeff)}{\ubound+\lbound}\right)
\end{align*}
they can guarantee linear convergence with an explicit, but complicated
\fxnote{take a closer look?}{contraction factor}. Notice that this is
considerably weaker than the condition
\begin{align*}
	0<\lrSq\lbound \le \dots \le \lrSq\ubound < 2(1+\momCoeff)
\end{align*}
from Theorem~\ref{thm: momentum - stable set of parameters}. In particular
momentum stops increasing the selection space for learning rates, just like
\(\lbound\) which rules out our optimal parameters immediately. 

Similarly they prove that for \(\Loss\in\lipGradientSet{\ubound}\) and
\begin{align*}
	\momCoeff\in[0,1),
	\qquad \lrSq\in \left(0, \frac{(1-\momCoeff)}{\ubound}\right]
\end{align*}
that the running minimum of loss evaluation converges to the minimal loss,
with their best guarantee achieved for \(\lrSq =(1-\momCoeff)/\ubound\)
resulting in
\begin{align*}
	\min_{k=0,\dots,n}\Loss(\weights_k) -\Loss(\weights_*)
	\le \frac{1}{1-\momCoeff}\frac{\ubound\|\weights_0-\weights_*\|^2}{2(n+1)}
\end{align*}
which provides a tighter convergence bound than 
Theorem~\ref{thm: convex function GD loss upper bound} for gradient decent 
without momentum \(\momCoeff=0\), but is actually harmed by momentum.
\fxnote{Check out improvements by \cite{sunNonErgodicConvergenceAnalysis2019}}

\section{Nesterov's Momentum Convergence}

The Eigenvalue analysis of Nesterov's Momentum starts out very similar and we
easily obtain the analog eigenspace transformation to (\ref{eq: eigenspace
momentum transformation})
%
\begin{align*}
	\Sigma_i = \begin{pmatrix}
		0 & 1\\
		-\momCoeff(1-\lrSq\hesseEV_i) & (1+\momCoeff)(1-\lrSq\hesseEV_i).
	\end{pmatrix}	
\end{align*}
%
which curiously reduces to the heavy ball version if we use
\begin{align}\label{eq: tilde momentum}
	\tilde{\momCoeff}_i = \momCoeff(1-\lrSq\hesseEV_i)
\end{align}
We therefore immediately get from Theorem~\ref{thm: momentum - stable set of
parameters} that \(\max\{|\sigma_{i1}|,|\sigma_{i2}|\}<1\) if and only if
\begin{align*}
	0<\lrSq\hesseEV_i < 2(1+\tilde{\momCoeff}_i)
	\quad \text{and} \quad
	|\tilde{\momCoeff}_i|<1
\end{align*}
%
which after some transformations is itself equivalent to
\begin{align*}
	0<\lrSq\hesseEV_i < 1+\frac{1}{1+2\momCoeff}
	\quad \text{and} \quad
	|1-\lrSq\hesseEV_i|<\frac{1}{|\momCoeff|}.
\end{align*}
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/annotated_nesterov_rates.pdf_tex}
	\caption{
		Heat plot of \(\max\{|\sigma_1|,|\sigma_2|\}\) for the Nesterov Momentum.
		``M'' and ``O'' stand for the ``Monotonic'' and ``Oscillating'' types of 
		real eigenvalues. Note that the separating line of these cases
		\(\lrSq\hesseEV=1+\tilde{\momCoeff}=1+\momCoeff(1-\lrSq\hesseEV)\)
		reduces to \(\lrSq\hesseEV=1\) but since we divide by \(1+\momCoeff\)
		the sides flip at \(\momCoeff=-1\). Again the third label is omitted to
		keep the remarkably sharp border unobstructed.
	}
	\label{fig: annotated nesterov rates}
\end{figure}

In a sense Nesterov's Momentum provides every eigenspace with its own
heavy ball momentum \(\tilde{\momCoeff}_i\) which is a good sign if we want
to avoid the ``Weakest Link Contraction Factor'' issue we had in heavy ball
momentum. And if we are in the complex case the convergence rate is
\(\sqrt{\momCoeff(1-\lrSq\hesseEV_i)}\) which looks like the root of the gradient
decent convergence rate multiplied by a factor we could make smaller than one.

To ensure all eigenspaces are in the complex case, we first need to ensure that
all \(\tilde{\momCoeff}_i\) are positive which requires
\begin{align*}
	1-\lrSq\hesseEV_i >= 0
\end{align*}
So let us select \(\lrSq= 1/\hesseEV_\dimension\). Now we just need to ensure
that the smallest eigenvalue is in the complex case, for that set it to be
right on the border \(\tilde{\momCoeff}_1=(1-\sqrt{\lrSq\hesseEV_1})^2\),
which then results in
\begin{align*}
	\momCoeff = \frac{(1-\sqrt{\lrSq\hesseEV_1})^2}{1-\lrSq\hesseEV_1}
	=\frac{1-\sqrt{\lrSq\hesseEV_1}}{1+\sqrt{\lrSq\hesseEV_1}}
	=\frac{1-\sqrt{1/\condition}}{1+\sqrt{1/\condition}} (< 1).
\end{align*}
Since
\begin{align*}
	\tilde{\momCoeff}_1 >= \dots >=\tilde{\momCoeff}_\dimension=0
\end{align*}
the worst rate convergence is exhibited by the first eigenspace and is equal to
\begin{align}\label{eq: nesterov convegence rate eigenvalue derivation}
	\sqrt{\tilde{\momCoeff}_1} = \sqrt{(1-\sqrt{\lrSq\hesseEV_1})^2}
	= 1-\sqrt{1/\condition} = 1- \frac{1}{\sqrt{\condition}}.
\end{align}
%
This rate of convergence is a tiny bit worse than the optimal heavy ball
convergence rate \(1-2/(1+\sqrt{\condition})\), which is also the complexity
bound, but it is quite similar.

Now if you had a look at Figure~\ref{fig:
annotated nesterov rates} you might have noticed that for a fixed learning
rate the \(\lrSq\hesseEV_i\) are \(\dimension\) vertical lines. We made sure
that the rightmost line was right on top of the black line and then moved up
our momentum term until all our rates where in the ripples area. From the
plot you might get the intuition that we can improve convergence if we
increase the learning rate to move the largest eigenvalue into the oscillating
range just like in the gradient decent. And this is in fact not the optimal
selection of learning rates. \textcite{lessardAnalysisDesignOptimization2016}
claim\footnote{The proof is left to the reader} that optimal tuning results in
\begin{align*}
	\lrSq = \frac{4}{3\ubound + \lbound}
	\quad \text{and} \quad
	\momCoeff = \frac{\sqrt{3\condition+1}-2}{\sqrt{3\condition+1}+2}
\end{align*}
with learning rate
\begin{align*}
	1-\frac{2}{\sqrt{3\condition+1}}.
\end{align*}
But as we have learned with
Heavy Ball Momentum, optimal learning rates are no use if we can not generalize
them.

\subsection{Estimating Sequences}

To prove convergence for gradient decent we proved that gradient decent
minimizes a ``local'' upper bound (Lemma~\ref{lem: smallest upper bound}). Local
in the sense that we only use the gradient information at our current position
and discard all previous gradient information. To prove better convergence rates
we need to utilize the previous gradients too.

Let us start with an upper bound of the form
%
\begin{align}\label{eq: Phi_0 (upper bound - kinda)}
	\Phi_0(\weights) = \underline{\Phi}_0 + \tfrac{\lbound_0}{2}\|\weights-z_0\|^2.
\end{align}
%
One example for such an upper bound is
%
\begin{align*}
	\weights \mapsto
	\Loss(y_0)
	+ \langle\nabla\Loss(y_0), \weights - y_0\rangle
	+ \tfrac{\ubound}{2}\|\weights -y_0\|^2.
\end{align*}
%
Which can be written as in (\ref{eq: Phi_0 (upper bound - kinda)}) with
\(\lbound_0=\ubound\) by selecting
%
\begin{align*}
	z_0 = y_0 - \ubound^{-1}\nabla\Loss(y_0).
\end{align*}
This is the same technique as we used in (\ref{eq: newton minimum approx}). Keep
in mind that our upper bound is a quadratic function which therefore
is equal to its second taylor derivative.

Now since we started with an upper bound, it is trivial to find \(\weights_0\)
such that
\begin{align*}
	\Loss(\weights_0) \le \min_{\weights}\Phi_0(\weights)
\end{align*}
as we can simply select \(\weights_0=z_0\) to get
\begin{align*}
	\Loss(\weights_0) =\Loss(z_0) \le \Phi_0(z_0) = \min_{\weights}\Phi_0(\weights).
\end{align*}
Now the idea is to morph our upper bound \(\Phi_0\) via \(\Phi_n\) slowly into
a lower bound while keeping 
\begin{align}\label{eq: staying ahead of the estimates}
	\Loss(\weights_n) \le \min_{\weights}\Phi_n(\weights)=:\underline{\Phi}_n.
\end{align}
This would force \(\weights_n\) towards the minimum as it needs to stay smaller
than \(\Phi_n\) which changes into a lower bound of \(\Loss\). By controlling
the speed of morphing we will be able to bound the convergence speed. But we
can not morph too fast otherwise we will not be able to stay ahead of it, i.e.
fulfill (\ref{eq: staying ahead of the estimates}). Okay now that we have set
up the gameplan let us make this more concrete.
The obvious lower bounds we want to utilize are made up of all the gradients we
collect along the way
\begin{align*}
	\phi_n(\weights)
	:= \Loss(y_n) + \langle\nabla\Loss(y_n), \weights-y_n\rangle
	+ \tfrac{\lbound}{2}\|\weights-y_n\|^2.
\end{align*}
where this includes the non strongly convex case with \(\lbound=0\). So let
us consider
\begin{align*}
	\Phi_1 = \gamma \phi_1 + (1-\gamma)\Phi_0
\end{align*}
For \(\gamma=1\) we would have a lower bound, but if \(\nabla\Loss(y_1)\)
does not happen to be zero, the tangent point \(y_1\) is sitting on a slope and
therefore is not smaller than the minimum of \(\phi_1=\Phi_1\) and since \(\phi_1\)
is a lower bound on \(\Loss\) it will generally be impossible to satisfy
(\ref{eq: staying ahead of the estimates}) with \(\weights_1\) in that case.
So the first take-away is, that we can generally not select \(\gamma=1\)
unless we are already in the minimum. But this also hints at another issue:
A convex combination of lower bounds
\begin{align*}
	\sum_{k=0}^n \hat{\gamma}_k \phi_k(\weights)
\end{align*}
is also a lower bound. But since they generally only have one tangent point
which is equal and are otherwise strictly lower, it is difficult to find
a \(\weights\) such that \(\Loss(\weights)\) is smaller than its minimum.
So the \(y_k\) already need to be equal to the minimum
of \(\Loss\) such that the \(\phi_k\) start out with no slope and their
minimum is also equal to \(\weights_*\). If we have
\begin{align*}
	\sum_{k=1}^n \hat{\gamma}_k \phi_k(\weights)
	+ \hat{\gamma}_0 \Phi_0(\weights)
\end{align*}
we get a bit more wiggle room but if we want to slowly remove our upper bound
we run into a similar problem. Therefore we also need to slowly remove the old
\(\phi_k\) upper bounds, constructed with \(y_k\) far away from the minimum
\(\weights_*\). This motivates why we might wish to select
\begin{align}\label{eq: estimating sequence recursion}
	\Phi_{n+1}(\weights) := \gamma_n \phi_{n+1}(\weights) + (1-\gamma_n)\Phi_n(\weights)
\end{align}
causing an exponential decay not only for the initial upper bound, but also for
our previous lower bounds. For
\begin{align*}
	\Gamma_k^n := \prod_{i=k}^{n-1} (1-\gamma_i)
\end{align*}
one can inductively convert the recursion (\ref{eq: estimating sequence
recursion}) into
\begin{align}
	\nonumber
	\Phi_n(\weights)
	&= \sum_{k=1}^n \Gamma_k^n\gamma_{k-1} \phi_k(\weights)
	+ \Gamma_0^n\Phi_0(\weights) \\
	\label{eq: estimating sequence explicit}
	&\le (1-\Gamma_0^n)\Loss(\weights) + \Gamma_0^n\Phi_0(\weights)
\end{align}

\begin{lemma}
	\label{lem: estimating sequence convergence speed}
	If the \(\weights_n\) satisfy (\ref{eq: staying ahead of the estimates}),
	then we have
	\begin{align*}
		\Loss(\weights_n) - \Loss(\weights_*)
		\le \Gamma_0^n (\Phi_0(\weights_*)-\Loss(\weights_*))
	\end{align*}
	and
	\begin{align*}
		\sum_{k=0}^\infty \gamma_k = \infty
	\end{align*}
	is sufficient for \(\Gamma_0^n\to 0\).
\end{lemma}
\begin{proof}
	Both statements are fairly easy to prove:
	\begin{align*}
		\Loss(\weights_n)
		\xle{(\ref{eq: staying ahead of the estimates})}
		\min_{\weights}\Phi_n(\weights)
		&\xle{(\ref{eq: estimating sequence explicit})} \min_{\weights} \{
		(1-\Gamma_0^n) \Loss(\weights) + \Gamma_0^n\Phi_n(\weights) \}\\ 
		&\le (1-\Gamma_0^n) \Loss(\weights_*) + \Gamma_0^n\Phi_n(\weights_*)
	\end{align*}
	Subtracting \(\Loss(\weights_*)\) from both sides lets us move on to the
	second statement:
	\begin{align*}
		\Gamma_0^{n+1} &= \prod_{i=0}^n (1-\gamma_i)
		\le\prod_{i=0}^n \exp(-\gamma_i)
		= \exp\left(-\sum_{i=0}^n\gamma_i\right) \to 0
		\qedhere
	\end{align*}
\end{proof}

Now we just need to make sure that we can actually satisfy (\ref{eq: staying
ahead of the estimates}) while making \(\gamma_k\) as large as possible.
For that we want an explicit representation of \(\underline{\Phi}_{n}\).
Fortunately we are only adding up quadratic functions therefore the function
\(\Phi_{n}\) is quadratic too and can thus be written in the form
\begin{align}\label{eq: centered Phi representation}
	\Phi_{n}(\weights) = \underline{\Phi}_{n} + \tfrac{\lbound_{n}}{2}\|\weights - z_n\|^2
\end{align}
where our convexity parameter \(\lbound_n\) morphs from \(\lbound_0\) to
\(\lbound\)
\begin{align*}
	\lbound_n =(1-\Gamma_0^n)\lbound + \Gamma_0^n \lbound_0
	= \gamma_{n-1} \lbound + (1-\gamma_{n-1})\lbound_{n-1}
\end{align*}
and \(z_n\) is the minimum. Assuming (\ref{eq: staying ahead of the estimates})
holds for \(n\) we can write our minimal value as
\begin{align*}
	\underline{\Phi}_{n+1}
	&= \Phi_{n+1}(y_{n+1}) - \tfrac{\lbound_{n+1}}{2}\|y_{n+1} - z_{n+1}\|^2 \\
	&\lxeq{(\ref{eq: estimating sequence recursion})}
	\gamma_n \underbrace{\phi_{n+1}(y_{n+1})}_{
		=\Loss(y_{n+1})
	}
	+ (1-\gamma_n)\underbrace{\Phi_n(y_{n+1})}_{
		= \underbrace{\underline{\Phi}_n}_{
			\ge \Loss(\weights_n)
			\mathrlap{\ge \Loss(y_{n+1}) + \langle \Loss(y_{n+1}), \weights_n - y_{n+1}\rangle}
		} +\mathrlap{\tfrac{\lbound_n}{2}\|y_{n+1} - z_n\|^2}
	} - \tfrac{\lbound_n}{2}\|y_n - z_n\|^2 \\
	&\ge \Loss(y_{n+1}) + \text{``junk''}.
\end{align*}
Now you might recall that \(y_{n+1}\) are our intermediate ``information
collection points'' which then define the actual iterate as
\begin{align*}
	\weights_{n+1} = y_{n+1} - \lrSq \nabla\Loss(y_{n+1})
\end{align*}
in Nesterov's Momentum method (Definition~\ref{def: nesterov's momentum}).
Now from Lemma~\ref{lem: smallest upper bound} we know that we can achieve
\begin{align*}
	\Loss(y_{n+1}) - \tfrac{1}{2\ubound}\|\nabla\Loss(y_{n+1})\|^2
	\ge \Loss(\weights_{n+1})
\end{align*}
by selecting the learning rate \(\lrSq=1/\ubound\). We can therefore satisfy
(\ref{eq: staying ahead of the estimates}) if we can lower bound our ``junk''
by \( - \tfrac{1}{2\ubound}\|\nabla\Loss(y_{n+1})\|^2\). Now our ``junk'' is equal to
\begin{align*}
	(1-\gamma_n)\left(
		\langle \Loss(y_{n+1}), \weights_n - y_{n+1}\rangle
		+ \tfrac{\lbound_n}{2}\|y_{n+1} - z_n\|^2
	\right)
	- \tfrac{\lbound_{n+1}}{2}\|y_{n+1} - z_{n+1}\|^2,
\end{align*}
which tells us we should probably have a closer look at \(z_{n+1}\) and maybe
express it in relation to \(z_n\).

As we know that \(z_{n+1}\) is the unique
minimum of \(\Phi_{n+1}\) so we can find it with
\begin{align*}
	0\xeq{!}\Phi'_{n+1}(z_{n+1})
	= \gamma_n\underbrace{\phi_{n+1}'(z_{n+1})}_{
		= \nabla\Loss(y_{n+1}) \mathrlap{+ \lbound(z_{n+1}-y_{n+1})}
	} +(1-\gamma_n)
	\underbrace{\Phi'_n(z_{n+1})}_{
		=\lbound_n\mathrlap{(z_{n+1}-z_n)}
	},
\end{align*}
where we have used the representation (\ref{eq: centered Phi representation})
for \(\Phi_n\). It is also quite reassuring that the gradient at \(y_{n+1}\)
which we ultimately need makes its first appearance here. Collecting all
the \(z_{n+1}\) we get
\begin{align*}
	\overbrace{\lbound_{n+1}}^{
		=\mathrlap{\gamma_n\lbound + (1-\gamma_n)\lbound_n}
	}z_{n+1}
	= \gamma_n\lbound y_{n+1} + (1-\gamma_n)\lbound_n z_n
	- \gamma_n\nabla\Loss(y_{n+1}).
\end{align*}
Subtracting \(\lbound_{n+1} y_{n+1}\) results in
\begin{align*}
	\lbound_{n+1}(z_{n+1}-y_{n+1})
	= (1-\gamma_n)\lbound_n(z_n-y_{n+1})
	- \gamma_n\nabla\Loss(y_{n+1}).
\end{align*}
Plugging this into the last part of our ``junk'', we get
\begin{align*}
	\tfrac{\lbound_{n+1}}2\|y_{n+1}-z_{n+1}\|^2
	&= \tfrac1{2\lbound_{n+1}}
	\underbrace{
		\|{\scriptstyle (1-\gamma_n)\lbound_n}(z_n-y_{n+1})
		- {\scriptstyle \gamma_n}\nabla\Loss(y_{n+1})\|^2
	}_{
	\begin{aligned}
		=&{\scriptstyle (1-\gamma_n)^2\lbound_n^2}\|z_n-y_{n+1}\|^2 \\
		&- {\scriptstyle 2(1-\gamma_n)\lbound_n\gamma_n}\langle
		\nabla\Loss(y_{n+1}), z_n-y_{n+1}\rangle \\
		&+ {\scriptstyle\gamma_n^2}\|\nabla\Loss(y_{n+1})\|^2
	\end{aligned}
	}
\end{align*}
Therefore our ``junk'' is equal to
\begin{align*}
	-\tfrac{\gamma_n^2}{2\lbound_{n+1}}\|\nabla\Loss(y_{n+1})\|^2
	\begin{aligned}[t]
		&+ {\scriptstyle(1-\gamma_n)}\left\langle \Loss(y_{n+1}),
		(\weights_n -y_{n+1})
		+ \tfrac{\lbound_n\gamma_n}{\lbound_{n+1}}(z_n - y_{n+1})
		\right\rangle\\
		&+\tfrac{(1-\gamma_n)\lbound_n}{2}
		\underbrace{\left(1-\tfrac{(1-\gamma_n)\lbound_n}{\lbound_{n+1}}\right)}_{
			= \tfrac{\gamma_n \lbound}{\lbound_{n+1}} \ge 0
		}
		\|y_{n+1} - z_n\|^2.
	\end{aligned}
\end{align*}
As the last summand is positive we can discard it and end up with two equations
to achieve our lower bound
\begin{subequations}
\begin{align}
	\label{eq: morphing speed equation}
	\frac{1}{\ubound}
	&= \frac{\gamma_n^2}{\lbound_{n+1}}
	\iff \ubound \gamma_n^2 = \lbound_{n+1} = \gamma_n \lbound +  (1-\gamma_n)\lbound_n
	\\
	0
	&= (\weights_n -y_{n+1}) + \tfrac{\lbound_n\gamma_n}{\lbound_{n+1}}(z_n - y_{n+1})
	\xiff{\text{reorder}}
	y_{n+1}
	= \tfrac{\lbound_{n+1}\weights_n + \lbound_n\gamma_n z_n}{\gamma_n\lbound + \lbound_n}
\end{align}
\end{subequations}
As we are allowed to select \(\gamma_n\) and \(y_{n+1}\) in iteration \(n\) we
can ensure that these equations are in fact fulfilled.

Now while \(\Phi_0\) being an upper bound was nice to build intuition, we do
not actually need it to be an upper bound. The only thing we really need is
for equation (\ref{eq: staying ahead of the estimates}) and (\ref{eq: centered
Phi representation}) to hold for \(n=0\) so that we can do our induction. In
particular picking any \(\weights_0\) and selecting
\begin{align*}
	\Phi_0(\weights):= \Loss(\weights_0) + \tfrac{\lbound}{2}\|\weights_0 - \weights\|^2
\end{align*}
works as well. And in that case we have \(\lbound_n = \lbound\) for all \(n\)
which simplifies (\ref{eq: morphing speed equation}) to
\begin{align*}
	\gamma_n = \frac1{\sqrt{\condition}} \implies \Gamma_0^n = (1-\tfrac{1}{\sqrt{\condition}})^n
\end{align*}
With Lemma~\ref{lem: estimating sequence convergence speed} in mind this implies
that we get our convergence rate (\ref{eq: nesterov convegence rate eigenvalue derivation})
we derived with eigenvalues back! But since \(\lbound=0\) implies \(\condition=\infty\)
which would set our rate of convergence to one, we are going to continue
to entertain the general case with general \(\lbound_0\).

\(y_{n+1}\) is currently looking a lot more strange than our simple momentum
movement in Definition~\ref{def: nesterov's momentum}, we still need to fix that.

% \section{Complexity Bounds}

\section{Adam \& Nadam}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
