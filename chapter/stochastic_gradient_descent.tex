% !TEX root = ../Masterthesis.tex

\chapter{Stochastic Gradient Descent (SGD)}\label{chap: sgd}

So far we have only discussed the minimization of a function we can get accurate
gradient information from. If our loss function is stochastic the best we can
do so far, is to minimize the empirical risk
\begin{align*}
	\Loss_\sample(\weights) = \frac{1}{\sampleSize}\sum_{k=1}^\sampleSize \loss(\weights, Z_k)
	= \E_{Z\sim \dist_\sample}[\loss(\weights, Z)]
\end{align*}
for some sample \(\sample=(Z_1,\dots,Z_\sampleSize)\) inducing the empirical
distribution \(\dist_\sample\). Now if we minimize \(\Loss_\sample\) to
get close to the minimum of \(\Loss\) we are really applying a type of triangle
inequality \parencite[e.g.][]{bottouOptimizationMethodsLargeScale2018}
\begin{align*}
	&\Loss(\hat{\weights}) - \Loss(\minimum)\\
	&=
	\underbrace{\Loss(\hat{\weights}) - \Loss_\sample(\hat{\weights})}_{
	\le \sup_{\weights}|\mathrlap{\Loss_\sample(\weights)-\Loss(\weights)|}
	}
	{
	+ \Loss_\sample(\hat{\weights}) - \Loss_\sample(\minimum^{(\sample)})
	+ \underbrace{
		\scriptstyle
		\Loss_\sample(\minimum^{(\sample)}) - \Loss_\sample(\minimum)
	}_{\le 0}
	}
	+ \underbrace{
		\Loss_\sample(\minimum)- \Loss(\minimum)
	}_{\le \sup_{\weights}|\Loss_\sample(\weights)-\Loss(\weights)|}
	\\
	&\le \underbrace{\Loss_\sample(\hat{\weights}) - \Loss_\sample(\minimum^{(\sample)})}_{
		\text{apply GD results}
	}
	+ 2\underbrace{\sup_{\weights}|\Loss_\sample(\weights)-\Loss(\weights)|}_{\text{Statistical Learning}}
\end{align*}
where \(\minimum^{(\sample)}\) minimizes \(\Loss_\sample\). Bounding the
distance between the theoretical and empirical risk uniformly is the topic
of statistical learning. In general this distance can be bounded with some
measure (the Vapnik-Chervonenkis (VC) dimension) of how expressive the
hypothesis class \(\{\model\}\) is \parencite{vapnikOverviewStatisticalLearning1999}.
In general the more dimensions our weight vectors \(\weights\) have the larger
the VC-dimension, but VC dimensions try to remove ``duplicates'', i.e.
parameters which do not actually allow for different models, e.g. chaining
linear functions only results in a linear function and therefore does not
increase the expressiveness of our hypothesis class. Nevertheless deep learning
models should still be highly over-parametrized according to this theory and
have no business generalizing to unseen examples, yet they seem to do.

To solve this mystery one has to make the upper bounds more algorithm specific
and avoid taking the supremum over \emph{all} possible parameters \(\weights\).
Instead we want to consider the weights that our algorithm would actually pick.
E.g. for an algorithm \(\algo\) \textcite{hardtTrainFasterGeneralize2016} use
the upper bound
\begin{align*}
	&\Loss(\algo(\sample)) - \Loss(\minimum)\\
	&\le \underbrace{\Loss(\algo(\sample)) - \Loss_\sample(\algo(\sample))}_{
		=: \epsilon_\text{gen}^\sample(\algo(\sample))
	}
	+ \underbrace{\Loss_\sample(\algo(\sample)) - \Loss_\sample(\minimum^{(\sample)})}_{\text{optimization error (GD)}}
	+ \Loss_\sample(\minimum^{(\sample)}) - \Loss(\minimum).
\end{align*}
In their analysis in expectation over \(\sample\) (and \(\algo\) if it is a
randomized algorithm), they use
\begin{align*}
	\Loss(\minimum) &= \inf_{\weights}\E[\loss(\weights, Z)]
	= \inf_{\weights}\E\underbrace{
		\left[\frac{1}{\sampleSize}\sum_{k=1}^\sampleSize\loss(\weights, Z_k)\right]
	}_{\ge \Loss_\sample(\minimum^{(\sample)})}
	\ge \E[\Loss_\sample(\minimum^{(\sample)})]
\end{align*}
to get rid of the last term and focus on the expected generalization error
\begin{align*}
	\epsilon_{\text{gen}}:=\E[\epsilon_{\text{gen}}^\sample]
	=\E[\Loss(\algo(\sample)) - \Loss_\sample(\algo(\sample))].
\end{align*}
Their bounds are much better in the sense that they are independent of
the (VC-)dimension of parameters. They only depend on the amount of data and
the sum of learning rates of all iterations in the convex case. Recall the sum of
all learning rates is equal to the time we let gradient flow run if we view
gradient descent as a discretization of this flow.\footnote{
	They also discuss the strongly convex case with compact parameter set, where
	one can get rid of this time dependency with the fact that gradient descent
	has contractive properties, i.e. starting from two different points and
	making one gradient step results in weights being closer to each other. 
	In a compact and thus bounded set the length of the path of movement gradient
	flow takes is thus finite and this is kind of the intuition why the length
	of time can be discussed away here.

	They also discuss the non-convex case where the relation to the time worsens,
	i.e. the bound is no longer proportional to the time spent in gradient flow
	but rather worsens faster than our time increases.
} And it is perhaps not too surprising that if we
do not run our optimizer for long, we do not get far from the start and thus
do not have time to overfit.

Stochastic Gradient Descent avoids splitting the error into generalization and
optimization error entirely which will provide us with even tighter bounds. The
price for this improved bound, is the requirement to never reuse data, i.e. we
need a source of independent \(\dist\) distributed random variables which we use
for a stochastic gradient
%
\begin{align*}
	\nabla_\weights\loss (\weights, X,Y) \qquad (X,Y)\sim\dist%\text{ or } \dist_\sample.
\end{align*}
%
If we draw from \(\dist\) the expected value of the stochastic gradient is the
gradient of the Risk \(\Loss\)\footnote{
	We need to be able to swap the expectation with differentiation. A sufficient 
	condition is continuity of \(\nabla\loss\) in \(\weights\), which allows us
	to use the mean value theorem
	\begin{align*}
		\frac{\partial}{\partial \weights_k}\E[\loss(\weights, X,Y)]
		&= \lim_{n\to\infty}
		\int\frac{\loss(\weights+\stdBasis_k/n, X,Y)-\loss(\weights,X,Y)}{1/n}d\Pr
		\\
		&=\lim_{n\to\infty} \int\frac{\partial}{\partial \weights_k}\loss(\xi_n, X,Y)d\Pr
		\qquad \xi_n \in [\weights, \weights + \stdBasis_k/n].
	\end{align*}
	Then we use the boundedness of a continuous function on the compact interval
	\([\weights, \weights + \stdBasis_k/n]\) to move the limit in using
	dominated convergence.
}.

If we were to reuse data, then we might argue we are really picking samples
from \(\dist_\sample\) not \(\dist\) and thus our results of convergence would
apply to \(\Loss_\sample\) instead of \(\Loss\). This would force us to use
these triangle inequality upper bounds again. Which (as of writing) are often so
much worse, that (even when the number of iterations go to infinity) our upper
bound is worse than if we had stopped the iteration at the number of samples
\(\sampleSize\) and used the bounds without the triangle inequality\footnote{
	E.g. \textcite{hardtTrainFasterGeneralize2016} use their bounds to prove that
	the averaging algorithm discussed in Subsec~\ref{subsec: SGD with Averaging}
	converges with rate
	\begin{align*}
		\Loss(\overline{\Weights}_n) - \Loss(\minimum)
		\le \Loss(\overline{\Weights}_n) - \E[\Loss_\sample(\minimum^{(\sample)})]
		\le \frac{\|\weights_0 -\minimum^{(\sample)}\|\sqrt{\lipConst^2 + \stdBound^2}}{\sqrt{\sampleSize}}
		\underbrace{\sqrt{\frac{\sampleSize+2n}{n}}}_{\to \sqrt{2} \quad (n\to\infty)}
	\end{align*}
}.

Of course intuitively, reusing a couple of examples should not harm
generalization that much. And since all the iterations up to \(\sampleSize\)
do not overfit in some sense, they might lead us into the vicinity of a well
generalizing minimum already. And if that is also a minimum of \(\Loss_\sample\)
further optimization with regard to \(\Loss_\sample\) might only barely hurt
generalization. And since the ``true'' model \(\minimum\) should also perform
well on \(\Loss_\sample\) this seems very plausible.

So even if it might not be realistic, we are going to assume access to an infinite
source of independent \(\dist\) distributed random variables \(((X_k,Y_k), k\ge 1)\).
We can always interpret them as drawn from \(\dist_\sample\) if appropriate.

Note that if one wants to minimize \(\Loss\) then full gradients are not possible in
the first place. But for \(\Loss_\sample\) one might compare convergence rates
of SGD and GD and deem GD on \(\nabla\Loss_\sample\) more appropriate.

But while it
is difficult to quantize due to the step change between no-reuse and reuse of
random variables, using the full gradient \(\nabla\Loss_\sample\) probably hurts
generalization. Empirically it does in fact
seem that SGD leads to wider\footnote{
\textcite{hochreiterFlatMinima1997} first suggested that flatter minima might
	generalize better since they require less precision in the weights (less
	information entropy) and are thus \emph{simpler} models.
}
(better generalizing) minima \parencite{liVisualizingLossLandscape2018}.
We will build intuition for this using mini-batches. Where we
are going to assume that they are independent within and between each other,
which is fine for small batch sizes with regard to \(\sampleSize\) but breaks
down if batch sizes approach full batches. For full batches our analysis would
only be valid for one gradient step which is obviously ridiculous.

To shorten notation we will assume that the gradient is always meant
with respect to \(\weights\) unless otherwise stated.
We define \emph{Stochastic Gradient Descent} as
%
\begin{align*}
	\Weights_{n+1}
	&= \Weights_n - \lr_n\nabla\loss(\Weights_n, X_{n+1},Y_{n+1})\\
	&= \Weights_n - \lr_n\nabla\Loss(\Weights_n)
	+ \lr_n\underbrace{
		[\nabla\Loss(\Weights_n) - \nabla\loss(\Weights_n, X_{n+1}, Y_{n+1})]
	}_{=:\martIncr_{n+1}}.
\end{align*}
Where \(\martIncr_n\) are martingale increments for the filtration
\begin{align*}
	\filtration_n :=\sigma((X_k, Y_k): k\le n )
	\qquad (X_k,Y_k)\stackrel{\text{iid}}{\sim}\dist,
\end{align*}
since \(\Weights_n\) is \(\filtration_n\)-measurable which is independent of, i.e.
\((X_{n+1},Y_{n+1})\)
\begin{align}\label{eq: conditional independence of martingale increments}
	\E[\martIncr_{n+1}\mid \filtration_n]
	= \E[\nabla\Loss(\Weights_n) - \nabla\loss(\Weights_n, X_{n+1}, Y_{n+1})\mid \filtration_n]
	= 0.
\end{align}
%
\section{ODE View}

The first way to view SGD is through the lense of approximating the ODE with
integral equation
\begin{align*}
	\weights(t) =  \weights_0 -\int_{0}^{t}\nabla\Loss(\weights(s))ds.
\end{align*}
Its Euler discretization is gradient descent 
\begin{align*}
	\weights_{t_{n+1}}:=\weights_{n+1}
	= \weights_n - \lr_n \nabla\Loss(\weights_n),
\end{align*}
where
\begin{align*}
	0=t_0 \le \dots \le t_N= T \quad \text{with} \quad t_{n+1}-t_n=\lr_n
	\quad \text{and}\quad \lr:=\max_n \lr_n
\end{align*}
and constant learning rate \(\lr\) represents an equidistant discretization.
Since \(\nabla\Loss\) is Lipschitz continuous, the local discretization error
\begin{align*}
	\|\weights(\lr)  - \weights_\lr \|
	&= \|(\weights_0 - \int_0^\lr\nabla\Loss(\weights(s))ds) - (\weights_0-\lr\nabla\Loss(\weights_0))\|\\
	&\le  \int_0^\lr \underbrace{\|\nabla\Loss(\weights_0) - \nabla\Loss(\weights(s))\|}_{
		\le \ubound \|\weights(s) - \weights(0)\| \in O(s) \mathrlap{\quad\text{(Taylor approx., Lip. \(\nabla\Loss\))}}
	}ds 
\end{align*}
is of order \(O(\lr^2)\). 
Using the discrete Gr\"onwall's inequality and stability of the ODE (due to Lipschitz continuity of
\(\nabla\Loss\) and the continuous Gr\"onwall inequality) the global discretization
error is therefore of order 1, i.e.
\begin{align*}
	\max_{n} \|\weights(t_n) - \weights_n\| \in O(\lr),
\end{align*}
\fxnote{refer to book instead of course?}{as is usually covered in ``Numerics of ODE's'' courses.}
Due to \((a+b)^2\le2(a^2+b^2)\), and therefore 
\begin{align*}
	\max_n\E[\|\weights(t_n)-\Weights_n\|^2]
	\le 2(\underbrace{\max_n \|\weights(t_n)-\weights_n\|^2}_{O(\lr^2)}
	+\max_{n}\E[\|\weights_n - W_n\|^2]),
\end{align*}
it is enough to bound the distance between SGD and GD.
\begin{theorem}\label{thm: distance SGD vs GD}
	In general we have
	\begin{align*}
		\E\|\weights(t_n)-\Weights_n\|^2
		\le \sum_{k=0}^{n-1}\lr_k^2\E\|\martIncr_{k+1}\|^2\exp\left(
			\sum_{j=k+1}^{n-1}\lr_j^2\ubound^2 + 2\lr_j\ubound
		\right)
	\end{align*}
	which can be simplified for constant learning rates \(\lr_n=\lr\) and bounded
	martingale increment variances \(\E\|\martIncr_n\|^2 \le \stdBound^2\) to
	\begin{align*}
		\max_{n}\E\|\weights(t_n)	-\Weights_n\|^2
		\le \lr T\stdBound^2\exp[T(\lr\ubound^2 + 2\ubound)] \in O(\lr)
	\end{align*}
\end{theorem}
Now before we get to the proof let us build some intuition why this result
is not surprising. Unrolling SGD we can rewrite it as
\begin{align}\label{eq: unrolled SGD}
	\Weights_{t_n}
	= \weights_0 - \sum_{k=0}^{n-1} \lr_k \nabla\Loss(\Weights_k)
	+ \sum_{k=0}^{n-1} \lr_k\martIncr_{k+1}.
\end{align}
Which is the same recursion as in GD except for the last term. Now for an
equidistance grid we have \(\lr=T/N\). Intuitively the last term in \(\Weights_T\)
should therefore disappear due to some law of large numbers for martingales
\begin{align}\label{eq: mean of martingale increments}
	\sum_{k=0}^{N-1}\lr\martIncr_{k+1} = \frac{T}{N}\sum_{k=1}^N\martIncr_k.
\end{align}
And since the variance of a sum of martingale increments is the sum of variances
as mixed terms disappear due to conditional independence (\ref{eq: conditional
independence of martingale increments})
\begin{align*}
	\E\left\|\sum_{k=1}^N\martIncr_k\right\|^2 = \sum_{k=1}^N \E\|\martIncr_k\|^2,
\end{align*}
the variance of a mean decreases with rate \(O(1/N)=O(\lr)\), which is the rate
we get in Theorem~\ref{thm: distance SGD vs GD}.
\begin{proof}[Proof (Theorem~\ref{thm: distance SGD vs GD})]
	Using the conditional independence (\ref{eq: conditional independence of martingale increments})
	we can get rid of a all the mixed terms with
	\(\E[\langle \cdot, \martIncr_{n+1}\rangle\mid\filtration_n]=0\) as everything
	else is \(\filtration_n\) measurable:
	\begin{align*}
		&\E\|\Weights_{n+1}-\weights_{n+1}\|^2
		= \E\|\Weights_n - \weights_n
		+ \lr_n(\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n))
		+ \lr_n\martIncr_{n+1}\|^2\\
		&\lxeq{(\ref{eq: conditional independence of martingale increments})}
		\begin{aligned}[t]
			&\E\|\Weights_n-\weights_n\|^2 + \lr_n^2\E\|\martIncr_{n+1}\|^2\\
			&+\underbrace{
				\lr_n^2\E\|\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\|^2
				+ 2\lr_n\E\langle\Weights_n-\weights_n,
				\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\rangle
			}_{
				\le 0 \qquad
				\text{if \(\Loss\) was convex (Lemma~\ref{lem: bermanDiv lower bound}) and }
				\lr_n<2/\ubound
			}
		\end{aligned}
	\end{align*}
	But since we do not want to demand convexity of \(\Loss\) just yet, we will
	have to get rid of these terms in a less elegant fashion. Using Lipschitz
	continuity and the Cauchy-Schwarz inequality we get
	\begin{align*}
		\|\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\|^2
		&\le \ubound^2\|\weights_n-\Weights_n\|^2\\
		\langle\Weights_n-\weights_n,
		\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\rangle
		&\le \ubound \|\weights_n-\Weights_n\|^2
	\end{align*}
	which leads us to
	\begin{align*}
		\E\|\Weights_{n+1}-\weights_{n+1}\|^2
		\le (1+\lr_n^2\ubound^2 + 2\lr_n\ubound)\E\|\Weights_n-\weights_n\|^2
		+ \lr_n^2\E\|\martIncr_{n+1}\|^2.
	\end{align*}
	Now we apply the discrete Gr\"onwall inequality (Lemma~\ref{lem-appendix:
	discrete gronwall}) to get
	\begin{align*}
		\E\|\Weights_n-\weights_n\|^2
		\le \sum_{k=0}^{n-1}\lr_k^2\E\|\martIncr_k\|^2
		\underbrace{
			\prod_{j=k+1}^{n-1}(1+\lr_j^2\ubound^2 + 2\lr_j\ubound)
		}_{
			\le \exp\left(\sum_{j=k+1}^{n-1}\lr_j^2\ubound^2 + 2\lr_j\ubound\right)
		}
	\end{align*}
	using \(1+x\le\exp(x)\) for the first claim. The rest 
	follows from \(n\lr\le T\).
\end{proof}

\subsection{Excursion: Stochastic Approximation}

Theorem~\ref{thm: distance SGD vs GD} can feel somewhat unnatural as it requires
you to completely restart SGD with smaller learning rates to get behavior closer
to GD and its ODE. If the ODE has an attraction point/area/etc. one might
instead want to prove convergence to that within one run of SGD using a
decreasing learning rate sequence. If we have a look at (\ref{eq: unrolled SGD})
it is quite intuitive that we are going to need
\begin{align*}
	\sum_{k=0}^\infty \lr_k = \lim_{n\to\infty}\sum_{k=0}^{n-1} \lr_k
	= \lim_{n\to\infty}t_n = \infty.
\end{align*}
Otherwise we can not really claim that SGD should behave similar to the asymptotic
behavior of an ODE as we can not let \(t\to\infty\) in the ODE. Additionally
we somehow need to do variance reduction to get rid of
\begin{align*}
	\sum_{k=m}^{m-1} \lr_k\martIncr_{k+1}
\end{align*}
to argue that from starting point \(\Weights_{t_m}\) onwards SGD behaves like the ODE
and therefore ends up in its attraction areas. So we also need
\(\lr_k \to 0\). How fast it needs to vanish depends on the amount of (bounded)
moments we have on \(\martIncr_k\) \parencite[p. 110]{kushnerStochasticApproximationAlgorithms1997}.
If we only have bounded second moments as we have assumed in Theorem~\ref{thm:
distance SGD vs GD} then we get the classic Stochastic Approximation conditions
on the learning rate
\begin{align*}
	\sum_{n=0}^\infty \lr_n = \infty \qquad \text{and} \qquad \sum_{n=0}^\infty \lr_n^2 <\infty
\end{align*}
I.e. learning rates \(\lr_n\) which decrease faster than \(O(1/\sqrt{n})\) but
not faster than \(O(1/n)\). See \textcite[ch.
5]{kushnerStochasticApproximationAlgorithms1997} for a proof of
convergence with probability one of the recursion
\begin{align*}
	\Weights_{n+1} = \Weights_n - \lr_n(g(\Weights_n) + \martIncr_{n+1})
\end{align*}
towards the asymptotic areas of the limiting
ODE
\begin{align*}
	\dot{\weights} = g(\weights)
\end{align*}
under those assumptions.

Note that \(g\) can be, but needs not be a gradient (we have not used that in
Theorem~\ref{thm: distance SGD vs GD} either). And this direction of
generalization is necessary to craft convergence proofs for Reinforcement
Learning where we do not get unbiased estimates of some gradients towards the
minimum, but rather unbiased estimates of a Bellman Operator (Fixed Point 
Iteration towards the optimum).

% \section{Batch Learning}

% Instead of using \(\nabla\loss(\Weights_n, X_{n+1}, Y_{n+1})\) at time \(n\) as an
% estimator for \(\nabla\Loss(\Weights_n)\) we could use the average of a
% ``batch'' of data \((X^{(i)}_{n+1}, Y^{(i)}_{n+1})_{i=1,\dots,m}\) (independently
% \(\dist\) distributed) instead, i.e.
% \begin{align*}
% 	\Weights_{n+1} = \Weights_n
% 	-\lr_n \underbrace{\frac1{m}\sum_{i=1}^m\nabla\loss(\Weights_n,X^{(i)}_{n+1}, Y^{(i)}_{n+1})}_{
% 		=:\nabla\loss_{n+1}^m(\Weights_n)
% 	}.
% \end{align*}
% leading to the modified martingale increment
% \begin{align*}
% 	\martIncr_n^{(m)}
% 	= \nabla\Loss(\Weights_{n-1})
% 	- \nabla\loss_n^m(\Weights_{n-1})
% \end{align*}
% with reduced variance
% \begin{align*}
% 	\E\|\martIncr_n^{(m)}\|^2 = \tfrac1m\E\|\martIncr_n^{(1)}\|^2 \le \tfrac1m \stdBound^2.
% \end{align*}
% Now while this variance reduction does reduce our upper bound on the distance
% between SGD and GD (cf. Theorem~\ref{thm: distance SGD vs GD}), this reduction
% comes at a cost: We now have to do \(m\) gradient evaluations per iteration!
% We would incur an equivalent cost, if we reduced our discretization to
% \begin{align*}
% 	\tilde{\lr}:=\frac{\lr}{m}=\frac{T}{Nm}
% \end{align*}
% which increases the number of iterations to reach \(T\) to \(Nm\) which implies
% \(Nm\) gradient evaluations using SGD without batches. And if we have a look
% at the upper bound in Theorem~\ref{thm: distance SGD vs GD} these two actions
% have the same effect on our distance bound between SGD and GD (especially if
% \(\Loss\) is convex as we can then get rid of the exponential term as hinted
% at in the proof of the theorem).

% Now of course an upper bound is no guarantee that there is no effect in reality.
% But if we have a look at the term (\ref{eq: mean of martingale increments})
% again which is in some sense the difference between SGD and GD, then we can
% see that these actions have quite a similar effect
% \begin{align*}
% 	\lr\sum_{k=1}^N\martIncr_k^{(m)}
% 	&=\frac{T}{N}\sum_{k=1}^N
% 	\nabla\Loss(\Weights_{k-1})- \nabla\loss_n^m(\Weights_{k-1})\\
% 	&=\frac{T}{Nm}\sum_{k=1}^N\sum_{i=1}^m
% 	[\nabla\Loss(\Weights_{k-1})-\nabla\loss(\Weights_{k-1},X^{(i)}_k, Y^{(i)}_k)]\\
% 	&\approx \tilde{\lr} \sum_{k=1}^{Nm}
% 	\underbrace{
% 		[\nabla\Loss(\Weights_{k-1})-\nabla\loss(\Weights_{k-1},X_k, Y_k)]
% 	}_{\widetilde{\martIncr}_k}.
% \end{align*}
% The difference is, that batch learning stays on some \(\Weights_k\) and collects
% \(m\) pieces of information before it makes a big step based on this information
% while SGD just makes \(m\) small steps. This also changes the true gradient
% evaluations \(\nabla\Loss(\Weights_k)\) slightly. But since we are only making
% small steps these \(\Weights_k\) will generally be similar. We still can not
% make a definitive assertion which one is better though\footnote{
% 	Whether or not you think batch-less SGD is superior might be correlated with
% 	whether or not you think that agile software development is a good idea
% }.

% Assuming they have the same effect on the deviation of SGD from GD we can
% make a few practical considerations:
% \begin{itemize}
% 	\item A smaller learning rate benefits GD \emph{and} reduces the distance
% 	between GD and SGD while batches only do the latter. If we have little data
% 	and want to achieve good results in as few gradient evaluations as possible
% 	we should in general choose smaller learning rates over larger batch sizes.
% 	\item SGD can not be easily parallelized since one needs the previous weights
% 	to calculate the next weights. Making multiple gradient evaluations at
% 	the same point is trivially parallelizable on the other hand and only
% 	requires broadcasting the result for summation. So if the bottleneck is
% 	computation time, it makes sense to select batch sizes equal to the number of
% 	threads, or if the broadcasting and summing takes significant time, batch
% 	sizes which are multiples of the number of threads.

% 	This is not without caveats though: Increasing the batch size to speed up
% 	learning requires a proportional increase in the learning rate to actually
% 	have an impact on the time required for optimization. And since GD stops
% 	converging if learing rates become too large, there is an upper bound on this.
% 	And while batches might be ``free'' in a computational sense due to
% 	parallelization, they still increase the sample consumption. And if we only
% 	have limited data, this often results in more passes over the same data and
% 	thus overfitting.
	
% 	Last but not least there might be other opportunities for parallelization
% 	like generating ensemble models by optimizing from different starting points
% 	or using different models altogether.
% \end{itemize}
% In summary: If you are concerned about the \emph{quality} of your model you should
% probably choose SGD without batches, while concerns about training time justify
% larger batch sizes in some cases.

% \fxnote{
% \textcite{hardtTrainFasterGeneralize2016} \textcite{hofferTrainLongerGeneralize2018}
% }

\section{Quadratic Loss Functions}

While we have proven SGD behaves roughly like GD for small learning rates, we
are not really that interested in how close SGD is to GD but rather how good it
is at optimizing \(\Loss\). So to build some intuition let us consider a
quadratic Loss function again before we get to general convex functions.
Using the same trick we used in Section~\ref{sec: visualize gd} 
\begin{align*}
	\nabla\Loss(\weights)
	= \nabla^2\Loss(\weights)(\weights-\minimum)
	= H(\weights-\minimum)
\end{align*}
we can rewrite SGD (by induction using the triviality of \(n=0\)) as
\begin{align}
	\nonumber
	&\Weights_{n+1}-\minimum\\
	\nonumber
	&= \Weights_n - \minimum - \lr_n H(\Weights_n-\minimum) + \lr_n\martIncr_{n+1}\\
	\label{eq: recursive SGD formula in the quadratic case}
	&=(1-\lr_n H)\underbrace{(\Weights_n - \minimum)}_{
		\xeq{\text{ind.}} (\weights_n - \minimum)
		\mathrlap{+ \sum_{k=0}^{n-1}\lr_k\left(\prod_{i=k+1}^{n-1}(1-\lr_iH)\right)\martIncr_{k+1}}
	} + \lr_n\martIncr_{n+1}\\
	\nonumber
	&= \underbrace{(1-\lr_n H)(\weights_n - \minimum)}_{=\weights_{n+1}-\minimum \implies (n\to n+1)}
	+ \sum_{k=0}^n\lr_k\left(\prod_{i=k+1}^n(1-\lr_iH)\right)\martIncr_{k+1}\\
	\label{eq: unrolled SGD weights (general quadratic loss case)}
	&=\left(\prod_{k=0}^n(1-\lr_kH)\right)(\weights_0-\minimum)
	+ \sum_{k=0}^n\lr_k\left(\prod_{i=k+1}^n(1-\lr_iH)\right)\martIncr_{k+1}.
\end{align}

\subsection{Variance Reduction}\label{subsec: variance reduction}

One particularly interesting case is \(H=\identity\). Why is that interesting?
Considering that
\begin{align*}
	\E[\tfrac12\|\weights-X\|^2]=\E[\loss(\weights,X)]=:\Loss(\weights)
\end{align*}
is minimized by \(\minimum=\E[X]\) and we have
\begin{align*}
	\E[\nabla\loss(\weights,X)]
	=\E[\weights-X]=\weights- \minimum=\nabla\Loss(\weights)
\end{align*}
we necessarily also have
\begin{align*}
	\Loss(\weights) = \tfrac12\|\weights-\minimum\|^2 + \text{const},
\end{align*}
which implies \(H=\nabla^2\Loss=\identity\). So this case is essentially
trying to find the expected value of some random variables as quickly as
possible.

\subsubsection{Constant Learning Rates}

For constant learning rates \(\lr=T/N<1\) we have
\begin{align*}
	\Weights_n - \minimum
	&= \underbrace{(1-\lr)^n(\weights_0 -\minimum)}_{=\weights_n-\minimum}
	+ \sum_{k=0}^{n-1}\lr(1-\lr)^{n-1-k}
	\smash{\overbrace{\martIncr_{k+1}}^{X_{k+1}-\minimum}}\\
	&= \left((1-h)^n\weights_0 + \sum_{k=0}^{n-1}\lr(1-\lr)^{n-1-k}X_{k+1}\right)-\minimum.
\end{align*}
This implies we do not weight our \(X_k\) equally, but rather use an
exponential decay giving the most recent data the most weight.

The first interesting thing to highlight is the fact that the convergence
rate of GD actually worsens with finer discretization \(N\)
\begin{align*}
	(1-h)^N = (1-T/N)^N \nearrow \exp(-T) \qquad (N\to\infty).
\end{align*}
But it does not worsen by much if the learning rate is small to begin with. A
learning rater of 1 on the other hand allows instant convergence while Gradient
Flow only converges for \(T\to\infty\). So even if we would get the additional
gradient evaluations for free, a finer discretization is worse. Now this is
similar to the analysis we did when selecting optimal learning rates for GD,
where the rates of the eigenspaces improved until we moved \(\lr\hesseEV\)
beyond 1. From then on they start to deteriorate again. And quickly, since we
do not only increase the absolute value of the factor, but also multiply fewer 
of them together while in the case below 1 these things counteract each other.
But this means in general that (at least for quadratic functions) gradient
descent converges faster than gradient flow!

Secondly, notice how we can reobtain the rates from Theorem~\ref{thm: distance
SGD vs GD} by throwing the exponential decay away
\begin{align*}
	\E\|\Weights_N-\weights_N\|^2
	&= \E\left\|\sum_{k=0}^{N-1}\lr(1-\lr)^{N-1-k}\martIncr_{k+1}\right\|^2\\
	&= \sum_{k=0}^{N-1}\lr^2\underbrace{(1-\lr)^{2(N-1-k)}}_{\le1}
	\underbrace{\E\|\martIncr_{k+1}\|^2}_{\le \stdBound^2}\\
	&\le \lr^2 N \stdBound^2 = \lr T\stdBound^2 = \frac{T^2}{N}\stdBound^2
\end{align*}
We can do the same thing with the general case (\ref{eq: unrolled SGD weights
(general quadratic loss case)}) if we take \(\lr<2/\ubound\) for \(|H|\le\ubound\).

With conditional independence \(\E[\langle \cdot, \martIncr_{n+1}\rangle\mid\filtration_n]=0\)
we can thus see how we need to increase \(T\) with \(N\) at a roughly
logarithmic rate
\begin{align*}
	\E\|\Weights_N - \minimum\|^2
	&= \E\|\Weights_N - \weights_N\|^2 +\E\|\weights_N - \minimum\|^2\\
	&\le \exp(-2T)\|\weights_0 - \minimum\|^2 + \frac{T^2}{N}\stdBound^2
\end{align*}
to minimize our \(L^2\) norm for a given \(N\). Therefore our convergence rate
would be roughly of order
\begin{align*}
	O(\log(N)/N).
\end{align*}
This is a bit worse than the rate \(O(1/N)\) we can achieve with the mean
\begin{align*}
	\E\|\overline{X}_N - \E[X]\|^2 = \frac{\E\|X-\E[X]\|^2}{N},
\end{align*}
which is the best linear unbiased estimator (BLUE) for the expected value.
Making only one step and increasing the batch size to our sample size realizes
the mean. So is batch learning better? Well if we use a particular learning rate,
we can actually realize the mean with SGD as well.

\subsubsection{Decreasing Learning Rates}

Using \(\lr_n=\frac1{n+1}\) we get by induction
\begin{align}
	\label{eq: getting rid of w_0 first step}
	\Weights_1 &= \weights_0 - \frac{1}{0+1}(\weights_0 - X_1) = X_1\\
	\label{eq: telescoping product}
	\Weights_{n+1}
	&= \Weights_n - \tfrac{1}{n+1}(\Weights_n - X_{n+1})
	\xeq{\text{ind.}}\smash{\underbrace{\left(1-\tfrac1{n+1}\right)}_{=\frac{n}{n+1}}}
	\frac1n\sum_{k=1}^n X_k + \tfrac{1}{n+1}X_{n+1}\\
	\nonumber
	&= \frac{1}{n+1}\sum_{k=1}^{n+1}X_k.
\end{align}

Okay this is nice and all, but we actually have to guess the correct learning
rate schedule to get this right. Batch learning on the other hand works just
fine without. Well this is half true since we need to guess the right learning
rate.

\subsection{Optimal Rates for General Quadratic Functions}

Since we know we can get optimal rates in the case \(\condition=1\), we might
ask ourselves whether this is possible in the general case as well. So let us
try. First we are going to use the fact that a basis change to a different
orthonormal basis is an isometry\footnote{
	Proof: \(\|x\|^2 = x^Tx = x^TVV^Tx = (V^T x)^T V^Tx = \|V^T x\|^2\)
}
and thus
\begin{align*}
	\E\|\Weights_{n+1} - \minimum\|^2
	&= \E\|V^T(\Weights_{n+1} - \minimum)\|^2
	= \sum_{j=1}^\dimension \E\langle\Weights_{n+1}-\minimum, v_j\rangle^2.
\end{align*}
So if \(V\) are the eigenvectors of our hesse matrix \(H\) we get
\begin{align*}
	\langle \Weights_{n+1}-\minimum, v_j \rangle
	&\xeq{(\ref{eq: recursive SGD formula in the quadratic case})} (1-\lr_n\lambda_j)\langle\Weights_n-\minimum, v_j\rangle
	+ \lr_n\langle\martIncr_{n+1},v_j\rangle
\end{align*}
Using conditional independence of \(\Weights_n\) and \(\martIncr_{n+1}\) again we get
\begin{align}
	\label{eq: expected square error in quadratic case}
	\E\|\Weights_{n+1} - \minimum\|^2
	&= \sum_{j=1}^\dimension (1-\lr_n \hesseEV_j)^2 \E\langle\Weights_{n+1}-\minimum\rangle^2
	+ \lr_n^2 \E\langle \martIncr_{n+1}, v_j\rangle^2.
\end{align}
Now optimizing over \(\lr_n\) by taking the derivative of the term above with
respect to \(\lr_n\) would result in
\begin{align*}
	\lr_n^* &= \frac{
		\sum_{j=1}^\dimension \hesseEV_j \E\langle \Weights_n-\minimum, v_j\rangle^2
	}{
		\sum_{j=1}^\dimension\hesseEV_j^2\E\langle\Weights_n-\minimum, v_j\rangle^2
		+ \E\langle\martIncr_{n+1}, v_j\rangle^2
	}\\
	&= \frac{
		\E\langle \Weights_n - \minimum, \nabla\Loss(\Weights_n)\rangle
	}{
		\E\|\nabla\Loss(\Weights_n)\|^2 + \E\|\martIncr_{n+1}\|^2
	}
	= \frac{
		\E\langle \Weights_n - \minimum, \nabla\Loss(\Weights_n)\rangle
	}{
		\E\|\nabla\loss(\Weights_n)\|^2
	}.
\end{align*}
But while we can estimate the divisor, we do not know the scalar product between
the true gradient and the direction to the minimum. If we knew that we could
probably get to the minimum much faster.

But we do not, which might be surprising since we got ``optimal'' rates when
we did the same for the non-stochastic gradient in Subsection~\ref{subsec:
necessary assumptions in the quadratic case}. But now, even if we assumed
\(\E\|\martIncr_{n+1}\|^2=0\), we get these weird optimal rates. What happened?
Well, in Subsection~\ref{subsec: necessary assumptions in the quadratic case}
we did not actually optimize over the euclidean norm. Instead we looked at the
reduction factor for every eigenspace and tried to minimize their maximum.
This does not take the actual size of the eigen-components
\(\E\langle\Weights_n-\minimum, v_j\rangle^2\) of our distance to the optimum
\(\minimum\) into account. And if one of these components is very small, then
we could sacrifice its reduction factor for a smaller factor in the other
eigenspaces. We did not do that in Subsection~\ref{subsec: necessary assumptions
in the quadratic case} because we used constant learning rates which would
sacrifice one eigenspace for another indefinitely. But no matter how big the
initial difference, if the rate of one eigenspace is smaller than another then
eventually the error in the eigenspace with the smaller rate will be
negligible compared to the one with the larger rate. This means that
asymptotically only the maximum of these factors matter.

Now if we assume rotationally invariant noise, i.e.
\begin{align*}
	\E\langle\martIncr_{n+1},v_j\rangle^2 = \E\langle\martIncr_{n+1}, v_i\rangle^2
	\qquad \forall i,j
\end{align*}
then due to
\begin{align*}
	(1-\lr_n \hesseEV_j)^2
	\le \max \{ (1-\lr_n\hesseEV_1)^2, (1-\lr_n\hesseEV_\dimension)^2 \}
\end{align*}
the error of the eigenspaces in-between will eventually be negligible. So we 
only really have a trade off between the smallest and largest eigenvalue again.
If we would use the learning rates to favour one over the other, then this would
make up for any differences that might have existed after a finite time and
from then on both errors would be similar in size which would mean we should
not treat them differently anymore. This motivates why it is reasonable to
neglect differences between the size of eigen-components at least for the
asymptotic rate.

Now we need to translate this intuition into actual mathematics. For that, we are
going to define a new norm
\begin{align}
	\|X\|_{V,\infty}
	:= \sqrt{\max_{i=1,\dots,\dimension} \E\langle X, v_i\rangle^2}.
\end{align}
Note that without the expectation this would just be the infinity norm with regard
to the basis \(v_1,\dots,v_\dimension\). Now positive definiteness and
scalability of the norm are trivial, and the triangle inequality follows from the
triangle inequality of the \(L^2\) norm \(\|X\|_{L^2} := \sqrt{\E\|X\|^2}\)
\begin{align*}
	\|X+Y\|_{V,\infty}
	= \max_{j=1,\dots,\dimension} \|\langle X, v_j\rangle + \langle Y, v_j\rangle\|_{L^2}
	\le \|X\|_{V,\infty} + \|Y\|_{V,\infty}.
\end{align*}
As we have used the \(L^2\) norm until now, let us consider its relation
to this new norm. Of course the norms are equivalent
\begin{align*}
	\|X\|_{V,\infty}^2
	= \max_{i=1,\dots,\dimension} \E\langle X, v_i\rangle^2
	\le \sum_{i=1}^\dimension \E\langle X, v_i\rangle^2
	= \|X\|_{L^2}
	\le \dimension \|X\|_{V,\infty}^2
\end{align*}
but \(\dimension\) is potentially quite large. But as we have argued in the
limiting case all the eigenspaces except the one with the largest and smallest
eigenvalue vanish, so then the factor comes closer to \(2\) independent of the
dimension \(\dimension\).

Armed with this norm, we can now tackle our problem again.

\begin{theorem}
	Let \(\Loss\) be a quadratic Loss function and assume
	\begin{align*}
		\E\|\martIncr\|_{V,\infty}^2
		\le \tilde{\stdBound}^2 = \stdBound^2 \hesseEV_1\hesseEV_\dimension,
	\end{align*}
	then we obtain optimal learning rates (w.r.t \(\|\cdot\|_{V,\infty}\)) of
	\begin{align*}
		\lr_n^*
		= \begin{cases}
			\frac{2}{\hesseEV_1+\hesseEV_\dimension}
			& \frac{\stdBound^2}{\|\Weights_n - \minimum\|_{V,\infty}^2}
			\le \frac{\condition-1}{2\condition} \\
			\frac1{\hesseEV_\dimension(\condition^{-1} + \frac{\stdBound^2}{\|\Weights_n-\minimum\|_{V,\infty}^2})}
			& \frac{\stdBound^2}{\|\Weights_n - \minimum\|_{V,\infty}^2}
			\ge \frac{\condition-1}{2\condition}
		\end{cases}
	\end{align*}
	We can interpret this as follows: If the noise is negligible compared to the
	distance to the minimum, we want to use the same learning rates as in the
	classical case -- simply ignoring the existence of noise. After this ``transient
	phase'' we are in the ``noise reduction'' phase and want to select a learning rate
	which decreases with the distance to the minimum. For the asymptotic rate of
	convergence only the second case matters but the transient phase might be much
	more important for practical applications. Only for \(\condition=1\) we are
	in this asymptotic phase immediately.
	
	Assuming that \(n_0\) is the time we enter the ``noise reduction'' phase,
	i.e. the first index where \(\frac{\stdBound^2}{\|\Weights_n -
	\minimum\|_{V,\infty}^2} \ge
	\frac{\condition-1}{2\condition}\),
	then our rates of convergence are
	\begin{align*}
		\|\Weights_n-\minimum\|_{V,\infty}^2
		\le \begin{cases}
			\left(1-\tfrac{2}{1+\condition}\right)^n\|\Weights_0-\minimum\|_{V,\infty}^2
			& n \le n_0\\
			\frac{\stdBound^2\condition(\condition+1)}{(n+1-n_0)(\condition-1)}
			& n \ge n_0
		\end{cases}
	\end{align*}
\end{theorem}
\begin{proof}
	Using the same
	transformations (in particular conditional independence) as in (\ref{eq:
	expected square error in quadratic case}), we get
	\begin{align}
		\nonumber
		\|\Weights_{n+1} -\minimum\|_{V,\infty}^2
		&= \max_{j=1,\dots,\dimension} \E\langle\Weights_{n+1} -\minimum, v_j\rangle^2\\
		\nonumber
		&= \max_{j=1,\dots,\dimension} (1-\lr_n\hesseEV_j)^2\E\langle\Weights_n -\minimum, v_j\rangle^2
		+ \lr_n^2 \E\langle \martIncr_{n+1}, v_j \rangle^2\\
		\nonumber
		&\le \max_{j=1,\dots,\dimension} (1-\lr_n\hesseEV_j)^2\|\Weights_n -\minimum\|_{V,\infty}^2
		+ \lr_n^2 \|\martIncr_{n+1}\|_{V,\infty}^2\\
		\label{eq: upper bound weird norm weight distance}
		&= \max_{j=1,\dimension} (1-\lr_n\hesseEV_j)^2\|\Weights_n -\minimum\|_{V,\infty}^2
		+ \lr_n^2 \underbrace{\|\martIncr_{n+1}\|_{V,\infty}^2}_{\le \tilde{\stdBound}^2}
	\end{align}
	if we assume bounded variance. Okay, so let us optimize over this bound. Since
	we are looking at a maximum over parabolas in \(\lr_n\), the first order
	condition would be enough as this is a convex function. But while the maximum is
	continuous we have a discontinuity in the derivative 
	\begin{align*}
		\frac{d}{d\lr_n}
		= \begin{cases}
		(2\lr_n \hesseEV_1^2 - 2\hesseEV_1)\|\Weights_n-\minimum\|_{V,\infty}^2
		+ 2 \lr_n \tilde{\stdBound}^2
		& \lr_n \le \frac{2}{\hesseEV_1+\hesseEV_\dimension}\\
		(2\lr_n \hesseEV_\dimension^2 - 2\hesseEV_\dimension)\|\Weights_n-\minimum\|_{V,\infty}^2
		+ 2 \lr_n \tilde{\stdBound}^2
		& \lr_n \ge \frac{2}{\hesseEV_1+\hesseEV_\dimension}
		\end{cases},
	\end{align*}
	which might prevent the monotonously increasing derivative from ever being equal
	to zero. In fact we need either
	\begin{align*}
		\frac{2}{\hesseEV_1+\hesseEV_\dimension} \xge{!} \lr_n
		&= \frac{
			\hesseEV_1 \|\Weights_n-\minimum\|_{V,\infty}^2
		}{
			\hesseEV_1^2 \|\Weights_n-\minimum\|_{V,\infty}^2 + \tilde{\stdBound}^2
		}\\
		&= \frac{1}{
			\hesseEV_1 + \frac{\tilde{\stdBound}^2}{
				\hesseEV_1\|\Weights_n-\minimum\|_{V,\infty}^2
			}
		}
	\end{align*}
	or equivalently
	\begin{align*}
		\frac{\tilde{\stdBound}^2}{
			\|\Weights_n-\minimum\|_{V,\infty}^2
		}
		\ge\frac{\hesseEV_1^2(\condition-1)}{2},
	\end{align*}
	covering the case \(\condition=1\) allowing us to assume \(\hesseEV_1 <
	\hesseEV_\dimension\) without loss of generality in the next case.
 	Or we want
	\begin{align*}
		\frac{2}{\hesseEV_1+\hesseEV_\dimension} \xle{!} \lr_n
		&= \frac{1}{
			\hesseEV_\dimension
			+ \frac{\tilde{\stdBound}^2}{
				\hesseEV_\dimension\|\Weights_n-\minimum\|_{V,\infty}^2
			}
		}
	\end{align*}
	which simplifies to
	\begin{align*}
		\frac{\hesseEV_1+\hesseEV_\dimension}{2}
		\xge{!} \hesseEV_\dimension
		+ \underbrace{\frac{\tilde{\stdBound}^2}{
			\hesseEV_\dimension\|\Weights_n-\minimum\|_{V,\infty}^2
		}}_{\ge 0}
		\ge \hesseEV_\dimension,
	\end{align*}
	which is a contradiction for \(\hesseEV_1 < \hesseEV_\dimension\).

	If the first condition is not satisfied either, then the optimal learning
	rate is at the discontinuity (the same as in the classical case). So in
	summary we have
	\begin{align*}
		\lr_n^* = \begin{cases}
			\frac{2}{\hesseEV_1+\hesseEV_\dimension}
			&\frac{\tilde{\stdBound}^2}{
				\|\Weights_n-\minimum\|_{V,\infty}^2
			}
			\le \frac{\hesseEV_1^2(\condition-1)}{2}\\
			\left(
				\hesseEV_1 + \frac{\tilde{\stdBound}^2}{
					\hesseEV_1\|\Weights_n-\minimum\|_{V,\infty}^2
				}
			\right)^{-1}
			&\frac{\tilde{\stdBound}^2}{
				\|\Weights_n-\minimum\|_{V,\infty}^2
			}
			\ge \frac{\hesseEV_1^2(\condition-1)}{2}
		\end{cases}.
	\end{align*}
	Plugging in the definition of \(\tilde{\stdBound}\) results in the version
	of our theorem.

	Before we plug our learning rate into our upper bound (\ref{eq: upper bound
	weird norm weight distance}), let us do some preparation 
	\begin{align}
		\nonumber
		&\|\Weights_{n+1} -\minimum\|_{V,\infty}^2\\
		\nonumber
		&\le \max_{j=1,\dimension} (1-\lr_n\hesseEV_j)^2\|\Weights_n -\minimum\|_{V,\infty}^2
		+ \lr_n^2 \tilde{\stdBound}^2\\
		\label{eq: upper bound useful for rate of convergence}
		&= \max_{j=1,\dimension}\left(1- 2\hesseEV_j\lr_n + \lr_n^2\hesseEV_j^2
			+\lr_n^2\tfrac{\tilde{\stdBound}^2}{\|\Weights_n-\minimum\|_{V,\infty}^2}
		\right)\|\Weights_n-\minimum\|_{V,\infty}^2\\
		\nonumber
		&= \max_{j=1,\dimension}\Bigg(1- \lr_n \hesseEV_j\Big[2 - \lr_n \left(
			\hesseEV_j + \tfrac{\tilde{\stdBound}^2}{\hesseEV_j\|\Weights_n-\minimum\|_{V,\infty}^2}
		\right)\Big] \Bigg)\|\Weights_n-\minimum\|_{V,\infty}^2.
	\end{align}
	Since the maximum is attained by the eigenspace with the smallest eigenvalue \(\hesseEV_1\)
	in the asymptotic case, our bound reduces to
	\begin{align*}
		\|\Weights_{n+1} -\minimum\|_{V,\infty}^2
		&\le (1- \lr_n^* \hesseEV_1)\|\Weights_n-\minimum\|_{V,\infty}^2\\
		&= \left(1- \tfrac{1}{1+ \tfrac{\tilde{\stdBound}^2}{\hesseEV_1^2\|\Weights_n-\minimum\|^2}}\right)
		\|\Weights_n-\minimum\|_{V,\infty}^2\\
		&= \left(1- \tfrac{
			\hesseEV_1^2 \|\Weights_n-\minimum\|_{V,\infty}^2
		}{
			\tilde{\stdBound}^2
			\left(\tfrac{\hesseEV_1^2\|\Weights_n-\minimum\|^2}{\tilde{\stdBound}^2}+1\right)
		}\right)
		\|\Weights_n-\minimum\|_{V,\infty}^2\\
		&\le \Bigg(
		1- \underbrace{\tfrac{
			\hesseEV_1^2
		}{
			\tilde{\stdBound}^2
			\left(\tfrac{2}{\condition -1}+1\right)
		}}_{
			=\tfrac{\hesseEV_1^2(\condition-1)}{\tilde{\stdBound}^2(\condition+1)}
			\mathrlap{=\tfrac{\condition-1}{\stdBound^2\condition(\condition+1)}}
		}
		\|\Weights_n-\minimum\|_{V,\infty}^2
		\Bigg)
		\|\Weights_n-\minimum\|_{V,\infty}^2,
	\end{align*}
	where we have used our lower bound on the variance-distance fraction
	\begin{align*}
		\frac{\tilde{\stdBound}^2}{
			\hesseEV_1^2\|\Weights_n-\minimum\|_{V,\infty}^2
		}
		\ge \frac{\condition-1}{2}
	\end{align*}
	characterizing the asymptotic phase. Using Lemma~\ref{lem-appendix: diminishing contraction}
	this implies our claim.
	% \begin{align*}
	% 	\|\Weights_n-\minimum\|_{V,\infty}^2
	% 	\le \frac{1}{
	% 		n\tfrac{\hesseEV_1^2(\condition-1)}{\stdBound^2(\condition+1)}
	% 		+ \tfrac{1}{\|\Weights_0-\minimum\|_{V,\infty}}
	% 	},
	% \end{align*}
	% assuming we have started in the asymptotic phase with \(\Weights_0\).
	Now in the transient phase we can apply
	\begin{align*}
		\frac{\tilde{\stdBound}^2}{
			\|\Weights_n-\minimum\|_{V,\infty}^2
		}
		\le \frac{\hesseEV_1^2(\condition-1)}{2}
	\end{align*}
	directly to (\ref{eq: upper bound useful for rate of convergence}) to get
	\begin{align*}
		\|\Weights_{n+1}-\minimum\|_{V,\infty}^2
		&= \max_{j=1,\dimension}\left((1- \hesseEV_j\lr_n)^2
			+\lr_n^2\tfrac{\tilde{\stdBound}^2}{\|\Weights_n-\minimum\|_{V,\infty}^2}
		\right)\|\Weights_n-\minimum\|_{V,\infty}^2\\
		&\le \max_{j=1,\dimension}\left((1- \hesseEV_j\lr_n)^2
			+\lr_n^2\tfrac{\hesseEV_1^2(\condition-1)}{2}
		\right)\|\Weights_n-\minimum\|_{V,\infty}^2.
	\end{align*}
	And for the learning rate \(\lr_n^*=\tfrac{2}{\hesseEV_1+\hesseEV_\dimension}=\tfrac{2}{\hesseEV_1(1+\condition)}\)
	we have
	\begin{align*}
		(1-\hesseEV_\dimension\lr_n^*)^2
		= (1-\hesseEV_1\lr_n^*)^2
		= \left(1-\tfrac{2}{1+\condition}\right)^2
	\end{align*}
	and therefore
	\begin{align*}
		\|\Weights_{n+1}-\minimum\|_{V,\infty}^2
		&\le \left(\left(1-\tfrac{2}{1+\condition}\right)^2
			+\left(\tfrac{2}{\hesseEV_1(1+\condition)}\right)^2\tfrac{\hesseEV_1^2(\condition-1)}{2}
		\right)\|\Weights_n-\minimum\|_{V,\infty}^2\\
		&=\left(
			\left(\tfrac{\condition-1}{\condition+1}\right)^2 + 2 \tfrac{\condition-1}{(\condition+1)^2}
		\right)\|\Weights_n-\minimum\|_{V,\infty}^2\\
		&= \tfrac{\condition-1}{(\condition+1)^2}(\condition-1 + 2)\|\Weights_n-\minimum\|_{V,\infty}^2\\
		&= \left(1-\tfrac{2}{1+\condition}\right)\|\Weights_n - \minimum\|_{V,\infty}^2.
		\qedhere
	\end{align*}
\end{proof}
\begin{remark}
	The condition
	\begin{align*}
		\E\|\martIncr\|_{V,\infty}^2
		\le \tilde{\stdBound}^2 = \stdBound^2 \hesseEV_1\hesseEV_\dimension,
	\end{align*}
	might seem strange, but this definition allows \(\stdBound\) to be scale
	invariant. I.e. when we scale \(\loss(\weights, Z)\) we scale \(\hesseEV_j\)
	linearly as well as \(\E\|\martIncr_n\|^2\) quadratically. So the
	bound \(\stdBound\) is scale invariant while \(\tilde{\stdBound}\) is not.
	And with scale invariance we get convergence rates dependent only on the
	condition number.
\end{remark}

Note that increasing the batch size during the transient phase would provide no
value as the increased batch size would only result in smaller \(\stdBound\)
which has no effect on the convergence rate. And while the second of the two
phases of SGD \parencite[first observed
by][]{darkenFasterStochasticGradient1991} determines the asymptotic properties
of SGD  ``in practice it seems that for deeper networks in particular, the first
phase dominates overall computation time as long as the second phase is cut off
before the remaining potential gains become either insignificant or entirely
dominated by overfitting (or both)''  
\parencite{sutskeverImportanceInitializationMomentum2013}.

In the asymptotic phase on the other hand, doubling the number of steps or the
batch size has the same
effect for our bounds. Now as we argued leading up to this theorem, the bounds
we used are quite tight. But guessing the correct learning rates is difficult.
Before we try to address this guessing problem, let us see if we can even
generalize this to non-quadratic functions.


\section{Strongly Convex Functions}

\begin{lemma}\label{lem: SGD bound with noise}
	Let \(\Loss\in\strongConvex{\lbound}{\ubound}\), then using SGD with learning
	rates \(\lr_n\) we have
	\begin{align*}
		\E\|\Weights_{n+1} - \minimum\|^2
		\le \left(1-2\lr_n\tfrac{\ubound\lbound}{\ubound+\lbound}\right)
		\E\|\Weights_n - \minimum\|^2 + \lr_n^2 \underbrace{\E\|\martIncr_{n+1}\|^2}_{\le\stdBound^2\ubound\lbound}.
	\end{align*}
\end{lemma}
\begin{proof}
	While we are not going to tether ourselves to Gradient Descent \(\weights_n\) in
	this section, we are still going to split off the noise
	\begin{align*}
		\|\Weights_{n+1} - \minimum\|^2
		&= \|\Weights_n -\minimum - \lr_n\nabla\Loss(\weights_n) +\lr_n \martIncr_{n+1}\|^2\\
		&= \begin{aligned}[t]
			&\|\Weights_n - \minimum - \lr_n\nabla\Loss(\Weights_n)\|^2\\
			&+ 2\langle \Weights_n - \minimum - \lr_n\nabla\Loss(\Weights_n), \lr_n \martIncr_{n+1}\rangle\\
			&+ \lr_n^2 \|\martIncr_{n+1}\|^2
		\end{aligned}
	\end{align*}
	The scalar product will disappear once we apply expectation due to conditional
	independence again. For the first part we are going to do the same as in the
	classical case in Theorem~\ref{thm: gd strong convexity convergence rate}:
	\begin{align}
		\nonumber
		&\|\Weights_n - \minimum - \lr_n\nabla\Loss(\Weights_n)\|^2\\
		\label{eq: getting rid of the scalar product (SGD)}
		&= \|\Weights_n - \minimum\|^2
		- 2\lr_n\underbrace{\langle\nabla\Loss(\Weights_n), \Weights_n-\minimum\rangle}_{
			\xge{\text{Lem.~\ref{lem: bermanDiv lower bound (strongly convex)}}}
			\tfrac{\ubound\lbound}{\ubound+\lbound}\|\Weights_n - \minimum\|^2
			\mathrlap{+ \tfrac{1}{\ubound+\lbound}\|\nabla\Loss(\Weights_n)\|^2}
		}
		+ \lr_n^2 \|\nabla\Loss(\Weights_n)\|^2\\
		\nonumber
		&\le \left(1-2\lr_n\tfrac{\ubound\lbound}{\ubound+\lbound}\right)
		\|\Weights_n - \minimum\|^2
		+ \lr_n(\lr_n - \tfrac{2}{\ubound+\lbound})
		\|\nabla\Loss(\Weights_n)\|^2
	\end{align}
	For \(\lr_n\le\tfrac{2}{\ubound+\lbound}\) we can drop the gradient again to
	obtain our claim.
\end{proof}

Now we can use this recursion to obtain a similar theorem as in the quadratic
case.

\begin{theorem}[Optimal Rates]\label{thm: optimal rates SGD}
	For a \(\Loss\in\strongConvex{\lbound}{\ubound}\) with
	\begin{align*}
		\E\|\martIncr_n\|^2 \le \stdBound^2\ubound\lbound
	\end{align*}
	optimal learning rates according to the bounds of Lemma~\ref{lem: SGD bound with noise} are
	\begin{align*}
		\lr_n
		= \begin{cases}
			\frac{2}{\ubound+\lbound}
			& \stdBound^2 \le \frac{\E\|\Weights_n - \minimum\|}{2}
			\qquad \text{``transient phase''}\\
			\frac{\E\|\Weights_n -\minimum\|^2}{\stdBound^2(\ubound+\lbound)}
			& \stdBound^2 \ge \frac{\E\|\Weights_n - \minimum\|}{2}
			\qquad \text{ ``asymptotic phase''}
		\end{cases}.
	\end{align*}
	If \(n_0\) is the start of the asymptotic phase, then we can bound the
	convergence rates of SGD by
	\begin{align*}
		\E\|\Weights_n - \minimum\|^2
		\le \begin{cases}
			\left(1-\frac{2}{(1+\condition^{-1})(1+\condition)}\right)^n\E\|\weights_0 -\minimum\|^2
			& n < n_0\\
			\frac{\stdBound^2(1+\condition^{-1})(1+\condition)}{n+1-n_0} & n \ge n_0.
		\end{cases}
	\end{align*}
\end{theorem}
\begin{remark}
	In the asymptotic phase the learning rate is thus bounded by
	\begin{align*}
		\lr_n \le \frac{(1+\condition^{-1})(1+\condition)}{(n+1-n_0)(\ubound + \lbound)}
	\end{align*}
	which is independent of the variance \(\stdBound^2\).
\end{remark}
\begin{remark}
	Note that using Theorem~\ref{thm: convergence chain} we get
	\begin{align*}
		\E[\Loss(\Weights_n)]- \Loss(\minimum)
		&\le \tfrac{\ubound}{2}\E\|\Weights_n-\minimum\|^2
	\end{align*}
	and thus also convergence of the loss in expectation.
\end{remark}

\begin{proof}
	In the classical case we just wanted to select \(\lr_n\) as large as possible,
	i.e. \(\lr_n=\tfrac{2}{\ubound+\lbound}\). But in the stochastic setting there
	are adverse effects of large learning rates due to noise. Now as our upper bound
	is a convex parabola in \(\lr_n\) we can just use the first order condition
	\begin{align*}
		\frac{d}{d\lr_n}
		= -2\tfrac{\ubound\lbound}{\ubound+\lbound}
		\E\|\Weights_n - \minimum\|^2 + 2\lr_n \stdBound^2\ubound\lbound
		\xeq{!} 0
	\end{align*}
	to find the best upper bound
	\begin{align}\label{eq: optimal learning rate SGD}
		\lr_n
		= \min\left\{
			\frac{\E\|\Weights_n - \minimum\|^2}{\stdBound^2(\ubound+\lbound)},
			\frac{2}{\ubound+\lbound}
		\right\}.
	\end{align}
	So we still want to ``max out our learing rate'' and use
	\(\lr_n=\tfrac{2}{\ubound+\lbound}\) if
	\begin{align*}
		2\stdBound^2 \le \E\|\Weights_n - \minimum\|^2.
	\end{align*}
	We find again: the stochastic variance only becomes a problem once we are
	close to the minimum. Before in the transient phase we basically want to
	treat our problem like the classical one. Using Lemma~\ref{lem: SGD bound
	with noise} here, we get a linear convergence rate
	\begin{align*}
		\E\|\Weights_{n+1} - \minimum\|^2
		&\le \left(1-2\tfrac{2}{\ubound+\lbound}\tfrac{\ubound\lbound}{\ubound+\lbound}\right)
		\E\|\Weights_n - \minimum\|^2 + \left(\tfrac{2}{\ubound+\lbound}\right)^2
		\underbrace{\stdBound^2\ubound\lbound}_{
			\le\tfrac{\ubound\lbound}{2} \E\|\Weights_n - \minimum\|^2
		}\\
		&\le \underbrace{
			\left(1-\tfrac{2\ubound\lbound}{(\ubound+\lbound)^2}\right)
		}_{
			=1-\frac{2}{(1+\condition^{-1})(1+\condition)}
		}
		\E\|\Weights_n - \minimum\|^2.
	\end{align*}
	Plugging our asymptotic learning rate into the inequality from Lemma~\ref{lem: SGD bound with noise}
	on the other hand we get
	\begin{align*}
		&\E\|\Weights_{n+1} - \minimum\|^2\\
		&\le \left(1-2
			\tfrac{\E\|\Weights_n - \minimum\|^2}{\stdBound^2(\ubound+\lbound)}
			\tfrac{\ubound\lbound}{\ubound+\lbound}
		\right)
		\E\|\Weights_n - \minimum\|^2
		+ \left(
			\tfrac{\E\|\Weights_n - \minimum\|^2}{\stdBound^2(\ubound+\lbound)}
		\right)^2
		\stdBound^2\\
		&\le \left(1-
			\tfrac{1}{\stdBound^2(1+\condition^{-1})(1+\condition)}
			\E\|\Weights_n - \minimum\|^2
		\right)
		\E\|\Weights_n - \minimum\|^2.
	\end{align*}
	Using Diminishing Contraction Lemma~\ref{lem-appendix: diminishing
	contraction} the recursion above finally implies
	\begin{align*}
		\E\|\Weights_n - \minimum\|^2
		&\le \frac{\stdBound^2(1+\condition^{-1})(1+\condition)}{n+1-n_0}
		\qedhere
	\end{align*}
\end{proof}

Another way to interpret Lemma~\ref{lem: SGD bound with noise} is to show
convergence to an area for constant learning rates
\begin{theorem}[{\cite[Theorem 4.6]{bottouOptimizationMethodsLargeScale2018}}]
	\label{thm: SGD converges to area}
	Let \(\Loss\in\strongConvex{\lbound}{\ubound}\) with
	\begin{align*}
		\E\|\martIncr_n\|^2 \le \stdBound \ubound\lbound,
	\end{align*}
	then using SGD with learning rate \(\lr\le\frac{2}{\ubound+\lbound}\) results in
	\begin{align*}
		\limsup_n\E\|\Weights_n-\minimum\|^2
		\le \stdBound^2 \lr\frac{(\ubound+\lbound)}{2}
	\end{align*}
	more specifically
	\begin{align*}
		&\E\|\Weights_n-\minimum\|^2\\
		&\le \stdBound^2 \lr\frac{(\ubound+\lbound)}{2}
		+ \left(1-2\lr\frac{\ubound\lbound}{\ubound+\lbound}\right)^n
		\left[
			\|\weights_0-\minimum\|^2-\stdBound^2 \lr\frac{(\ubound+\lbound)}{2}
		\right]
	\end{align*}
\end{theorem}
\begin{proof}
	Subtracting \(\stdBound^2 \lr\frac{(\ubound+\lbound)}{2}\) from both sides
	of Lemma~\ref{lem: SGD bound with noise} we get
	\begin{align*}
		&\E\|\Weights_{n+1}-\minimum\|^2 - \stdBound^2 \lr\frac{(\ubound+\lbound)}{2}\\
		&\le \left(1-2\lr\frac{\ubound\lbound}{\ubound+\lbound}\right)
		\E\|\Weights_n-\minimum\|^2
		-\stdBound^2 \lr\frac{(\ubound+\lbound)}{2} + \lr^2\stdBound^2\ubound\lbound\\
		&\le \left(1-2\lr\frac{\ubound\lbound}{\ubound+\lbound}\right)
		\left[
			\E\|\Weights_n-\minimum\|^2-\stdBound^2 \lr\frac{(\ubound+\lbound)}{2}
		\right]
		\qedhere
	\end{align*}
\end{proof}


\section{Simplified Learning Rate Schedules}

Selecting a learning rate
\begin{align*}
	\lr_n = \frac{\E\|\Weights_n - \minimum\|^2}{\stdBound^2(\ubound+\lbound)}
\end{align*}
is difficult without very good knowledge of the problem which would likely mean
we do not need to use SGD for optimization. But since we know that the learning
rate should behave like \(O(1/n)\) we could try to guess it and see what happens.

\subsection{Diminishing Learning Rate Schedules}

\begin{theorem}\label{thm: convergence rates for diminishing lr schedules}
	For a quadratic and convex Loss \(\Loss\) selecting the learning rate
	schedule
	\begin{align*}
		\lr_n = \frac{a}{n+b}	
	\end{align*}
	for some \(a>0\), \(b\ge a\hesseEV_\dimension\) means SGD behaves like
	\begin{align*}
		\E\|\Weights_n - \minimum\|^2 
		&\in O(n^{-2\hesseEV_1a})\|\weights_0-\minimum\|^2
		+ \tilde{\stdBound}^2 O(n^{-1} + n^{-2\hesseEV_1 a})
	\end{align*}
	In particular \(b\) does not matter, and selecting \(a\ge\tfrac{1}{2\hesseEV_1}\),
	ensures a convergence rate of \(O(1/n)\). A possible selection to
	achieve \(O(1/n)\) convergence is thus for example (\(a=1/\hesseEV_1\), \(b=a\hesseEV_\dimension\))
	\begin{align*}
		\lr_n = \frac{1}{\hesseEV_1(n+\condition)}.
	\end{align*}
	For \(\hesseEV_1=\hesseEV_\dimension=1\), this results in the mean we found
	optimal in our expected value search in Subsection~\ref{subsec: variance
	reduction}
\end{theorem}
\begin{proof}
	By diagonalizing \(H\) and using the fact that
	\begin{align*}
		&\prod_{k=0}^{n-1} (1-\lr_k V\diag[\hesseEV_1, \dots, \hesseEV_\dimension]V^T)\\
		&= V \diag\left[
			\prod_{k=0}^{n-1}(1-\lr_k\hesseEV_1),
			\dots, \prod_{k=0}^{n-1}(1-\lr_k\hesseEV_\dimension)
		\right]V^T
	\end{align*}
	we can consider all the eigenspaces of (\ref{eq: unrolled SGD weights (general
	quadratic loss case)}) separately again
	\begin{align*}
		\langle \Weights_{n+1}-\minimum, v_j \rangle
		= \begin{aligned}[t]
			&\left(\prod_{k=0}^n(1-\lr_k\hesseEV_j)\right) \langle \weights_0-\minimum, v_j\rangle\\
			&+ \sum_{k=0}^n \lr_k \left(\prod_{i=k+1}^n(1-\lr_i\hesseEV_j)\right)\langle\martIncr_{k+1},v_j\rangle.
		\end{aligned}
	\end{align*}
	With the usual conditional independence argument we get
	\begin{align*}
		\E\|\Weights_{n+1}-\minimum\|^2
		&= \E[(V^T [\Weights_{n+1}-\minimum])^T(V^T[\Weights_{n+1}-\minimum])]\\
		&= \sum_{j=1}^\dimension \E\langle\Weights_{n+1}-\minimum, v_j \rangle^2\\
		&= \sum_{j=1}^\dimension
		\begin{aligned}[t]
			&\Bigg[\left(\prod_{k=0}^n(1-\lr_k\hesseEV_j)\right)^2 \langle \weights_0-\minimum, v_j\rangle^2\\
			&+ \sum_{k=0}^n \lr_k^2 \left(\prod_{i=k+1}^n(1-\lr_i\hesseEV_j)\right)^2\E[\langle\martIncr_{k+1},v_j\rangle^2]\Bigg].
		\end{aligned}
	\end{align*}
	Note that since the eigenvalues are all different, we can not select \(\lr_n=\frac{1}{\hesseEV(n+1)}\)
	to achieve
	\begin{align*}
		1-\lr_n\hesseEV = \frac{n}{n+1}
	\end{align*}
	which would be the product equivalent of a telescoping sum as we have used in
	(\ref{eq: telescoping product}). To make the product go away we could use
	\(1+x\le \exp(x)\) inside the square to convert it into a sum. But since \(x\to
	x^2\) is only monotonous for positive \(x\) we need
	\begin{align*}
		1-\lr_k \hesseEV_j \ge 0 \qquad \forall k\ge 0, \forall j \in\{1,\dots,\dimension\}
	\end{align*}
	Which is why we need the requirement \(b\ge a\hesseEV_\dimension\).

	Okay so now we enact this idea and use the exponential function bound
	\begin{align*}
		\left(\prod_{i=k+1}^{n-1}(1-\lr_i\hesseEV)\right)
		&\le\exp\left(-2\sum_{i=k+1}^{n-1}\lr_i\hesseEV_j\right)\\
		&\le \exp\left(-2\hesseEV_j a[\log(n+b)-\log(k+1+b)]\right)\\
		&= \left(\frac{n+b}{k+b+1}\right)^{-2\hesseEV_j a}.
	\end{align*}
	Here we have used
	\begin{align*}
		\sum_{i=k}^{n-1} \lr_i
		= \int_{k}^{n} \frac{a}{\lfloor x\rfloor+b} dx
		\ge \int_{k}^{n} \frac{a}{x+b} dx
		= a(\log(n+b) - \log(k+b)).
	\end{align*}
	Using this detour over the exponential function and back we can upper bound
	our deterministic part by
	\begin{align}
		\nonumber
		\sum_{j=1}^\dimension\left(\prod_{k=0}^n(1-\lr_k\hesseEV_j)\right)^2
		\langle \weights_0-\minimum, v_j\rangle^2
		&\le \sum_{j=1}^\dimension \left(\frac{n+b}{b+1}\right)^{-2\hesseEV_j a}
		\langle\weights_0-\minimum, v_j\rangle^2\\
		\label{eq: upper bound on convergence rate using smallest eigenvalue 1}
		&\le \left(\frac{n+b}{b+1}\right)^{-2\hesseEV_1 a}\|\weights_0-\minimum\|^2\\
		\nonumber
		&\in O(n^{-2\hesseEV_1a})\|\weights_0-\minimum\|^2.
	\end{align}
	And our noise by
	\begin{align}
		\nonumber
		&\sum_{j=1}^\dimension\sum_{k=0}^n \lr_k^2
		\left(\prod_{i=k+1}^n(1-\lr_i\hesseEV_j)\right)^2
		\E[\langle\martIncr_{k+1},v_j\rangle^2]\\
		\nonumber
		&\le \sum_{j=1}^d
			\sum_{k=0}^n \lr_k^2 \left(\tfrac{n+b}{k+b+1}\right)^{-2\hesseEV_j a}
			\E[\langle\martIncr_{k+1},v_j\rangle^2]
		\\
		\label{eq: upper bound on convergence rate using smallest eigenvalue 2}
		&\le \sum_{k=0}^{n-1} \lr_k^2 \left(\tfrac{n+b}{k+b+1}\right)^{-2\hesseEV_1 a}
		\underbrace{\E[\|\martIncr_{k+1}\|^2]}_{\le \tilde{\stdBound}^2}
		\\
		\nonumber
		&\le
		\tilde{\stdBound}^2a^2 (n+b)^{-2\hesseEV_1 a}
		\underbrace{\sum_{k=0}^{n-1} \frac{(k+b+1)^{2\hesseEV_1 a}}{(k+b)^2}}_{
			\sim \int_b^{n+b} \frac{(x+1)^{2\hesseEV_1 a}}{x^2}dx} \\
		\nonumber
		&\in \tilde{\stdBound}^2 O(n^{-2\hesseEV_1 a})  O(n^{-1+2\hesseEV_1a} + 1)
		= \tilde{\stdBound}^2 O(n^{-1} + n^{-2\hesseEV_1 a}).
	\end{align}
	Putting things together we obtain our claim.
\end{proof}
\begin{remark}
	In equations (\ref{eq: upper bound on convergence rate using smallest eigenvalue 1})
	and (\ref{eq: upper bound on convergence rate using smallest eigenvalue 1})
	we bounded the convergence rate of all eigenspaces with the convergence rate
	of the eigenspace with the smallest eigenvalue \(\hesseEV_1\). If we select
	our learning rate schedule wrong, the convergence rate in this eigenspace
	is smaller. But the other eigenspaces might still converge at rate \(O(1/n)\).
\end{remark}
\begin{remark}
	The integral lower bounds on the sum of learning rates could also be upper bounds
	up to some constant shift, so concerning the asymptotic rate this bound is
	tight. The only other estimation method we used (except replacing all
	eigenvalues with the worst) is bounding \(1+x\) by \(\exp(x)\) which is
	also tight for small \(x\). Since the learning rates are decreasing, this
	bound becomes better and better and should thus asymptotically be tight.
\end{remark}

The next Theorem provides a more general statement with uglier requirements on
the learning rates but with more explicit bounds.

\begin{theorem}[{\cite[cf. Theorem 4.7]{bottouOptimizationMethodsLargeScale2018}}]
	For \(\Loss\in\strongConvex{\lbound}{\ubound}\) with
	\begin{align*}
		\E\|\martIncr_n\|^2 \le \stdBound^2 \ubound\lbound,
	\end{align*}
	SGD with step sizes
	\begin{align*}
		\lr_n = \frac{\tilde{a}}{n+b},
		\qquad \tilde{a} =  a\frac{\ubound+\lbound}{2\ubound\lbound},
		\quad a > 1,
		\quad b \ge a\frac{(1+\condition)(1+\condition^{-1})}{4}
	\end{align*}
	converges at rate
	\begin{align*}
		\E\|\Weights_n - \minimum\|^2
		\le \frac{\nu}{n+b}
	\end{align*}
	where
	\begin{align*}
		\nu = \max\Bigg\{
			\frac{\stdBound^2a^2(\condition+1)(1+\condition^{-1})}{a-1},
			\underbrace{
				b\|\weights_0 - \minimum\|^2
			}_{\text{induction start}}
		\Bigg\}
	\end{align*}
\end{theorem}
\begin{proof}
	We will prove the statement by induction. The induction start is covered by
	definition of \(\nu\). For the induction step we ues Lemma~\ref{lem: SGD
	bound with noise} for which we need \(\lr \ge \tfrac{2}{\ubound+\lbound}\)
	(which is ensured by our requirement on \(b\)) and define \(\tilde{n}=n+b\)
	for
	\begin{align*}
		\E\|\Weights_{n+1}-\minimum\|^2
		&\le \left(1-2\lr_n \frac{\ubound\lbound}{\ubound+\lbound}\right)\frac{\nu}{\tilde{n}}
		+ \lr_n^2 \stdBound^2 \ubound\lbound\\
		&= \frac{\tilde{n} - a}{\tilde{n}}\frac{\nu}{\tilde{n}}
		+ \frac{\stdBound^2}{\tilde{n}^2}\frac{a^2(\ubound+\lbound)^2}{\ubound\lbound}\\
		&= \frac{\tilde{n} - 1}{\tilde{n}^2}\nu
		- \underbrace{\frac{(a -1)\nu + \stdBound^2 a^2(\condition+1)(1+\condition^{-1})}{\tilde{n}^2}}_{
			\ge 0\qquad \text{definition of \(\nu\) and } a>1
		}\\
		&\le \frac{\tilde{n}-1}{(\tilde{n}+1)(\tilde{n}-1)}\nu = \frac{\nu}{n+1+b}
		\qedhere
	\end{align*}
\end{proof}

\subsection{Piecewise Constant Learning Rates}

As we have seen in Theorem~\ref{thm: optimal rates SGD}, we can achieve
much better results if we keep the learning rate high during the transient
phase. If we use the results above we miss out on the exponential reduction that
constant learning rates provide. So the first idea is to use constant learning
rates until we ``stop making progress''. I.e. we have converged to an area like
in Theorem~\ref{thm: SGD converges to area}. But convergence in this theorem is
exponential meaning we do not actually converge in finite time. And due to
stochasticity it is difficult to tell when we actually stop making progress.
Especially because a plateau in the loss could also be due to a saddle point
region cf. Figure~\ref{fig: visualize saddle point gd}.

Detecting convergence is thus its own research field \parencite[for a recent
approach including an overview of previous work see
e.g.][]{pesmeConvergenceDiagnosticBasedStep2020}.\fxnote{expand on this?}

Another problem with diminishing learning rates is: if we reduce learning rates
too fast (e.g. \(1/(n+1)\) when \(\hesseEV_1=1/4\) requires something like \(2/n+b\)),
then we have seen in Theorem~\ref{thm: convergence rates for diminishing lr schedules}
that our convergence rate is reduced below \(O(1/n)\). Since \(\hesseEV_1\) can
be arbitrarily slow our convergence rate \(O(n^{-2\hesseEV_1 a})\) could be
arbitrarily slow in fact.

So we want to avoid reducing the learning rate too fast. One possibility is
choosing a learning rate schedule with \(O(n^{-\alpha}\) where \(1/2<\alpha<1\),
which sacrifices convergence speed on purpose to avoid it becoming arbitrarily
bad. Or another idea is to recycle the ``convergence detection'' algorithm we
need anyway to transition from the transient phase into the asymptotic phase.
In other words: Use piecewise constant learning rates.


\section{General Convex Case -- Averaging}\label{subsec: SGD with Averaging}

Since the noise prevents us from using Lemma~\ref{lem: bermanDiv lower bound} in
place of Lemma~\ref{lem: bermanDiv lower bound (strongly convex)} to ensure a
decreasing distance from the optimum, we can not use the same approach we used
in the classical convex loss function setting (Section~\ref{sec: convex convergence theorems}).

But since we know that the best asymptotic rates we can get will be
\(O(1/\sqrt{n})\) due to the averaging example in Subsection~\ref{subsec:
variance reduction} we are going to use the technique developed to show
convergence for the subgradient method (cf. Subsection~\ref{subsec: subgradient
method}).
In particular we are going to assume \(\Loss \in
\lipGradientSet[0,0]{\lipConst}\). And instead of the Lemmas mentioned above we
are going to simply use the (subgradient)
definition of convexity
\begin{align*}
	\Loss(\minimum)
	\ge \Loss(\Weights_n) + 
	\langle\nabla\Loss(\Weights_n), -(\Weights_n - \minimum)\rangle
\end{align*}
in (\ref{eq: getting rid of the scalar product (SGD)})
\begin{align*}
	&\|\Weights_n - \minimum - \lr_n\nabla\Loss(\Weights_n)\|^2\\
	&\le \|\Weights_n - \minimum\|^2
	- 2\lr_n\underbrace{\langle\nabla\Loss(\Weights_n), \Weights_n-\minimum\rangle}_{
		\ge \Loss(\Weights_n) - \Loss(\minimum)
	}
	+ \lr_n^2 \|\nabla\Loss(\Weights_n)\|^2
\end{align*}
to obtain
\begin{align*}
	\E\|\Weights_{n+1}-\minimum\|^2
	&\le
	\begin{aligned}[t]
		&\E\|\Weights_n -\minimum\|^2
		- 2\lr_n \E[\Loss(\Weights_n) - \Loss(\minimum)]\\
		&+ \lr_n^2 [
			\underbrace{\E\|\nabla\Loss(\Weights_n)\|^2}_{\le \lipConst^2}
			+ \underbrace{\E\|\martIncr_{n+1}\|^2}_{\le \stdBound^2}
		]
	\end{aligned}
\end{align*}
as \(\Loss\in\lipGradientSet[0,0]{\lipConst}\) implies Lipschitz continuity and thus bounded
gradients. Note that splitting \(\nabla\loss\) into \(\nabla\Loss\) and \(\martIncr\)
was of no use here as we end up summing them in the end. \textcite{nemirovskiRobustStochasticApproximation2009}
uses bounded \(\E\|\nabla\loss\|^2\) from the get go to get the same result.
This results in
\begin{align*}
	&2\sum_{n=0}^{N-1} \lr_n\E[\Loss(\Weights_n)-\Loss(\minimum)]\\
	&\le \sum_{n=0}^{N-1} \E\|\Weights_n -\minimum\|^2 - \E\|\Weights_{n+1}-\minimum\|^2
	+ \lr_n^2 [\lipConst^2 + \stdBound^2]\\
	&\le \E\|\weights_0 - \minimum\|^2 + [\lipConst^2+\stdBound^2]\sum_{n=0}^{N-1} \lr_n^2
\end{align*}
Dividing both sides by \(2\sum_{n=0}^{N-1}\lr_n=2T\) makes the term on the left a
convex combination allowing us to finally get 
\begin{align*}
	\E\left[\Loss\left(\sum_{n=0}^{N-1} \frac{\lr_n}{T}\Weights_n\right)\right]-\Loss(\minimum)
	&\le \sum_{n=0}^{N-1} \frac{\lr_n}{T}\E[\Loss(\Weights_n)]-\Loss(\minimum)\\
	&\le \frac{
		\|\weights_0 - \minimum\|^2 + [\lipConst^2+\stdBound^2]\sum_{n=0}^N \lr_n^2
	}{
		2T
	}.
\end{align*}
Now if there was no stochasticity one could instead use the minimum of
\(\Loss(\Weights_n)\) instead which is how one obtains the result in
Subsection~\ref{subsec: subgradient method}. But since we do have randomness
it appears to make sense to average the weights \(\Weights_n\) we collect over
time.
Now we can ask ourselves what learning rates \(\lr_n\) we should pick to
minimize our bound. Now due to convexity of the square we have
\begin{align*}
	1 = \left(\frac{1}{N}\sum_{n=0}^{N-1}\frac{N}{T}\lr_n\right)^2
	\xle{\text{conv.}} \frac{1}{N}\sum_{n=0}^{N-1}\left(\frac{N}{T}\lr_n\right)^2
	\le \frac{N}{T^2}\sum_{n=0}^{N-1}\lr_n^2,
\end{align*}
which means that constant learning rates \(\lr=T/N\) are optimal
\begin{align*}
	\sum_{n=0}^{N-1}\lr^2 = \frac{T^2}{N} \le \sum_{n=0}^{N-1}\lr_n^2.
\end{align*}
This leads to
\begin{align*}
	\E\Big[
		\Loss\Big(\underbrace{
			\frac{1}{N}\sum_{n=0}^{N-1}\Weights_n
		}_{=:\overline{\Weights}_N}\Big)
	\Big]-\Loss(\minimum)
	&\le \frac{
		\|\weights_0 - \minimum\|^2 + [\lipConst^2+\stdBound^2]N\lr^2
	}{2\lr N}.
\end{align*}
Where minimizing over \(\lr\) for a constant number of steps \(N\) now tunes the
ODE time \(T\) of which an increase entails larger discretizations but also
allows for more time following the ODE.
This minimization results in
\begin{align*}
	\lr_* = \frac{\|\weights_0-\minimum\|}{\sqrt{\lipConst^2+\stdBound^2}\sqrt{N}}
\end{align*}
and convergence rate
\begin{align*}
	\E[\Loss(\overline{\Weights}_N)] -\Loss(\minimum)
	\le \frac{\|\weights_0-\minimum\|\sqrt{\lipConst^2+\stdBound^2}}{\sqrt{N}}.
\end{align*}
In particular note that again decreasing \(\stdBound^2\) is (at best) equivalent
to increasing \(N\) and only if \(\lipConst\) is negligible. This matches our
theory that SGD without batches is preferable.

While this approach helps us lose the strong convexity assumption, which might
ensure decent convergence rates along eigenspaces of the hesse matrix with
small or zero eigenvalues, it is in a sense much more dependent on the convexity
of the loss function \(\Loss\) as our averaging of weights is only a
guaranteed improvement because of it. While Gradient Descent should work
intuitively on non-convex functions as well for finding a local minimum (only with
worse rates of convergence in eigenspaces with small eigenvalues, cf.
Subsection~\ref{subsec: condition in quadratic SGD}), this averaging technique
will generally not play nicely with non-convex functions.

Although if we moved into a convex area relatively quickly and stayed there for
most of our learning time then the first few weights outside this convex
area will be negligible for the average.
But it might still make sense to wait with the averaging until the time it is
actually needed to resume convergence especially since the rate of convergence
is also much worse. \textcite{bachNonstronglyconvexSmoothStochastic2013}
recover the convergence rate \(O(1/N)\) only for a very specific model.

\section{SDE View}

While we got bounds on \(\E\|\Weights_n - \minimum\|^2\) which entails a
distributional bound using something like Chebyshev's inequality, we only
have convergence of the expected value of the loss. It might therefore be
interesting to further develop our understanding of the distribution of \(\Weights_n\).

A more recent approach is to model SGD not as an approximation of an ODE but a
stochastic differential equation (SDE).


[\textcite{simsekliTailIndexAnalysisStochastic2019}]


\section{Heuristics}

\subsection{Adagrad}

\subsection{Adadelta}

\subsection{RMSProp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
