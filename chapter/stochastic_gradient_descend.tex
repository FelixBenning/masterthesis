% !TEX root = ../Masterthesis.tex

\chapter{Stochastic Gradient Decent (SGD)}

Until now we have always assumed we had access to the theoretical loss \(\Loss\) 
directly. In reality we almost never have. What we can do instead is use the
stochastic gradient
%
\begin{align*}
	\nabla_\weights\loss (\weights, X,Y) \qquad (X,Y)\sim\dist.
\end{align*}
%
Its expected value is the theoretical loss again\footnote{
	We need to be able to swap the expectation with differentiation. A sufficient 
	condition is continuity of \(\nabla\loss\) in \(\weights\), which allows us
	to use the mean value theorem
	\begin{align*}
		\frac{\partial}{\partial \weights_k}\E[\loss(\weights, X,Y)]
		&= \lim_{n\to\infty}
		\int\frac{\loss(\weights+\stdBasis_k/n, X,Y)-\loss(\weights,X,Y)}{1/n}d\Pr
		\\
		&=\lim_{n\to\infty} \int\frac{\partial}{\partial \weights_k}\loss(\xi_n, X,Y)d\Pr
		\qquad \xi_n \in [\weights, \weights + \stdBasis_k/n].
	\end{align*}
	Then we use the boundedness of a continuous function on the compact interval
	\([\weights, \weights + \stdBasis_k/n]\) to move the limit in using
	dominated convergence.
}. To shorten notation we will assume that the gradient is always meant
with respect to \(\weights\) unless otherwise stated.
Sampling an independent sequence \(((X_k,Y_k), k\ge 1)\) from the 
distribution \(\dist\) results in \emph{Stochastic Gradient Decent}
%
\begin{align*}
	\Weights_{n+1}
	&= \Weights_n - \lr_n\nabla\loss(\Weights_n, X_{n+1},Y_{n+1})\\
	&= \Weights_n - \lr_n\nabla\Loss(\Weights_n)
	+ \lr_n\underbrace{
		[\nabla\Loss(\Weights_n) - \nabla\loss(\Weights_n, X_{n+1}, Y_{n+1})]
	}_{=:\martIncr_{n+1}}
\end{align*}
Where \(\martIncr_n\) are martingale increments for the filtration
% \begin{align*}
% 	\martingale_n := \martingale_{n-1} + \martIncr_n \qquad \martingale_0:=0
% \end{align*}
\begin{align*}
	\filtration_n :=\sigma((X_k, Y_k): k\le n )
	\qquad (X_k,Y_k)\stackrel{\text{iid}}{\sim}\dist,
\end{align*}
since \(\Weights_n\) is \(\filtration_n\)-measurable which is independent of, i.e.
\((X_{n+1},Y_{n+1})\)
\begin{align}\label{eq: conditional independence of martingale increments}
	\E[\martIncr_{n+1}\mid \filtration_n]
	= \E[\nabla\Loss(\Weights_n) - \nabla\loss(\Weights_n, X_{n+1}, Y_{n+1})\mid \filtration_n]
	= 0.
\end{align}
%
\section{ODE View}

The first way to view SGD is through the lense of approximating the ODE with
integral equation
\begin{align*}
	\weights(t) =  \weights_0 -\int_{0}^{t}\nabla\Loss(\weights(s))ds.
\end{align*}
Its Euler discretization is gradient descend 
\begin{align*}
	\weights_{t_{n+1}}:=\weights_{n+1}
	= \weights_n - \lr_n \nabla\Loss(\weights_n),
\end{align*}
where
\begin{align*}
	0=t_0 \le \dots \le t_N= T \quad \text{with} \quad t_{n+1}-t_n=\lr_n
	\quad \text{and}\quad \lr:=\max_n \lr_n
\end{align*}
and constant learning rate \(\lr\) represents an equidistant discretization.
Since \(\nabla\Loss\) is Lipschitz continuous, the local discretization error
\begin{align*}
	\|\weights(\lr)  - \weights_\lr \|
	&= \|(\weights_0 - \int_0^\lr\nabla\Loss(\weights(s))ds) - (\weights_0-\lr\nabla\Loss(\weights_0))\|\\
	&\le  \int_0^\lr \underbrace{\|\nabla\Loss(\weights_0) - \nabla\Loss(\weights(s))\|}_{
		\le \ubound \|\weights(s) - \weights(0)\| \in O(s) \mathrlap{\quad\text{(Taylor approx., Lip. \(\nabla\Loss\))}}
	}ds 
\end{align*}
is of order \(O(\lr^2)\). 
Using the discrete Gr\"onwall's inequality and stability of the ODE (due to Lipschitz continuity of
\(\nabla\Loss\) and the continuous Gr\"onwall inequality) the global discretization
error is therefore of order 1, i.e.
\begin{align*}
	\max_{n} \|\weights(t_n) - \weights_n\| \in O(\lr),
\end{align*}
\fxnote{appendix?}{as is usually covered in ``Numerics of ODE's'' courses.}
Due to \((a+b)^2\le2(a^2+b^2)\), and therefore 
\begin{align*}
	\max_n\E[\|\weights(t_n)-\Weights_n\|^2]
	\le 2(\underbrace{\max_n \|\weights(t_n)-\weights_n\|^2}_{O(\lr^2)}
	+\max_{n}\E[\|\weights_n - W_n\|^2]),
\end{align*}
it is enough to bound the distance between SGD and GD.
\begin{theorem}\label{thm: distance SGD vs GD}
	In general we have
	\begin{align*}
		\E\|\weights(t_n)-\Weights_n\|^2
		\le \sum_{k=0}^{n-1}\lr_k^2\E\|\martIncr_{k+1}\|^2\exp\left(
			\sum_{j=k+1}^{n-1}\lr_j^2\ubound^2 + 2\lr_j\ubound
		\right)
	\end{align*}
	which can be simplified for constant learning rates \(\lr_n=\lr\) and bounded
	martingale increment variances \(\E\|\martIncr_n\|^2 \le \stdBound^2\) to
	\begin{align*}
		\max_{n}\E\|\weights(t_n)	-\Weights_n\|^2
		\le \lr T\stdBound^2\exp[T(\lr\ubound^2 + 2\ubound)] \in O(\lr)
	\end{align*}
\end{theorem}
Now before we get to the proof let us build some intuition why this result
is not surprising. Unrolling SGD we can rewrite it as
\begin{align}\label{eq: unrolled SGD}
	\Weights_{t_n}
	= \weights_0 - \sum_{k=0}^{n-1} \lr_k \nabla\Loss(\Weights_k)
	+ \sum_{k=0}^{n-1} \lr_k\martIncr_{k+1}.
\end{align}
Which is the same recursion as in GD except for the last term. Now for an
equidistance grid we have \(\lr=T/N\). Intuitively the last term in \(\Weights_T\)
should therefore disappear due to some law of large numbers for martingales
\begin{align}\label{eq: mean of martingale increments}
	\sum_{k=0}^{N-1}\lr\martIncr_{k+1} = \frac{T}{N}\sum_{k=1}^N\martIncr_k.
\end{align}
And since the variance of a sum of martingale increments is the sum of variances
as mixed terms disappear due to conditional independence (\ref{eq: conditional
independence of martingale increments})
\begin{align*}
	\E\left\|\sum_{k=1}^N\martIncr_k\right\|^2 = \sum_{k=1}^N \E\|\martIncr_k\|^2,
\end{align*}
the variance of a mean decreases with rate \(O(1/N)=O(\lr)\), which is the rate
we get in Theorem~\ref{thm: distance SGD vs GD}.
\begin{proof}[Proof (Theorem~\ref{thm: distance SGD vs GD})]
	Using the conditional independence (\ref{eq: conditional independence of martingale increments})
	we can get rid of a all the mixed terms with
	\(\E[\langle \cdot, \martIncr_{n+1}\rangle\mid\filtration_n]=0\) as everything
	else is \(\filtration_n\) measurable:
	\begin{align*}
		&\E\|\Weights_{n+1}-\weights_{n+1}\|^2
		= \E\|\Weights_n - \weights_n
		+ \lr_n(\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n))
		+ \lr_n\martIncr_{n+1}\|^2\\
		&\lxeq{(\ref{eq: conditional independence of martingale increments})}
		\begin{aligned}[t]
			&\E\|\Weights_n-\weights_n\|^2 + \lr_n^2\E\|\martIncr_{n+1}\|^2\\
			&+\underbrace{
				\lr_n^2\E\|\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\|^2
				+ 2\lr_n\E\langle\Weights_n-\weights_n,
				\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\rangle
			}_{
				\le 0 \qquad
				\text{for convex \(\Loss\) (Lemma~\ref{lem: bermanDiv lower bound}) and }
				\lr_n<2/\ubound
			}
		\end{aligned}
	\end{align*}
	But since we do not want to demand convexity of \(\Loss\) just yet, we will
	have to get rid of these terms in a less elegant fashion using Lipschitz
	continuity and the Cauchy-Schwarz inequality
	\begin{align*}
		\|\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\|^2
		&\le \ubound^2\|\weights_n-\Weights_n\|^2\\
		\langle\Weights_n-\weights_n,
		\nabla\Loss(\weights_n)-\nabla\Loss(\Weights_n)\rangle
		&\le \ubound \|\weights_n-\Weights_n\|^2
	\end{align*}
	which leads us to
	\begin{align*}
		\E\|\Weights_{n+1}-\weights_{n+1}\|^2
		\le (1+\lr_n^2\ubound^2 + 2\lr_n\ubound)\E\|\Weights_n-\weights_n\|^2
		+ \lr_n^2\E\|\martIncr_{n+1}\|^2.
	\end{align*}
	Now we apply the discrete Gr\"onwall inequality (Lemma~\ref{lem-appendix:
	discrete gronwall}) to get
	\begin{align*}
		\E\|\Weights_n-\weights_n\|^2
		\le \sum_{k=0}^{n-1}\lr_k^2\E\|\martIncr_k\|^2
		\underbrace{
			\prod_{j=k+1}^{n-1}(1+\lr_j^2\ubound^2 + 2\lr_j\ubound)
		}_{
			\le \exp\left(\sum_{j=k+1}^{n-1}\lr_j^2\ubound^2 + 2\lr_j\ubound\right)
		}
	\end{align*}
	using \(1+x\le\exp(x)\) for the first claim. The rest 
	follows from \(n\lr\le T\).
\end{proof}

\subsection{Excursion: Stochastic Approximation}

Theorem~\ref{thm: distance SGD vs GD} can feel somewhat unnatural as it requires
you to restart SGD completely with smaller learning rates to get behavior closer
to GD and its ODE. If the ODE has an attraction point/area/etc. one might
instead want to prove convergence to that within one run of SGD by selecting a
decreasing learning rate sequence. If we have a look at (\ref{eq: unrolled SGD})
it is quite intuitive that we are going to need
\begin{align*}
	\sum_{k=0}^\infty \lr_k = \lim_{n\to\infty}\sum_{k=0}^{n-1} \lr_k
	= \lim_{n\to\infty}t_n = \infty.
\end{align*}
Otherwise we can not really claim that SGD should behave similar to the asymptotic
behavior of an ODE as we can not let \(t\to\infty\) in the ODE. Additionally
we somehow need to do variance reduction to get rid of
\begin{align*}
	\sum_{k=m}^{m-1} \lr_k\martIncr_{k+1}
\end{align*}
to argue that from starting point \(\Weights_{t_m}\) onwards SGD behaves like the ODE
and therefore ends up in its attraction areas. So we also need
\(\lr_k \to 0\). How fast it needs to vanish depends on the amount of (bounded)
moments we have on \(\martIncr_k\) \parencite[p. 110]{kushnerStochasticApproximationAlgorithms1997}.
If we only have bounded second moments as we have assumed in Theorem~\ref{thm:
distance SGD vs GD} then we get the classic Stochastic Approximation conditions
on the learning rate
\begin{align*}
	\sum_{n=0}^\infty \lr_n = \infty \qquad \text{and} \qquad \sum_{n=0}^\infty \lr_n^2 <\infty
\end{align*}
I.e. learning rates \(\lr_n\) which decrease faster than \(O(1/\sqrt{n})\) but
not faster than \(O(1/n)\). See \textcite[ch.
5]{kushnerStochasticApproximationAlgorithms1997} for a proof of
convergence with probability one of the recursion
\begin{align*}
	\Weights_{n+1} = \Weights_n - \lr_n(g(\Weights_n) + \martIncr_{n+1})
\end{align*}
towards the asymptotic areas of the limiting
ODE
\begin{align*}
	\dot{\weights} = g(\weights)
\end{align*}
under those assumptions.

Note that \(g\) can be, but needs not be a gradient (we have not used that in
Theorem~\ref{thm: distance SGD vs GD} either). And this direction of
generalization is necessary to craft convergence proofs for Reinforcement
Learning where we do not get unbiased estimates of some gradients towards the
minimum, but rather unbiased estimates of a Bellman Operator (Fixed Point 
Iteration towards the optimum).

\section{Batch Learning}

Instead of using \(\nabla\loss(\Weights_n, X_{n+1}, Y_{n+1})\) at time \(n\) as an
estimator for \(\nabla\Loss(\Weights_n)\) we could use the average of a
``batch'' of data \((X^{(i)}_{n+1}, Y^{(i)}_{n+1})_{i=1,\dots,m}\) (independently
\(\dist\) distributed) instead, i.e.
\begin{align*}
	\Weights_{n+1} = \Weights_n
	-\lr_n \underbrace{\frac1{m}\sum_{i=1}^m\nabla\loss(\Weights_n,X^{(i)}_{n+1}, Y^{(i)}_{n+1})}_{
		=:\nabla\loss_{n+1}^m(\Weights_n)
	}.
\end{align*}
leading to the modified martingale increment
\begin{align*}
	\martIncr_n^{(m)}
	= \nabla\Loss(\Weights_{n-1})
	- \nabla\loss_n^m(\Weights_{n-1})
\end{align*}
with reduced variance
\begin{align*}
	\E\|\martIncr_n^{(m)}\|^2 = \tfrac1m\E\|\martIncr_n^{(1)}\|^2 \le \tfrac1m \stdBound^2.
\end{align*}
Now while this variance reduction does reduce our upper bound on the distance
between SGD and GD (cf. Theorem~\ref{thm: distance SGD vs GD}), this reduction
comes at a cost: We now have to do \(m\) gradient evaluations per iteration!
We would incur an equivalent cost, if we reduced our discretization to
\begin{align*}
	\tilde{\lr}:=\frac{\lr}{m}=\frac{T}{Nm}
\end{align*}
which increases the number of iterations to reach \(T\) to \(Nm\) which implies
\(Nm\) gradient evaluations using SGD without batches. And if we have a look
at the upper bound in Theorem~\ref{thm: distance SGD vs GD} these two actions
have the same effect on our distance bound between SGD and GD (especially if
\(\Loss\) is convex as we can then get rid of the exponential term as hinted
at in the proof of the theorem).

Now of course an upper bound is no guarantee that there is no effect in reality.
But if we have a look at the term (\ref{eq: mean of martingale increments})
again which is in some sense the difference between SGD and GD, then we can
see that these actions have quite a similar effect
\begin{align*}
	\lr\sum_{k=1}^N\martIncr_k^{(m)}
	&=\frac{T}{N}\sum_{k=1}^N
	\nabla\Loss(\Weights_{k-1})- \nabla\loss_n^m(\Weights_{k-1})\\
	&=\frac{T}{Nm}\sum_{k=1}^N\sum_{i=1}^m
	[\nabla\Loss(\Weights_{k-1})-\nabla\loss(\Weights_{k-1},X^{(i)}_k, Y^{(i)}_k)]\\
	&\approx \tilde{\lr} \sum_{k=1}^{Nm}
	\underbrace{
		[\nabla\Loss(\Weights_{k-1})-\nabla\loss(\Weights_{k-1},X_k, Y_k)]
	}_{\widetilde{\martIncr}_k}.
\end{align*}
The difference is, that batch learning stays on some \(\Weights_k\) and collects
\(m\) pieces of information before it makes a big step based on this information
while SGD just makes \(m\) small steps. This also changes the true gradient
evaluations \(\nabla\Loss(\Weights_k)\) slightly. But since we are only making
small steps these \(\Weights_k\) will generally be similar. We still can not
make a definitive assertion which one is better though\footnote{
	Whether or not you think batch-less SGD is superior might be correlated with
	whether or not you think that agile software development is a good idea
}.

Assuming they have the same effect on the deviation of SGD from GD we can
make a few practical considerations:
\begin{itemize}
	\item A smaller learning rate benefits GD \emph{and} reduces the distance
	between GD and SGD while batches only do the latter. If we have little data
	and want to achieve good results in as few gradient evaluations as possible
	we should in general choose smaller learning rates over larger batch sizes.
	\item SGD can not be easily parallelized since one needs the previous weights
	to calculate the next weights. Making multiple gradient evaluations at
	the same point is trivially parallelizable on the other hand and only
	requires broadcasting the result for summation. So if the bottleneck is
	computation time, it makes sense to select batch sizes equal to the number of
	threads, or if the broadcasting and summing takes significant time, batch
	sizes which are multiples of the number of threads.

	This is not without caveats though: Increasing the Batch Size to speed up
	learning requires a proportional increase in the learning rate to actually
	have an impact on the time required for optimization. And since GD stops
	converging if learing rates become too large, there is an upper bound on this.
	And while batches might be ``free'' in a computational sense due to
	parallelization, they still increase the sample consumption. And if we only
	have limited data, this often results in more passes over the same data and
	thus overfitting.
	
	Last but not least there might be other opportunities for parallelization
	like generating ensemble models by optimizing from different starting points
	or using different models altogether.
\end{itemize}
In summary: If you are concerned about the \emph{quality} of your model you should
probably choose SGD without batches, while concerns about training time justify
larger batch sizes in some cases.

\fxnote{
\textcite{hardtTrainFasterGeneralize2016} \textcite{hofferTrainLongerGeneralize2018}
}

\section{Quadratic Loss Functions}

While we have proven SGD behaves roughly like GD for small learning rates, we
are not really that interested in how close SGD is to GD but rather how good it
is at optimizing \(\Loss\). So to build some intuition let us consider a
quadratic Loss function again before we get to general convex functions.
Using the same trick we used in Section~\ref{sec: visualize gd} 
\begin{align*}
	\nabla\Loss(\weights)
	= \nabla^2\Loss(\weights)(\weights-\minimum)
	= H(\weights-\minimum)
\end{align*}
we can rewrite SGD (by induction using the triviality of \(n=0\)) as
\begin{align}
	\nonumber
	&\Weights_{n+1}-\minimum\\
	\nonumber
	&= \Weights_n - \minimum - \lr_n H(\Weights_n-\minimum) + \lr_n\martIncr_{n+1}\\
	\nonumber
	&=(1-\lr_n H)\underbrace{(\Weights_n - \minimum)}_{
		\xeq{\text{ind.}} (\weights_n - \minimum)
		\mathrlap{+ \sum_{k=0}^{n-1}\lr_k\left(\prod_{i=k+1}^{n-1}(1-\lr_iH)\right)\martIncr_{k+1}}
	} + \lr_n\martIncr_{n+1}\\
	\nonumber
	&= \underbrace{(1-\lr_n H)(\weights_n - \minimum)}_{=\weights_{n+1}-\minimum \implies (n\to n+1)}
	+ \sum_{k=0}^n\lr_k\left(\prod_{i=k+1}^n(1-\lr_iH)\right)\martIncr_{k+1}\\
	\label{eq: unrolled SGD weights (general quadratic loss case)}
	&=\left(\prod_{k=0}^n(1-\lr_kH)\right)(\weights_0-\minimum)
	+ \sum_{k=0}^n\lr_k\left(\prod_{i=k+1}^n(1-\lr_iH)\right)\martIncr_{k+1}.
\end{align}

\subsection{Variance Reduction}\label{subsec: variance reduction}

One particularly interesting case is \(H=\identity\). Why is that interesting?
Considering that
\begin{align*}
	\E[\tfrac12\|\weights-X\|^2]=\E[\loss(\weights,X)]=:\Loss(\weights)
\end{align*}
is minimized by \(\minimum=\E[X]\) and we have
\begin{align*}
	\E[\nabla\loss(\weights,X)]
	=\E[\weights-X]=\weights- \minimum=\nabla\Loss(\weights)
\end{align*}
we necessarily also have
\begin{align*}
	\Loss(\weights) = \tfrac12\|\weights-\minimum\|^2 + \text{const},
\end{align*}
which implies \(H=\nabla^2\Loss=\identity\). So this case is essentially
trying to find the expected value of some random variables as quickly as
possible. Now by (\ref{eq: unrolled SGD weights (general
quadratic loss case)}) and
\begin{align*}
	\martIncr_n = \nabla\Loss(\Weights_{n-1}) -\nabla\loss(\weights_{n-1}, X_n)
	= X_n - \E[X]
\end{align*}
the \(\Weights_n\) are just an affine transformation of the \(X_n\). And we
happen to know the best linear unbiased estimator (``BLUE'') of \(\E[X]\)
which is the mean of the \(X_n\). This means that \(\Weights_n\) is
necessarily worse that the mean. But with \(\lr_n=\frac1{n+1}\) and by
induction
\begin{align}
	\label{eq: getting rid of w_0 first step}
	\Weights_1 &= \weights_0 - \frac{1}{0+1}(\weights_0 - X_1) = X_1\\
	\nonumber
	\Weights_{n+1}
	&= \Weights_n - \tfrac{1}{n+1}(\Weights_n - X_{n+1})
	\xeq{\text{ind.}}\smash{\underbrace{\left(1-\tfrac1{n+1}\right)}_{=\frac{n}{n+1}}}
	\frac1n\sum_{k=1}^n X_k + \tfrac{1}{n+1}X_{n+1}\\
	\nonumber
	&= \frac{1}{n+1}\sum_{k=1}^{n+1}X_k
\end{align}
we can also achieve this optimum. For constant learning rates \(\lr<1\) on the
other hand one can similarly show that we have
\begin{align*}
	\Weights_n = (1-\lr)^n\weights_0 + \sum_{k=0}^{n-1}\lr(1-\lr)^{n-1-k}X_{k+1}
\end{align*}
or to highlight the connection to (\ref{eq: unrolled SGD weights (general
quadratic loss case)})
\begin{align*}
	\Weights_n - \minimum
	= \underbrace{(1-\lr)^n(\weights_0 -\minimum)}_{=\weights_n-\minimum}
	+ \sum_{k=0}^{n-1}\lr(1-\lr)^{n-1-k}
	\smash{\overbrace{\martIncr_{k+1}}^{X_{k+1}-\minimum}}.
\end{align*}
This implies we do not weight our \(X_k\) equally, but rather use an
exponential decay giving the most recent data the most weight.

Notice how we can reobtain the rates from Theorem~\ref{thm: distance SGD vs GD}
by throwing this entire exponential decay away
\begin{align*}
	\E\|\Weights_n-\weights_n\|^2
	&= \E\left\|\sum_{k=0}^{n-1}\lr(1-\lr)^{n-1-k}\martIncr_{k+1}\right\|^2\\
	&= \sum_{k=0}^{n-1}\lr^2\underbrace{(1-\lr)^{2(n-1-k)}}_{\le1}
	\underbrace{\E\|\martIncr_{k+1}\|^2}_{\le \stdBound^2}\\
	&\le \lr^2 n \stdBound^2 \le \lr T\stdBound^2.
\end{align*}
We can do the same thing with the general case (\ref{eq: unrolled SGD weights
(general quadratic loss case)}) if we take \(\lr<2/\ubound\) for \(|H|\le\ubound\).

Now this fact is making me quite anxious about the claim that batch learning is
worse than SGD in general. But we also have to keep in mind how special this
case is. In (\ref{eq: getting rid of w_0 first step}) we can get rid of the
initial bias \(\weights_0\) completely. This is only possible because our
condition number \(\condition\) is one which allows one step convergence of GD.

\subsection{The Effect of the Condition Number}

Now by diagonalizing \(H\) and using the fact that
\begin{align*}
	&\prod_{k=0}^{n-1} (1-\lr_k V\diag[\hesseEV_1, \dots, \hesseEV_\dimension]V^T)\\
	&= V \diag\left[
		\prod_{k=0}^{n-1}(1-\lr_k\hesseEV_1),
		\dots, \prod_{k=0}^{n-1}(1-\lr_k\hesseEV_\dimension)
	\right]V^T
\end{align*}
we can consider all the eigenspaces of (\ref{eq: unrolled SGD weights (general
quadratic loss case)}) separately again
\begin{align*}
	\langle \Weights_{n+1}-\minimum, v_j \rangle
	= \begin{aligned}[t]
		&\left(\prod_{k=0}^n(1-\lr_k\hesseEV_j)\right) \langle \weights_0-\minimum, v_j\rangle\\
		&+ \sum_{k=0}^n \lr_k \left(\prod_{i=k+1}^n(1-\lr_i\hesseEV_j)\right)\langle\martIncr_{k+1},v_j\rangle.
	\end{aligned}
\end{align*}
With the usual conditional independence argument we get
\begin{align*}
	\E\|\Weights_{n+1}-\minimum\|^2
	&= \E[(V^T [\Weights_{n+1}-\minimum])^T(V^T[\Weights_{n+1}-\minimum])]\\
	&= \sum_{j=1}^\dimension \E\langle\Weights_{n+1}-\minimum, v_j \rangle^2\\
	&\le \sum_{j=1}^\dimension
	\begin{aligned}[t]
		&\Bigg[\left(\prod_{k=0}^n(1-\lr_k\hesseEV_j)^2\right) \langle \weights_0-\minimum, v_j\rangle^2\\
		&+ \sum_{k=0}^n \lr_k^2 \left(\prod_{i=k+1}^n(1-\lr_i\hesseEV_j)\right)^2\E[\langle\martIncr_{k+1},v_j\rangle^2]\Bigg].
	\end{aligned}
\end{align*}
it is pretty obvious at this point that only the largest and smallest eigenvalue
are going to matter again for the final convergence rate. Sacrificing the
constant \(\dimension\) for simplicity and looking at only one eigenspace
we can use \(1+x\le\exp(x)\) for
\begin{align*}
		&\left(\prod_{k=0}^n(1-\lr_k\hesseEV)^2\right) \|\weights_0-\minimum\|^2
		+ \sum_{k=0}^n \lr_k^2 \left(\prod_{i=k+1}^n(1-\lr_i\hesseEV)\right)^2\stdBound^2\\
		&\le \exp\left(-2\sum_{k=0}^n\lr_k\hesseEV\right)\|\weights_0-\minimum\|^2
		+ \sum_{k=0}^n \lr_k^2 \exp\left(-2\sum_{i=k+1}^n\lr_i\hesseEV\right)\stdBound^2\\
\end{align*}

\section{Expected Square Error Analysis}

This section is inspired by \textcite{nemirovskiRobustStochasticApproximation2009}.

\subsection{Strongly Convex Case}

While we are not going to tether ourselves to Gradient Decent \(\weights_n\) in
this section, we are still going to split off the noise
\begin{align*}
	\|\Weights_{n+1} - \minimum\|^2
	&= \|\Weights_n -\minimum - \lr_n\nabla\Loss(\weights_n) +\lr_n \martIncr_{n+1}\|^2\\
	&= \begin{aligned}[t]
		&\|\Weights_n - \minimum - \lr_n\nabla\Loss(\Weights_n)\|^2\\
		&+ 2\langle \Weights_n - \minimum - \lr_n\nabla\Loss(\Weights_n), \lr_n \martIncr_{n+1}\rangle\\
		&+ \lr_n^2 \|\martIncr_{n+1}\|^2
	\end{aligned}
\end{align*}
The scalar product will disappear once we apply expectation due to conditional
independence again. For the first part we are going to do the same as in the
classical case in Theorem~\ref{thm: gd strong convexity convergence rate}:
\begin{align}
	\nonumber
	&\|\Weights_n - \minimum - \lr_n\nabla\Loss(\Weights_n)\|^2\\
	\label{eq: getting rid of the scalar product (SGD)}
	&= \|\Weights_n - \minimum\|^2
	- 2\lr_n\underbrace{\langle\nabla\Loss(\Weights_n), \Weights_n-\minimum\rangle}_{
		\xge{\text{Lem.~\ref{lem: bermanDiv lower bound (strongly convex)}}}
	 	\tfrac{\ubound\lbound}{\ubound+\lbound}\|\Weights_n - \minimum\|^2
		\mathrlap{+ \tfrac{1}{\ubound+\lbound}\|\nabla\Loss(\Weights_n)\|^2}
	}
	+ \lr_n^2 \|\nabla\Loss(\Weights_n)\|^2\\
	\nonumber
	&\le \left(1-2\lr_n\tfrac{\ubound\lbound}{\ubound+\lbound}\right)
	\|\Weights_n - \minimum\|^2
	+ \lr_n(\lr_n - \tfrac{2}{\ubound+\lbound})
	\|\nabla\Loss(\Weights_n)\|^2
\end{align}
For \(\lr_n\le\tfrac{2}{\ubound+\lbound}\) we can drop the gradient again to
obtain
\begin{align*}
	\E\|\Weights_{n+1} - \minimum\|^2
	\le \left(1-2\lr_n\tfrac{\ubound\lbound}{\ubound+\lbound}\right)
	\E\|\Weights_n - \minimum\|^2 + \lr_n^2 \underbrace{\E\|\martIncr_{n+1}\|^2}_{\le\stdBound^2}.
\end{align*}
In the classical case we just wanted to select \(\lr_n\) as large as possible,
i.e. \(\lr_n=\tfrac{2}{\ubound+\lbound}\). But in the stochastic setting there
are adverse effects of large learning rates due to noise. Now as our upper bound
is a convex parabola in \(\lr_n\) we can just use the first order condition
\begin{align*}
	\frac{d}{d\lr_n}
	= -2\tfrac{\ubound\lbound}{\ubound+\lbound}
	\E\|\Weights_n - \minimum\|^2 + 2\lr_n \stdBound^2
	\xeq{!} 0
\end{align*}
to find the best upper bound
\begin{align*}
	\lr_n
	= \min\left\{
		\tfrac{\ubound\lbound\E\|\Weights_n - \minimum\|^2}{\stdBound^2(\ubound+\lbound)},
		\frac{2}{\ubound+\lbound}
	\right\}.
\end{align*}
So we still want to ``max out our learing rate'' and use
\(\lr_n=\tfrac{2}{\ubound+\lbound}\) if
\begin{align*}
	\frac{2\stdBound^2}{\ubound\lbound} \le \E\|\Weights_n - \minimum\|^2.
\end{align*}
In other words: the stochastic variance only becomes a problem once we are
close to the minimum. Before that we basically want to treat our problem like
the classical one. This results in two training phases, the \emph{transient}
phase in which we are still far from the minimum, our learning rate is constant
and we get a linear convergence rate
\begin{align*}
	\E\|\Weights_{n+1} - \minimum\|^2
	&\le \left(1-2\tfrac{2}{\ubound+\lbound}\tfrac{\ubound\lbound}{\ubound+\lbound}\right)
	\E\|\Weights_n - \minimum\|^2 + \left(\tfrac{2}{\ubound+\lbound}\right)^2
	\underbrace{\stdBound^2}_{
		\le\tfrac{\ubound\lbound}{2} \E\|\Weights_n - \minimum\|^2
	}\\
	&\le \underbrace{
		\left(1-\tfrac{2\ubound\lbound}{(\ubound+\lbound)^2}\right)
	}_{
		=1-\frac{2}{(1+\condition^{-1})(1+\condition)}
	}
	\E\|\Weights_n - \minimum\|^2.
\end{align*}
In the \emph{asymptotic} phase on the other hand, we have to reduce our learning
rate with the distance to the minimum. The convergence rate is
\begin{align*}
	&\E\|\Weights_{n+1} - \minimum\|^2\\
	&\le \left(1-2
		\tfrac{\ubound\lbound\E\|\Weights_n - \minimum\|^2}{\stdBound^2(\ubound+\lbound)}
		\tfrac{\ubound\lbound}{\ubound+\lbound}
	\right)
	\E\|\Weights_n - \minimum\|^2
	+ \left(
		\tfrac{\ubound\lbound\E\|\Weights_n - \minimum\|^2}{\stdBound^2(\ubound+\lbound)}
	\right)^2
	\stdBound^2\\
	&\le \left(1-
		\left(\tfrac{\ubound\lbound}{\stdBound(\ubound+\lbound)}\right)^2
		\E\|\Weights_n - \minimum\|^2
	\right)
	\E\|\Weights_n - \minimum\|^2
\end{align*}
Using Corollary~\ref{cor-appendix: diminishing contraction} the recursion above
implies
\begin{align*}
	\E\|\Weights_n - \minimum\|^2\in O(1/n).
\end{align*}
This also implies \(\lr_n\in O(1/n)\).

Due to Lipschitz continuity of the gradient, the convergence rate of the Loss is
\begin{align*}
	\E[\Loss(\Weights_n)]- \Loss(\minimum) \le \tfrac{\ubound}{2}\E\|\Weights_n-\minimum\|^2
\end{align*}

While the second of the two phases of SGD \parencite[first observed
by][]{darkenFasterStochasticGradient1991} determines the asymptotic properties
of SGD  ``in practice it seems that for deeper networks in particular, the first
phase dominates overall computation time as long as the second phase is cut off
before the remaining potential gains become either insignificant or entirely
dominated by overfitting (or both).''  
\parencite{sutskeverImportanceInitializationMomentum2013}. They also say about
temporary plateaus, which could be a sign that the first phase is over since
we do no longer have a contraction for constant learning rates, that 
``despite appearing to not make much progress, or even becoming significantly
non-monotonic, the optimizers were doing something apparently useful over these
extended periods'' as the final errors observed where lower than when they
tampered with the system (in their case change the momentum coefficient
\(\momCoeff\) which we will discuss in chapter~\ref{chap: momentum}).
Recalling Figure~\ref{fig: visualize saddlepoint gd} these temporary plateaus
might be saddlepoints.

Since it is difficult to discern the second phase from plateaus due to saddlepoints
this makes the decisions whether or not to decrease the learning rate in those
cases difficult. The averaging approach in the next subsection is a possible
solution to this problem. And it also does away with the strong convexity
assumption.

\subsection{General Convex Case -- Averaging}\label{subsec: SGD with Averaging}

Since the noise prevents us from using Lemma~\ref{lem: bermanDiv lower bound} in
place of Lemma~\ref{lem: bermanDiv lower bound (strongly convex)} to ensure a
decreasing distance from the optimum, we can not use the same approach we used
in the classical convex loss function setting (Section~\ref{sec: convex convergence theorems}).

But since we know that the best asymptotic rates we can get will be
\(O(1/\sqrt{n})\) due to the averaging example in Subsection~\ref{subsec:
variance reduction} we are going to use the technique developed to show
convergence for the subgradient method (cf. Subsection~\ref{subsec: subgradient
method}).
In particular we are going to assume \(\Loss \in
\lipGradientSet[0,0]{\lipConst}\). And instead of the Lemmas mentioned above we
are going to simply use \fxnote{appendix equivalence}{the (subgradient)
definition of convexity}
\begin{align*}
	\Loss(\minimum)
	\ge \Loss(\Weights_n) + 
	\langle\nabla\Loss(\Weights_n), -(\Weights_n - \minimum)\rangle
\end{align*}
in (\ref{eq: getting rid of the scalar product (SGD)})
\begin{align*}
	&\|\Weights_n - \minimum - \lr_n\nabla\Loss(\Weights_n)\|^2\\
	&\le \|\Weights_n - \minimum\|^2
	- 2\lr_n\underbrace{\langle\nabla\Loss(\Weights_n), \Weights_n-\minimum\rangle}_{
		\ge \Loss(\Weights_n) - \Loss(\minimum)
	}
	+ \lr_n^2 \|\nabla\Loss(\Weights_n)\|^2
\end{align*}
to obtain
\begin{align*}
	\E\|\Weights_{n+1}-\minimum\|^2
	&\le
	\begin{aligned}[t]
		&\E\|\Weights_n -\minimum\|^2
		- 2\lr_n \E[\Loss(\Weights_n) - \Loss(\minimum)]\\
		&+ \lr_n^2 [
			\underbrace{\E\|\nabla\Loss(\Weights_n)\|^2}_{\le \lipConst^2}
			+ \underbrace{\E\|\martIncr_{n+1}\|^2}_{\le \stdBound^2}
		]
	\end{aligned}
\end{align*}
as \(\Loss\in\lipGradientSet[0,0]{\lipConst}\) implies Lipschitz continuity and thus bounded
gradients. Note that splitting \(\nabla\loss\) into \(\nabla\Loss\) and \(\martIncr\)
was of no use here as we end up summing them in the end. \textcite{nemirovskiRobustStochasticApproximation2009}
uses bounded \(\E\|\nabla\loss\|^2\) from the get go to get the same result.
This results in
\begin{align*}
	&2\sum_{n=0}^{N-1} \lr_n\E[\Loss(\Weights_n)-\Loss(\minimum)]\\
	&\le \sum_{n=0}^{N-1} \E\|\Weights_n -\minimum\|^2 - \E\|\Weights_{n+1}-\minimum\|^2
	+ \lr_n^2 [\lipConst^2 + \stdBound^2]\\
	&\le \E\|\weights_0 - \minimum\|^2 + [\lipConst^2+\stdBound^2]\sum_{n=0}^{N-1} \lr_n^2
\end{align*}
Dividing both sides by \(2\sum_{n=0}^{N-1}\lr_n=2T\) makes the term on the left a
convex combination allowing us to finally get 
\begin{align*}
	\E\left[\Loss\left(\sum_{n=0}^{N-1} \frac{\lr_n}{T}\Weights_n\right)\right]-\Loss(\minimum)
	&\le \sum_{n=0}^{N-1} \frac{\lr_n}{T}\E[\Loss(\Weights_n)]-\Loss(\minimum)\\
	&\le \frac{
		\|\weights_0 - \minimum\|^2 + [\lipConst^2+\stdBound^2]\sum_{n=0}^N \lr_n^2
	}{
		2T
	}.
\end{align*}
Now if there was no stochasticity one could instead use the minimum of
\(\Loss(\Weights_n)\) instead which is how one obtains the result in
Subsection~\ref{subsec: subgradient method}. But since we do have randomness
it appears to make sense to average the weights \(\Weights_n\) we collect over
time.
Now we can ask ourselves what learning rates \(\lr_n\) we should pick to
minimize our bound. Now due to convexity of the square we have
\begin{align*}
	1 = \left(\frac{1}{N}\sum_{n=0}^{N-1}\frac{N}{T}\lr_n\right)^2
	\xle{\text{conv.}} \frac{1}{N}\sum_{n=0}^{N-1}\left(\frac{N}{T}\lr_n\right)^2
	\le \frac{N}{T^2}\sum_{n=0}^{N-1}\lr_n^2,
\end{align*}
which means that constant learning rates \(\lr=T/N\) are optimal
\begin{align*}
	\sum_{n=0}^{N-1}\lr^2 = \frac{T^2}{N} \le \sum_{n=0}^{N-1}\lr_n^2.
\end{align*}
This leads to
\begin{align*}
	\E\Big[
		\Loss\Big(\underbrace{
			\frac{1}{N}\sum_{n=0}^{N-1}\Weights_n
		}_{=:\Weights^N}\Big)
	\Big]-\Loss(\minimum)
	&\le \frac{
		\|\weights_0 - \minimum\|^2 + [\lipConst^2+\stdBound^2]N\lr^2
	}{2\lr N}.
\end{align*}
Where minimizing over \(\lr\) for a constant number of steps \(N\) now tunes the
ODE time \(T\) of which an increase entails larger discretizations but also
allows for more time following the ODE.
This minimization results in
\begin{align*}
	\lr_* = \frac{\|\weights_0-\minimum\|}{\sqrt{\lipConst^2+\stdBound^2}\sqrt{N}}
\end{align*}
and convergence rate
\begin{align*}
	\E[\Loss(\Weights^N)] -\Loss(\minimum)
	\le \frac{\|\weights_0-\minimum\|\sqrt{\lipConst^2+\stdBound^2}}{\sqrt{N}}.
\end{align*}

While this approach helps us lose the strong convexity assumption, which might
ensure descend convergence rates along eigenspaces of the hesse matrix with
small or zero eigenvalues, it is in a sense much more dependent on the convexity
of the loss function \(\Loss\) as our averaging of weights is only a
guaranteed improvement because of it. While Gradient Decent should work
intuitively on non-convex functions as well for finding a local minimum, this
averaging technique will generally not play nicely with non-convex functions.
At the same time we are not averaging during iteration but only at the end.
So if we moved into a convex area relatively quickly and stayed there for
most of our learning time then the first few weights outside this convex
area will be negligible for the average.
But it might still make sense to wait with the averaging until the time it is
actually needed to resume convergence.

\fxnote{cf. \cite{bachNonstronglyconvexSmoothStochastic2013}}

\section{SDE View}

\textcite{simsekliTailIndexAnalysisStochastic2019}


\section{Heuristics}

\subsection{Adagrad}

\subsection{Adadelta}

\subsection{RMSProp}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
