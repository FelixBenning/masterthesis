% !TEX root = ../Masterthesis.tex
\chapter{Other Methods}


\section{Heuristics for Adaptive Learning Rates}
\label{sec: heuristics for adpative learning rates}

There are a number of widely used heuristics which are provided in the
default optimizer selection of machine learning frameworks such as Tensorflow.

As we have only covered \ref{eq: gradient descent} and momentum so far, we have
quite a bit of ground to cover. But at the same time most of the remaining
common optimizer \parencite[as reviewed by e.g.][]{ruderOverviewGradientDescent2017}
can be motivated in a similar way, which might or might not be new, as I did not
properly search previous work for this idea.

Recall that the \ref{eq: newton minimum approx}
\begin{align*}
	\weights_{n+1}	= \weights_n - (\nabla^2\Loss(\weights_n))^{-1}\nabla\Loss(\weights_n)
\end{align*}
finds the vertex of a quadratic loss function \(\Loss\) immediately, because we
have
\begin{align*}
	\nabla\Loss(\weights_n) = \nabla^2\Loss(\weights_n)[\weights_n - \hat{\weights}_n],
\end{align*}
where \(\hat{\weights}_n\) was the vertex of the second Taylor approximation.
To better understand how that works, note that for
\begin{align*}
	\nabla^2\Loss(\weights_n) = V \diag(\hesseEV_1,\dots,\hesseEV_\dimension)V^T
\end{align*}
we have
\begin{align*}
	\nabla^2\Loss(\weights_n)^{-1}
	= V \diag(\tfrac1\hesseEV_1,\dots,\tfrac1\hesseEV_\dimension)V^T.
\end{align*}
To simplify things, let us assume a quadratic loss function
\(H=\nabla^2\Loss(\weights_n)\) now.  In this case we found that every
eigenspace scales independently
\begin{align*}
	\langle \weights_{n+1} - \minimum, v_i \rangle
	\le (1-\lr \hesseEV_i)\langle \weights_n - \minimum, v_i \rangle
\end{align*}
So the \ref{eq: newton minimum approx} rescales the steps by dividing every
eigenspace through its respective eigenvalue. For positive eigenvalues this is
all nice and dandy. But for negative eigenvalues, which represent a maximum in
their eigenspace, the \ref{eq: newton minimum approx} multiplies the gradient by
a negative value \(\tfrac1{\hesseEV_i}\) and therefore causes movement towards
this eigenvalue.

Okay, now that we remembered these crucial problems, let us make one egregious
assumption. Let us assume that the standard basis are already the eigenvectors,
i.e. \(V=\identity\). Then the components of the gradient of our loss function
are proportional to their respective eigenvalues
\begin{align*}
	\nabla\Loss(\weights_n)^{(i)}
	= \hesseEV_i [\weights_n - \minimum]^{(i)}.
\end{align*}
If we define
\begin{align*}
	G_n^{(i)} = \sum_{k=0}^n [\nabla\Loss(\weights_n)^{(i)}]^2 \qquad i=1,\dots,\dimension,
\end{align*}
then the algorithm
\begin{align}
	\label{eq: almost adagrad}
	\weights_{n+1}^{(i)}
	&= \weights_n^{(i)} - \frac{\lr_n}{\sqrt{G_n^{(i)}}}\nabla\Loss(\weights_n)^{(i)}\\
	\nonumber
	&= \weights_n^{(i)} - \frac{\lr_n}{
		\sqrt{\cancel{\hesseEV_i^2}\sum_{k=0}^n([\weights_k - \minimum]^{(i)})^2}
	}
	\cancel{\hesseEV_i}[\weights_n - \minimum]^{(i)}
\end{align}
also cancels out the eigenvalues. But as we divide through the absolute value of
the eigenvalues in contrast to the \ref{eq: newton minimum approx}, we get to
keep the sign
\begin{align}\label{eq: adagrad motivation contraction}
	[\weights_{n+1}-\minimum]^{(i)} = \left(1-\frac{\lr_n\sign(\hesseEV_i)}{
			\sqrt{\sum_{k=0}^n([\weights_k - \minimum]^{(i)})^2}
		}\right)[\weights_n - \minimum]^{(i)}.
\end{align}
Of course the notion that the standard basis vectors are already the eigenvectors
is pretty ridiculous. But for some reason the following algorithms seem to
work. And they are basically based on this notion of reweighting the components
by dividing them through the squared past gradients in some sense. The intuition
provided by \textcite{ruderOverviewGradientDescent2017} is, that
``It adapts the learning rate to the parameters, performing smaller updates
(i.e. low learning rates) for parameters associated with frequently occurring
features, and larger updates (i.e. high learning rates) for parameters
associated with infrequent features.'' But even this intuition seems to assume
that these ``features'' seem to somehow correspond to the coordinates.

\begin{description}
	\item[AdaGrad] \parencite{duchiAdaptiveSubgradientMethods2011}
	We have already motivated most of AdaGrad in (\ref{eq: almost adagrad}).
	But we are going to use the stochastic gradients
	\begin{align*}
		G_n^{(i)} := \sum_{k=0}^n [\nabla\loss(\weights_n, Z_n)^{(i)}]^2 \qquad i=1,\dots,\dimension,
	\end{align*}
	and introduce a small term \(\epsilon\) \parencite[\(\approx 10^{-8}\)][]{ruderOverviewGradientDescent2017}
	to avoid numerical instability. This result in the ``AdaGrad'' algorithm
	\begin{align*}
		\Weights_{n+1}^{(i)}
		&= \Weights_n^{(i)}
		- \frac{\lr_n}{\sqrt{G_n^{(i)}+\epsilon}}\nabla\loss(\Weights_n, Z_n)^{(i)}
		\qquad i=1,\dots,\dimension.
	\end{align*}
	If we were to divide the sum of squared gradients \(G_n^{(i)}\) by the number
	of summands, we would get an estimator for the second moment. This second
	moment consists not only of the average squares of the actual loss function,
	but also of a variance estimator. This means that high variance can also
	reduce the learning rate. This is an additional benefit, as we have found
	when looking at SGD, that we need to reduce the learning rate when the
	variance starts to dominate.

	\item[RMSProp]\parencite[lecture 6e]{hintonNeuralNetworksMachine2012} As the sum of squares increases monotonically, we can see 
	in (\ref{eq: adagrad motivation contraction}) that this quickly reduces
	the learning rate of AdaGrad in every step. So RMSProp actually uses an
	estimator for the second moments, i.e. divides \(G_n^{(i)}\) by \(n\). Well,
	that is a bit of a lie. To avoid calculations it uses a recursive exponential
	average
	\begin{align*}
		\hat{\E}[(\nabla\loss(\Weights_n)^{(i)})^2]
		:= \gamma \hat{\E}[(\nabla\loss(\Weights_{n-1})^{(i)})^2]
		+ (1-\gamma)(\nabla\loss(\Weights_n, Z_n)^{(i)})^2.
	\end{align*}
	Another benefit of this exponential average is, that we weigh more recent
	gradients higher. As the Second Taylor Approximation is only locally
	accurate, so it is probably a good idea to discount older gradients further
	away. Anyway, the algorithm ``RMSProp'' is defined as
	\begin{align*}
		\Weights_{n+1}^{(i)}
		&= \Weights_n^{(i)}
		- \frac{\lr_n}{\text{RMS}(\nabla\loss)_n}\nabla\loss(\Weights_n, Z_n)^{(i)}
		\qquad i=1,\dots,\dimension,
	\end{align*}
	where we denote the Root Mean Square
	\begin{align*}
		\text{RMS}(\nabla\loss)_n := \sqrt{\hat{\E}[\nabla\loss(\Weights_n)^{(i)}]^2+\epsilon}.
	\end{align*}

	\item[AdaDelta] \parencite{zeilerADADELTAAdaptiveLearning2012} The motivation
	for the independently developed AdaDelta starts exactly like RMSProp, but
	there is one additional step.
	``When considering the parameter updates, [\(\Delta \weights\)], being applied to
	[\(\weights\)], the units should match. That is, if the parameter had some
	hypothetical units, the changes to the parameter should be changes in those
	units as well. When considering SGD, Momentum, or ADAGRAD, we can see that
	this is not the case'' \parencite[p. 3]{zeilerADADELTAAdaptiveLearning2012}.
	They also note that the \ref{eq: newton minimum approx} does have matching
	units. So they consider the case where the Hessian is diagonal (the case we
	have been considering where the eigenvectors are the standard basis), and
	somehow deduce that one should use a similar estimator
	\(\text{RMS}(\Delta\Weights)\) for \(\sqrt{\E[\Delta\Weights^2]}\) as for
	\(\sqrt{\E[\nabla\loss^2]}\) to obtain ``AdaDelta''
	\begin{align*}
		\Weights_{n+1}^{(i)} = \Weights_n^{(i)}
		-\lr_n\frac{\text{RMS}(\Delta\Weights)_{n-1}}{\text{RMS}(\nabla\loss)_n}
		\nabla\loss(\Weights_n, Z_n)^{(i)}.
	\end{align*}

	\item[Adam] \parencite{kingmaAdamMethodStochastic2017} applies the same idea
	as RMSProp to the momentum update, i.e.
	\begin{align*}
		\Weights_{n+1} = \Weights_n + \frac{\lr_n}{\text{RMS}(\nabla\loss)_n} \momentum_{n+1}
	\end{align*}
	where we recall that momentum was defined as
	\begin{align*}
		\momentum_{n+1} = \underbrace{(1-\friction \lr_n)}_{\momCoeff_n} \momentum_n - \lr_n\nabla\Loss(\Weights_n).
	\end{align*}
	Additional to this idea, the paper also introduces the idea of a correction term
	as we have to initialize our exponential averages of the momentum term \(\momentum_n\)
	and \(\text{RMS}(\nabla\loss)_n\). Since these initializations usually are zero
	we have a bias towards zero, which Adam tries to compensate by multiplying
	the term \(\momentum_n\) with \(\frac{1}{1-\momCoeff^n}\) assuming a constant
	momentum coefficient, and similarly for \(\text{RMS}(\nabla\loss)_n\). Then
	this modified momentum and second moment estimator of the gradients are
	used for the update instead.


	\item[NAdam] \parencite{dozatIncorporatingNesterovMomentum2016}
	uses the same idea as Adam just with Nesterov momentum instead of heavy ball
	momentum.

	\item[AdaMax] \parencite{kingmaAdamMethodStochastic2017} In the same paper
	as Adam, the authors also note that the Root Mean Squared
	\(\text{RMS}(\nabla\loss)\) are basically a \(2\)-norm and one could use a
	different norm for the gradients as well.  Recall that we essentially want
	the absolute eigenvalue of the Hessian in the end. For that, squaring
	the gradient components works just as well as taking the absolute value (i.e.
	using the maximum norm). This idea results in the ``AdaMax'' algorithm.

	\item[AMSGrad] \parencite{reddiConvergenceAdam2019} While we have argued that
	the exponential moving average used for second moment estimation is a good
	thing, \textcite{reddiConvergenceAdam2019} find, that in the cases where
	these methods do not converge it is due to some seldomly occurring directions
	getting discounted too fast. So they suggest using a running maximum instead.
\end{description}

Since all these optimizers make component-wise adjustments which could be written
as a diagonal matrix, Assumption~\ref{assmpt: parameter in generalized linear
hull of gradients} still applies. Which means our complexity bounds from
Section~\ref{sec: complexity bounds} still apply. Therefore these algorithms
can not be an improvement over standard momentum in general as Nesterov's
momentum is already asymptotically optimal.

And the theoretical justification for these algorithms is quite thin. There
are some fantastic animation by \textcite{radfordVisualizingOptimizationAlgos2014}
which heavily imply that these ``adaptive algorithms'' are better at breaking
symmetry. But these toy problems are generally axis aligned which means their
eigenspaces will be the eigenvectors. And at that point we know that these
algorithms should behave like Newton's algorithm. So it is no surprise that they
perform really well.

On the other hand one can find examples where these adaptive methods become very
unstable
\parencite[e.g.][]{wilsonMarginalValueAdaptive2018,reddiConvergenceAdam2019}.
But these examples might be unrealistic as well.

In empirical benchmarks such as \textcite{schmidtDescendingCrowdedValley2021} no
optimizer seems to outperform the others consistently.

\section{Geometric Methods}

\subsection{Cutting Plane Methods}

Notice how the \(\dimension-1\) dimensional hyperplane
\begin{align*}
	\mathcal{H}
	:= \{\theta : \langle \nabla\Loss(\weights), \theta - \weights \rangle = 0\}
\end{align*}
separates the set in which direction the loss is increasing
\begin{align*}
	\mathcal{H}_+
	:= \{\theta : \langle \nabla\Loss(\weights), \theta - \weights \rangle > 0\},
\end{align*}
from the set in which direction the loss is decreasing \(\mathcal{H}_-\). While
this is only a local statement, convexity implies that \(\Loss(\weights)\) is
smaller than the loss at any \(\theta\in\mathcal{H}_+\) because
\begin{align*}
	\Loss(\theta)
	\ge \Loss(\weights) + \underbrace{\langle \nabla\Loss(\weights), \theta -\weights\rangle}_{>0}
	> \Loss(\weights)
\end{align*}
Therefore we know the minimum can not be in \(\mathcal{H}_+\), which allows us
to cut away this entire set from consideration.

Cutting plane methods use this fact to iteratively shrink the considered set
until the minimum is found. As convexity is vital here, it is unlikely they
might ever work for non-convex loss functions.

\subsubsection{Center of Gravity Method}

This method simply evaluates the gradient at the center of gravity of the
remaining set in the hope that the cutting plane method cuts away roughly half
of the remaining set. Therefore convergence speed can be shown to be
linear\footnote{i.e. exponential in non numerical-analysis jargon} 
\parencite[e.g.][Theorem 2.1]{bubeckConvexOptimizationAlgorithms2015}.
Unfortunately this requires calculating the center of gravity of an arbitrary
set which is its own computational problem. 

\subsubsection{Ellipsoid Method}

To work around the center of gravity problem, the ellipsoid methods starts out
with an ellipsoid set, makes one cut through the center, and finds a new
ellipsoid around the remaining set. This new ellipsoid is larger than the
set we would actually have to consider, but at least finding its center is
tractable. The convergence speed is still linear \parencite[e.g][Theorem
2.4]{bubeckConvexOptimizationAlgorithms2015}.


\subsubsection{Vaidya's Cutting Plane Method}

This method uses polytopes instead of ellipsoids as base objects to make the
center of gravity search tractable, and seems to be more efficient
computationally \parencite[e.g.][Section
2.3]{bubeckConvexOptimizationAlgorithms2015}.

\subsection{Geometric Descent}

\textcite{bubeckGeometricAlternativeNesterov2015} realized that strong convexity
\begin{align*}
	\Loss(\theta)
	\ge \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights\rangle
	+ \tfrac{\lbound}{2}\|\theta-\weights\|^2
\end{align*}
is equivalent to
\begin{align*}
	\tfrac{\lbound}{2}\|\theta - (\weights - \tfrac1\lbound \nabla\Loss(\weights))\|^2
	\le \frac{\|\nabla\Loss(\weights)\|^2}{2\lbound} - (\Loss(\weights)-\Loss(\theta)),
\end{align*}
which implies that for \(\theta=\minimum\) we know that the minimum
\(\minimum\) is inside the ball
\begin{align*}
	\minimum\in B\left(\weights- \tfrac1\lbound\nabla\Loss(\weights), 
		\frac{\|\nabla\Loss(\weights)\|^2}{\lbound^2}
		- \frac{ \Loss(\weights)-\Loss(\minimum) }{\lbound}
	\right).
\end{align*}
With Lipschitz continuity of the gradient \textcite{bubeckGeometricAlternativeNesterov2015}
further refine this ``confidence ball'' around the minimum and use it to create
an algorithm with the same rate of convergence as Nesterov's momentum.
Unfortunately this looks like the algorithm is much more dependent on
convexity than the momentum method.


\section{(Quasi) Newton Methods}

\subsection{Cubic Regularization of Newton's Method}

We have motivated the Newton-Raphson method on quadratic functions where
convergence is instant. Convergence analysis for general loss functions requires
an analysis of the way the Hessian changes. In Section~\ref{sec:
lipschitz continuity of the Gradient} we used Lipschitz continuity of the
gradient to get an upper bound on our Loss (\ref{eq: lin approx + distance
penalty notion})
\begin{align*}
	\Loss(\theta)
	\le \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac{\ubound}2 \|\theta-\weights\|^2,
\end{align*}
which we then optimized over in Lemma~\ref{lem: smallest upper bound} to find
that gradient descent with learning rate \(\tfrac1\ubound\) is optimal with
regards to this bound. The Lipschitz constant \(\ubound\) bounds the second
derivative and thus the rate of change of the gradient. This allows us to
formulate for how long we can trust our gradient evaluation in some sense.

In the case of the Newton-Raphson method, we need a similar rate of change
bound on the Hessian. In this case we want to bound the rate of change of the
second derivative so we assume a Lipschitz continuous second derivative (or 
in other words bounded third derivative). This similarly results in the upper
bound
\begin{align*}
	\Loss(\theta)
	\le \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac12\langle \nabla^2\Loss(\weights)(\theta-\weights), \theta-\weights \rangle
	+ \tfrac{M}6\|\theta-\weights\|^3.
\end{align*}
The solution of which represents the cubic regularization of Newton's method
\parencite[Section 4.1]{nesterovLecturesConvexOptimization2018}.
The solution of which can unfortunately not be written explicitly.

Since this method is history agnostic again, Nesterov utilizes estimating sequences
again, to derive an ``accelerated cubic Newton scheme'' \parencite[Section
4.2]{nesterovLecturesConvexOptimization2018} similar to Nesterov momentum, i.e.
the accelerated gradient descent.

\subsection{Quasi Newton Methods}

While the analysis and regularization of Newton's method itself is important,
it can not be used directly as the calculation and storage of the Hessian is
usually too expensive at cost \(O(\dimension^2)\), and if that is not the case,
then inversion of this matrix at cost \(O(\dimension^3)\) almost certainly is,
for most machine learning applications. Additionally we have saddle point
repelling properties on our wish list.



\subsubsection{L-BFGS}

\subsubsection{Krylov Subspace Descent}

\textcite{vinyalsKrylovSubspaceDescent2012}

Turned into Saddle Point repelling methods by \textcite{dauphinIdentifyingAttackingSaddle2014}

\fxnote{Generalized Minimal Residual method?}

\subsection{Conjugate Gradient Descent}

Similar to momentum, conjugate gradient descent is another attempt to solve
high condition problems. The best introduction I found so far was
\textcite{shewchukIntroductionConjugateGradient1994}. But here is another
attempt.

Recall that we can write
our loss function as an approximate paraboloid (cf. (\ref{eq: paraboloid
approximation of L}))
\begin{align*}
	\Loss(\theta)
	= (\theta-\weights)^T \nabla^2\Loss(\weights)(\theta-\weights)
	+ c(\weights) + o(\|\theta-\weights\|^2)
\end{align*}
If the second taylor approximation is accurate and we have a constant
positive definite Hessian \(H:=\nabla^2\Loss(\weights)\), then our loss can also
be written as
\begin{align*}
	\Loss(\weights) = \Loss(\minimum) + 
	(\weights-\minimum)^T H(\weights-\minimum)
	= \Loss(\minimum) + \|\weights-\minimum\|_H^2,
\end{align*}
where the norm \(\|\cdot\|_H\) is induced by the scalar product
\begin{align*}
	\langle x, y \rangle_H := \langle x, Hy\rangle.
\end{align*}
In the space \((\reals^d, \|\cdot\|_H)\) our problem therefore has condition one
(\(\condition=1\))!

\subsubsection{Excursion: The Gradient in \((\reals^\dimension, \|\cdot\|_H)\)}

We have made our problem much simpler but the space we are considering much
more complicated. The question is now, what methods we can realistically use
in this new space?

Naively one might wish to take the gradient with respect to this space. As this
gradient would point straight towards the minimum because we are in the
\(\condition=1\) case. This means, that this gradient should be just as
powerful as the Newton-Raphson method, which would be great. So let us try to
obtain this gradient. From the definition of the gradient we have
\begin{align}
	\label{eq: gradient definition equation}
	0\xeq{!}&\lim_{v\to 0}\frac{
		|\Loss(\weights + v) - \Loss(\weights) - \langle \nabla_H\Loss(\weights), v\rangle_H |
	}{\|v\|_H}\\
	\nonumber
	&=\lim_{v\to 0}\frac{
		|\Loss(\weights + v) - \Loss(\weights) -  \nabla_H\Loss(\weights)^T H v |
	}{\|v\|}
	\frac{\|v\|}{\|v\|_H}\\
	\nonumber
	&=\lim_{v\to 0}\frac{
		|\Loss(\weights + v) - \Loss(\weights) -  \langle H\nabla_H\Loss(\weights), v\rangle |
	}{\|v\|}
	\frac{\|v\|}{\|v\|_H}
\end{align}
Where we have used symmetry of the Hessian \(H\) for the last equation. Now if
we had
\begin{align*}
	H \nabla_H \Loss(\weights) = \nabla\Loss(\weights),
\end{align*}
then the first fraction would go to zero by the definition of the gradient.
While the second fraction would be bounded due to norm equivalence, since all norms
are equivalent in final dimensional real vector spaces. Therefore
\begin{align*}
	\nabla_H \Loss(\weights) := H^{-1}\nabla\Loss(\weights)
\end{align*}
is a (and due to uniqueness the) solution of (\ref{eq: gradient definition
equation}). So we have found a method equivalent to Newton's method.
Unfortunately it \emph{is} Newtons's method. And since we still do not want
to invert the Hessian, we have to think of something else. Fortunately, that
something else only has to work for \(\condition=1\).

\subsubsection{Conjugate Directions}

Let us take any (normed) direction \(d^{(1)}\) and complete it to an H-orthogonal
basis \(d^{(1)},\dots,d^{(\dimension)}\). H-orthogonal is also known as
``conjugate''. Then we can represent any vector \(\theta\) as a linear
combination of this conjugate basis
\begin{align*}
	\theta = \sum_{k=1}^\dimension \theta^{(k)} d^{(k)}.
\end{align*}
Denoting \(\Delta\weights_n := \weights_n -\minimum\), let us now optimize over
the learning rate for a move in direction \(d^{(1)}\)
\begin{align*}
	\Delta\weights_{1} = \Delta\weights_0 + \lr_0 d^{(1)}
	= (\Delta\weights^{(1)} + \lr_0)d^{(1)}
	+ \sum_{k=2}^\dimension \Delta\weights^{(k)} d^{(k)}.
\end{align*}
Due to conjugacy we have
\begin{align*}
	\Loss(\weights_{1})
	= \Loss(\minimum) + \|\Delta\weights_{1}\|_H^2
	= \Loss(\minimum) + \sum_{k=1}^{\dimension} (\Delta\weights_{1}^{(k)})^2\|d^{(k)}\|^2.
\end{align*}
And since most of the \(\Delta\weights^{(k)}_{1}\) stay constant no matter the
learning rate \(\lr_n\), the only one we actually optimize over is
\((\Delta\weights^{(1)}_{1})^2\).
And we can make that coordinate zero with \(\lr_0 :=-\Delta\weights_0^{(1)}\).

So if we do a line search (optimize the loss over one direction, i.e. \(d^{(1)}\)),
then we know that we will never ever have to move in the direction \(d^{(1)}\)
again. The target \(\minimum\) lies H-orthogonal to \(d^{(1)}\)\fxwarning{
	picture why this goes wrong for conditions other than one
}. Now if we
optimize over the next direction the same holds true. In fact we get the globally
optimal solution in the span of search directions using this method 
\begin{align*}
	\Delta\weights_n &= \sum_{k=n+1}^\dimension\Delta\weights^{(k)} d^{(k)}\\
	&= \arg\min \{
		\Loss(\weights) : \weights \in \weights_0 + \linSpan[d^{(1)},\dots, d^{(n)}]
	\} - \minimum.
\end{align*}
In this light, recall that we provided complexity bounds for weights in the
linear span of gradients shifted by \(\weights_0\) in Section~\ref{sec: complexity bounds}
(cf. Assumption~\ref{assmpt: parameter in linear hull of gradients}). So if our
search directions were the gradients, we would obtain optimal convergence for
this class of methods.

\subsubsection{Simplifying Gram-Schmidt}

Unfortunately our gradients are not conjugate in general. But that is no
trouble, we can just use Gram-Schmidt conjugation. Right?

So here is the problem: We need knowledge of the hessian \(H\) for this
procedure and have to remove the components from all previous directions. This
blows up time complexity making this procedure not worthwhile. But as it turns
out, the gradients do have some additional structure we can exploit to remove
these issues.

First let us summarize what we have so far. We want to obtain the same conjugate
search directions \((d^{(1)},\dots,d^{(\dimension)})\) from
\((\nabla\Loss(\weights_0),\dots,\nabla\Loss(\weights_{\dimension-1}))\)
we would get if we used Gram-Schmidt conjugation, which also implies
\begin{align*}
	\mathcal{D}_k := \linSpan\{\nabla\Loss(\weights_0),\dots,\nabla\Loss(\weights_k)\}
	= \linSpan\{d^{(1)},\dots,d^{(k+1)}\}.
\end{align*}
And we select our weights with line searches along the search directions
\begin{align*}
	\lr_n := \arg\min_\lr \Loss(\weights_n - \lr d^{(n+1)})
\end{align*}
resulting in
\begin{align*}
	\weights_{n+1} = \weights_n - \lr_n d^{(n+1)}
	= \arg\min\{ \Loss(\weights) : \weights\in\weights_0 + \mathcal{D}_n\}.
\end{align*}
This last condition implies that the gradient \(\nabla\Loss(\weights_n)\)
must necessarily be orthogonal to \(\mathcal{D}_{n-1}\) (unless we have already
converged to \(\minimum\)). Otherwise \(\weights_n\) would not be optimal. This
orthogonality also allows us to argue that \(\mathcal{D}_k\) actually has rank
\(k+1\) and its respective generators are basis, which was necessary for the
conjugate directions argument.

So if we could somehow translate this euclidean orthogonality into a
H-orthogonality we would be done. And here comes in one crucial insight:
\(\mathcal{D}_n\) is a ``Krylov subspace'', i.e. can be generated by repeated
application of an operator. More specifically we have
\begin{lemma}[\(\mathcal{D}_n\) is \(H\) Krylov subspace]
	\begin{align*}
		\mathcal{D}_n = \{H^1 \Delta\weights_0, \dots, H^{n+1}\Delta\weights_0\}.
	\end{align*}
\end{lemma}
\begin{proof}
	The induction start \(n=0\) is simply due to \(\nabla\Loss(\weights_0) =
	H\Delta\weights_0\) and the induction step \(n-1\to n\) starts with
	\begin{align}
		\nonumber
		\nabla\Loss(\weights_n)= H\Delta\weights_n
		&= H(\Delta\weights_{n-1} - \lr_{n-1} d^{(n)})\\
		\label{eq: represent nabla Loss(weights_n) with prev gradient and d(n)}
		&= \underbrace{\nabla\Loss(\weights_{n-1})}_{\in \mathcal{D}_{n-1}} - \lr_{n-1} H
		\underbrace{d^{(n)}}_{\in \mathcal{D}_{n-1}}.
	\end{align}
	To finish this step, we apply the induction assumption and notice that the
	second summand might now contain the vector \(H^{n+1}\Delta\weights_0\) if the
	coefficient of \(H^{n}\Delta\weights_0\) is not zero. Therefore
	\(\mathcal{D}_{n}\) is contained in the Krylov subspace. And due to the
	number of elements in the generator of the Krylov subspace it can not have
	higher rank than \(n+1\), and thus cannot be larger.
\end{proof}
This is useful, because it implies that \(H\mathcal{D}_{n-2}\) is contained
in \(\mathcal{D}_{n-1}\) to which \(\nabla\Loss(\weights_n)\) is orthogonal.
Therefore \(\nabla\Loss(\weights_n)\) is already \(H\)-orthogonal to
\(\mathcal{D}_{n-2}\) from the get go. In other words we already have
\begin{align*}
	\langle \nabla\Loss(\weights_n), d^{(k)} \rangle_H = 0 
	\qquad \forall k\le n-1.
\end{align*}
To construct a new conjugate direction \(d^{(n+1)}\) we therefore only have to
remove the previous direction \(d^{(n)}\) component from our gradient! So we
want to select
\begin{align}\label{eq: next search direction}
	d^{(n+1)} = \nabla\Loss(\weights_n)
	- \frac{\langle \nabla\Loss(\weights_n), d^{(n)}\rangle_H}{\|d^{(n)}\|_H^2}d^{(n)}.
\end{align}

\subsubsection{Evaluating the \(H\)-Dot-Product}

Now, we ``just'' have to find a way to evaluate this factor without actually
knowing the Hessian \(H\). Using the representation (\ref{eq: represent nabla Loss(weights_n) with
prev gradient and d(n)}) of \(\nabla\Loss(\weights_n)\), we get
\begin{align*}
	\langle \nabla\Loss(\weights_n), \nabla\Loss(\weights_n) \rangle
	= \langle \nabla\Loss(\weights_n), \nabla\Loss(\weights_{n-1}) \rangle
	- \lr_{n-1}\langle \nabla\Loss(\weights_n), H d^{(n)}\rangle.
\end{align*}
Using the definition of \(\langle\cdot,\cdot\rangle_H\) and reordering results in
\begin{align}
	\nonumber
	\langle \nabla\Loss(\weights_n), d^{(n)}\rangle_H
	&= \tfrac1{\lr_{n-1}}\left[
		\langle \nabla\Loss(\weights_n), \nabla\Loss(\weights_{n-1}) \rangle
		- \langle \nabla\Loss(\weights_n), \nabla\Loss(\weights_n) \rangle
	\right]\\
	\label{eq: evaluated H dot product}
	&= \frac{- \|\nabla\Loss(\weights_n)\|^2}{\lr_{n-1}},
\end{align}
where we have used the orthogonality of \(\nabla\Loss(\weights_n)\) to
\(\nabla\Loss(\weights_{n-1})\in\mathcal{D}_{n-1}\).
Using the optimality of the learning rate \(\lr_{n-1}\)
\begin{align*}
	0\xeq{!}\frac{d}{d\lr}\Loss(\weights_{n-1} - \lr d^{(n)})
	&= -\langle \nabla\Loss(\weights_{n-1} - \lr d^{(n)}), d^{(n)} \rangle\\
	&= -\langle H(\Delta\weights_{n-1} -\lr d^{(n)}), d^{(n)} \rangle\\
	&= \lr\|d^{(n)}\|_H^2 - \langle \nabla\Loss(\weights_{n-1}), d^{(n)}\rangle,
\end{align*}
we get
\begin{align}\label{eq: optimal learning rate CG}
	\lr_{n-1}
	= - \frac{\langle \nabla\Loss(\weights_{n-1}), d^{(n)}\rangle}{\|d^{(n)}\|_H^2}.
\end{align}
Using (\ref{eq: evaluated H dot product}) and (\ref{eq: optimal learning rate CG})
we can replace the Gram-Schmidt constant from (\ref{eq: next search direction})
with things we can actually calculate resulting in
\begin{align*}
	d^{(n+1)} = \nabla\Loss(\weights_n)
	- \frac{
		\|\nabla\Loss(\weights_n)\|^2
	}{\langle \nabla\Loss(\weights_{n-1}), d^{(n)}\rangle}d^{(n)}
\end{align*}

\fxnote{relation to momentum}

\subsubsection{Hessian Free Gradient Descent}
\textcite{martensDeepLearningHessianfree2010}


\subsection{Gauß-Newton Algorithm}

The (classical) Gauß-Newton Algorithm variant of the Newton-Raphson method
specialized for loss functions which are sums of squared residuals. In other
words loss functions of the form
\begin{align*}
	\Loss(\weights) = \sum_{k=1}^n r_k(\weights)^2.
\end{align*}
The generalized Gauß-Newton Algorithm 
\textcite{bottouOptimizationMethodsLargeScale2018}
\fxnote{Levenberg Marquard algorithm?}



\section{Natural Gradient Descent}

``Natural gradient descent'' would require a deep dive into ``information geometry''
for which we do not have time. But not mentioning this algorithm at all would do it
injustice. So to avoid playing a game of telephone, here is a direct quote of
the description by \textcite{bottouOptimizationMethodsLargeScale2018}:

``We have seen that Newton's method is invariant to linear transformations of
the parameter vector w. By contrast, the natural gradient method [5, 6] aims to
be invariant with respect to all differentiable and invertible transformations.
The essential idea consists of formulating the gradient descent algorithm in the
space of prediction functions rather than specific parameters. Of course, the
actual computation takes place with respect to the parameters, but accounts for
the anisotropic relation between the parameters and the decision function. That
is, in parameter space, the natural gradient algorithm will move the parameters
more quickly along directions that have a small impact on the decision function,
and more cautiously along directions that have a large impact on the decision
function.

We remark at the outset that many authors [119, 99] propose
quasi-natural-gradient methods that are strikingly similar to the quasi-Newton
methods described in §6.2. The natural gradient approach therefore offers a
different justification for these algorithms, one that involves qualitatively
different approximations. It should also be noted that research on the design of
methods inspired by the natural gradient is ongoing and may lead to markedly
different algorithms.''

\section{Coordinate Descent}

\fxnote{elaborate or cut}
\textcite[7.3]{bottouOptimizationMethodsLargeScale2018}, \textcite[6.4]{bubeckConvexOptimizationAlgorithms2015}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
