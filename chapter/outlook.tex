% !TEX root = ../Masterthesis.tex
\chapter{Outlook}

\section{Second Order Methods}

\section{Heuristics}

There are a number of widely used heuristics which are provided in the
default optimizer selection of machine learning frameworks such as Tensorflow.

As we have only covered \ref{eq: gradient descent} and momentum so far, we have
quite a bit of ground to cover. But at the same time most of the remaining
common optimizer \parencite[as reviewed by e.g.][]{ruderOverviewGradientDescent2017}
can be motivated in a similar way, which might or might not be new, as I did not
properly search previous work for this idea.

Recall that the \ref{eq: newton minimum approx}
\begin{align*}
	\weights_{n+1}	= \weights_n - (\nabla^2\Loss(\weights_n))^{-1}\nabla\Loss(\weights_n)
\end{align*}
finds the vertex of a quadratic loss function \(\Loss\) immediately, because we
have
\begin{align*}
	\nabla\Loss(\weights_n) = \nabla^2\Loss(\weights_n)[\weights_n - \hat{\weights}_n],
\end{align*}
where \(\hat{\weights}_n\) was the vertex of the second Taylor approximation.
To better understand how that works, note that for
\begin{align*}
	\nabla^2\Loss(\weights_n) = V \diag(\hesseEV_1,\dots,\hesseEV_\dimension)V^T
\end{align*}
we have
\begin{align*}
	\nabla^2\Loss(\weights_n)^{-1}
	= V \diag(\tfrac1\hesseEV_1,\dots,\tfrac1\hesseEV_\dimension)V^T.
\end{align*}
To simplify things, let us assume a quadratic loss function
\(H=\nabla^2\Loss(\weights_n)\) now.  In this case we found that every
eigenspace scales independently
\begin{align*}
	\langle \weights_{n+1} - \minimum, v_i \rangle
	\le (1-\lr \hesseEV_i)\langle \weights_n - \minimum, v_i \rangle
\end{align*}
So the \ref{eq: newton minimum approx} rescales the steps by dividing every
eigenspace through its respective eigenvalue. For positive eigenvalues is all
nice and dandy. But for negative eigenvalues, which represent a maximum in their
eigenspace, the \ref{eq: newton minimum approx} multiplies by a negative value
\(\tfrac1{\hesseEV_i}\) and therefore causes movement towards this eigenvalue.

Okay, now that we remembered these crucial problems, let us make one egregious
assumption. Let us assume that the standard basis are already the eigenvectors,
i.e. \(V=\identity\). Then the components of the gradient of our loss function
are proportional to their respective eigenvalues
\begin{align*}
	\nabla\Loss(\weights_n)^{(i)}
	= \hesseEV_i [\weights_n - \minimum]^{(i)}.
\end{align*}
If we define
\begin{align*}
	G_n^{(i)} = \sum_{k=0}^n [\nabla\Loss(\weights_n)^{(i)}]^2 \qquad i=1,\dots,\dimension,
\end{align*}
then the algorithm
\begin{align}
	\label{eq: almost adagrad}
	\weights_{n+1}^{(i)}
	&= \weights_n^{(i)} - \frac{\lr_n}{\sqrt{G_n^{(i)}}}\nabla\Loss(\weights_n)^{(i)}\\
	\nonumber
	&= \weights_n^{(i)} - \frac{\lr_n}{
		\sqrt{\cancel{\hesseEV_i^2}\sum_{k=0}^n([\weights_k - \minimum]^{(i)})^2}
	}
	\cancel{\hesseEV_i}[\weights_n - \minimum]^{(i)}
\end{align}
also cancels out the eigenvalues. But as we divide through the absolute value of
the eigenvalues in contrast to the \ref{eq: newton minimum approx}, we get to
keep the sign
\begin{align}\label{eq: adagrad motivation contraction}
	[\weights_{n+1}-\minimum]^{(i)} = \left(1-\frac{\lr_n\sign(\hesseEV_i)}{
			\sqrt{\sum_{k=0}^n([\weights_k - \minimum]^{(i)})^2}
		}\right)[\weights_n - \minimum]^{(i)}.
\end{align}
Of course the notion that the standard basis vectors are already the eigenvectors
is pretty ridiculous. But for some reason the following algorithms seem to
work. And they are basically based on this notion of reweighting the components
by dividing them through the squared past gradients in some sense. The intuition
provided by \textcite{ruderOverviewGradientDescent2017} is, that
``It adapts the learning rate to the parameters, performing smaller updates
(i.e. low learning rates) for parameters associated with frequently occurring
features, and larger updates (i.e. high learning rates) for parameters
associated with infrequent features.''
But I am not sure if I buy that.


\begin{description}
	\item[AdaGrad] \parencite{duchiAdaptiveSubgradientMethods2011}
	We have basically already motivated AdaGrad in (\ref{eq: almost adagrad}).
	We are going to use the stochastic gradients though
	\begin{align*}
		G_n^{(i)} := \sum_{k=0}^n [\nabla\loss(\weights_n, Z_n)^{(i)}]^2 \qquad i=1,\dots,\dimension,
	\end{align*}
	and introduce a small \parencite[i.e. \(\approx 10^{-8}\)][]{ruderOverviewGradientDescent2017}
	term \(\epsilon\) to avoid numerical instability due to dividing through zero.
	This result in the ``AdaGrad'' algorithm
	\begin{align*}
		\Weights_{n+1}^{(i)}
		&= \Weights_n^{(i)}
		- \frac{\lr_n}{\sqrt{G_n^{(i)}+\epsilon}}\nabla\loss(\Weights_n, Z_n)^{(i)}
		\qquad i=1,\dots,\dimension.
	\end{align*}
	If we were to divide the sum of squared gradients \(G_n^{(i)}\) by the number
	of summands, we would get an estimator for the second moment. This second
	moment consists not only of the average squares of the actual loss function,
	but also of a variance estimator. This means that high variance can also
	reduce the learning rate. This is an additional benefit, as we have found
	when looking at SGD, that we need to reduce the learning rate when the
	variance starts to dominate.

	\item[RMSProp]\parencite[lecture 6e]{hintonNeuralNetworksMachine2012} Now as the sum of squares increases monotonically, we can see 
	in (\ref{eq: adagrad motivation contraction}) that this essentially reduces
	the learning rate of AdaGrad in every step. So RMSProp actually uses an
	estimator for the second moments, i.e. divides \(G_n^{(i)}\) by \(n\). Well,
	that is a bit of a lie. To avoid calculations it uses a recursive exponential
	average
	\begin{align*}
		\hat{\E}[(\nabla\loss(\Weights_n)^{(i)})^2]
		:= \gamma \hat{\E}[(\nabla\loss(\Weights_{n-1})^{(i)})^2]
		+ (1-\gamma)(\nabla\loss(\Weights_n, Z_n)^{(i)})^2.
	\end{align*}
	Another benefit of this exponential average is, that we weigh more recent
	gradients higher. If we do not work on a quadratic function, as we assumed in
	our motivation, the motivation only works locally. So it is probably a good
	idea to discount older gradients further away. Anyway, the algorithm ``RMSProp''
	is defined as
	\begin{align*}
		\Weights_{n+1}^{(i)}
		&= \Weights_n^{(i)}
		- \frac{\lr_n}{\text{RMS}(\nabla\loss)_n}\nabla\loss(\Weights_n, Z_n)^{(i)}
		\qquad i=1,\dots,\dimension,
	\end{align*}
	where we denote
	\begin{align*}
		\text{RMS}(\nabla\loss)_n := \sqrt{\hat{\E}[\nabla\loss(\Weights_n)^{(i)}]^2+\epsilon}.
	\end{align*}

	\item[AdaDelta] \parencite{zeilerADADELTAAdaptiveLearning2012} The motivation
	for the independently developed AdaDelta starts exactly like RMSProp, but
	there is one additional step.
	``When considering the parameter updates, [\(\Delta \weights\)], being applied to
	[\(\weights\)], the units should match. That is, if the parameter had some
	hypothetical units, the changes to the parameter should be changes in those
	units as well. When considering SGD, Momentum, or ADAGRAD, we can see that
	this is not the case'' \parencite[p. 3]{zeilerADADELTAAdaptiveLearning2012}.
	They also note that the \ref{eq: newton minimum approx} does have matching
	units. So they consider the case where the Hessian is diagonal (the case we
	have been considering where the eigenvectors are the standard basis), and
	somehow deduce that one should use a similar estimator
	\(\text{RMS}(\Delta\Weights)\) for \(\sqrt{\E[\Delta\Weights^2]}\) as for
	\(\sqrt{\E[\nabla\loss^2]}\) to obtain ``AdaDelta''
	\begin{align*}
		\Weights_{n+1}^{(i)} = \Weights_n^{(i)}
		-\lr_n\frac{\text{RMS}(\Delta\Weights)_{n-1}}{\text{RMS}(\nabla\loss)_n}
		\nabla\loss(\Weights_n, Z_n)^{(i)}.
	\end{align*}

	\item[Adam] \parencite{kingmaAdamMethodStochastic2017} applies the same idea
	as RMSProp to the momentum update, i.e.
	\begin{align*}
		\Weights_{n+1} = \Weights_n + \frac{\lr_n}{\text{RMS}(\nabla\loss)_n} \momentum_{n+1}
	\end{align*}
	where we recall that momentum was defined as
	\begin{align*}
		\momentum_{n+1} = \underbrace{(1-\friction \lr_n)}_{\momCoeff_n} \momentum_n - \lr_n\nabla\Loss(\Weights_n).
	\end{align*}
	Additional to this idea, the paper also introduces the idea of a correction term
	as we have to initialize our exponential averages of the momentum term \(\momentum_n\)
	and \(\text{RMS}(\nabla\loss)_n\). Since these initializations usually are zero
	we have a bias towards zero, which Adam tries to compensate by multiplying
	the term \(\momentum_n\) with \(\frac{1}{1-\momCoeff^n}\) assuming a constant
	momentum coefficient, and similarly for \(\text{RMS}(\nabla\loss)_n\). Then
	this modified momentum and second moment estimator of the gradients are
	used for the update instead.


	\item[NAdam] \parencite{dozatIncorporatingNesterovMomentum2016}
	uses the same idea as Adam just with Nesterov momentum instead of heavy ball
	momentum.

	\item[AdaMax] \parencite{kingmaAdamMethodStochastic2017} In the same paper
	as Adam, the authors also note that \(\text{RMS}(\nabla\loss)\) is basically
	a \(2\)-norm and one could use a different norm for the gradients as well.
	Recall that we essentially want the absolute eigenvalue of the hesse matrix
	in the end. For that, squaring the gradient components works just as well as
	taking the absolute value (i.e. using the maximum norm). This idea results in
	the ``AdaMax'' algorithm.

	\item[AMSGrad] 
\end{description}

Since all these optimizers make component-wise adjustments which could be written
as a diagonal matrix, Assumption~\ref{assmpt: parameter in generalized linear
hull of gradients} still applies. Which means our complexity bounds from
Section~\ref{sec: complexity bounds} still apply. Therefore these algorithms
can not be an improvement over standard momentum in general as Nesterov's
momentum is already asymptotically optimal.

And the theoretical justification for these algorithms is quite thin. There
are some fantastic animation by \textcite{radfordVisualizingOptimizationAlgos2014}
which heavily imply that these ``adaptive algorithms'' are better at breaking
symmetry. But these toy problems are generally axis aligned which means their
eigenspaces will be the eigenvectors. And at that point we know that these
algorithms should behave like Newton's algorithm. So it is no surprise that they
perform really well.

\textcite{wilsonMarginalValueAdaptive2018} on the other hand find examples where
these adaptive methods become very unstable. But these examples might be
unrealistic as well.

In empirical benchmarks such as \textcite{schmidtDescendingCrowdedValley2021} no
optimizer seems to outperform the others consistently.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
