% !TEX root = ../Masterthesis.tex
\chapter{Other Methods}


\section{Heuristics for Adaptive Learning Rates}

There are a number of widely used heuristics which are provided in the
default optimizer selection of machine learning frameworks such as Tensorflow.

As we have only covered \ref{eq: gradient descent} and momentum so far, we have
quite a bit of ground to cover. But at the same time most of the remaining
common optimizer \parencite[as reviewed by e.g.][]{ruderOverviewGradientDescent2017}
can be motivated in a similar way, which might or might not be new, as I did not
properly search previous work for this idea.

Recall that the \ref{eq: newton minimum approx}
\begin{align*}
	\weights_{n+1}	= \weights_n - (\nabla^2\Loss(\weights_n))^{-1}\nabla\Loss(\weights_n)
\end{align*}
finds the vertex of a quadratic loss function \(\Loss\) immediately, because we
have
\begin{align*}
	\nabla\Loss(\weights_n) = \nabla^2\Loss(\weights_n)[\weights_n - \hat{\weights}_n],
\end{align*}
where \(\hat{\weights}_n\) was the vertex of the second Taylor approximation.
To better understand how that works, note that for
\begin{align*}
	\nabla^2\Loss(\weights_n) = V \diag(\hesseEV_1,\dots,\hesseEV_\dimension)V^T
\end{align*}
we have
\begin{align*}
	\nabla^2\Loss(\weights_n)^{-1}
	= V \diag(\tfrac1\hesseEV_1,\dots,\tfrac1\hesseEV_\dimension)V^T.
\end{align*}
To simplify things, let us assume a quadratic loss function
\(H=\nabla^2\Loss(\weights_n)\) now.  In this case we found that every
eigenspace scales independently
\begin{align*}
	\langle \weights_{n+1} - \minimum, v_i \rangle
	\le (1-\lr \hesseEV_i)\langle \weights_n - \minimum, v_i \rangle
\end{align*}
So the \ref{eq: newton minimum approx} rescales the steps by dividing every
eigenspace through its respective eigenvalue. For positive eigenvalues this is
all nice and dandy. But for negative eigenvalues, which represent a maximum in
their eigenspace, the \ref{eq: newton minimum approx} multiplies the gradient by
a negative value \(\tfrac1{\hesseEV_i}\) and therefore causes movement towards
this eigenvalue.

Okay, now that we remembered these crucial problems, let us make one egregious
assumption. Let us assume that the standard basis are already the eigenvectors,
i.e. \(V=\identity\). Then the components of the gradient of our loss function
are proportional to their respective eigenvalues
\begin{align*}
	\nabla\Loss(\weights_n)^{(i)}
	= \hesseEV_i [\weights_n - \minimum]^{(i)}.
\end{align*}
If we define
\begin{align*}
	G_n^{(i)} = \sum_{k=0}^n [\nabla\Loss(\weights_n)^{(i)}]^2 \qquad i=1,\dots,\dimension,
\end{align*}
then the algorithm
\begin{align}
	\label{eq: almost adagrad}
	\weights_{n+1}^{(i)}
	&= \weights_n^{(i)} - \frac{\lr_n}{\sqrt{G_n^{(i)}}}\nabla\Loss(\weights_n)^{(i)}\\
	\nonumber
	&= \weights_n^{(i)} - \frac{\lr_n}{
		\sqrt{\cancel{\hesseEV_i^2}\sum_{k=0}^n([\weights_k - \minimum]^{(i)})^2}
	}
	\cancel{\hesseEV_i}[\weights_n - \minimum]^{(i)}
\end{align}
also cancels out the eigenvalues. But as we divide through the absolute value of
the eigenvalues in contrast to the \ref{eq: newton minimum approx}, we get to
keep the sign
\begin{align}\label{eq: adagrad motivation contraction}
	[\weights_{n+1}-\minimum]^{(i)} = \left(1-\frac{\lr_n\sign(\hesseEV_i)}{
			\sqrt{\sum_{k=0}^n([\weights_k - \minimum]^{(i)})^2}
		}\right)[\weights_n - \minimum]^{(i)}.
\end{align}
Of course the notion that the standard basis vectors are already the eigenvectors
is pretty ridiculous. But for some reason the following algorithms seem to
work. And they are basically based on this notion of reweighting the components
by dividing them through the squared past gradients in some sense. The intuition
provided by \textcite{ruderOverviewGradientDescent2017} is, that
``It adapts the learning rate to the parameters, performing smaller updates
(i.e. low learning rates) for parameters associated with frequently occurring
features, and larger updates (i.e. high learning rates) for parameters
associated with infrequent features.'' But even this intuition seems to assume
that these ``features'' seem to somehow correspond to the coordinates.

\begin{description}
	\item[AdaGrad] \parencite{duchiAdaptiveSubgradientMethods2011}
	We have already motivated most of AdaGrad in (\ref{eq: almost adagrad}).
	But we are going to use the stochastic gradients
	\begin{align*}
		G_n^{(i)} := \sum_{k=0}^n [\nabla\loss(\weights_n, Z_n)^{(i)}]^2 \qquad i=1,\dots,\dimension,
	\end{align*}
	and introduce a small term \(\epsilon\) \parencite[\(\approx 10^{-8}\)][]{ruderOverviewGradientDescent2017}
	to avoid numerical instability. This result in the ``AdaGrad'' algorithm
	\begin{align*}
		\Weights_{n+1}^{(i)}
		&= \Weights_n^{(i)}
		- \frac{\lr_n}{\sqrt{G_n^{(i)}+\epsilon}}\nabla\loss(\Weights_n, Z_n)^{(i)}
		\qquad i=1,\dots,\dimension.
	\end{align*}
	If we were to divide the sum of squared gradients \(G_n^{(i)}\) by the number
	of summands, we would get an estimator for the second moment. This second
	moment consists not only of the average squares of the actual loss function,
	but also of a variance estimator. This means that high variance can also
	reduce the learning rate. This is an additional benefit, as we have found
	when looking at SGD, that we need to reduce the learning rate when the
	variance starts to dominate.

	\item[RMSProp]\parencite[lecture 6e]{hintonNeuralNetworksMachine2012} As the sum of squares increases monotonically, we can see 
	in (\ref{eq: adagrad motivation contraction}) that this quickly reduces
	the learning rate of AdaGrad in every step. So RMSProp actually uses an
	estimator for the second moments, i.e. divides \(G_n^{(i)}\) by \(n\). Well,
	that is a bit of a lie. To avoid calculations it uses a recursive exponential
	average
	\begin{align*}
		\hat{\E}[(\nabla\loss(\Weights_n)^{(i)})^2]
		:= \gamma \hat{\E}[(\nabla\loss(\Weights_{n-1})^{(i)})^2]
		+ (1-\gamma)(\nabla\loss(\Weights_n, Z_n)^{(i)})^2.
	\end{align*}
	Another benefit of this exponential average is, that we weigh more recent
	gradients higher. As the Second Taylor Approximation is only locally
	accurate, so it is probably a good idea to discount older gradients further
	away. Anyway, the algorithm ``RMSProp'' is defined as
	\begin{align*}
		\Weights_{n+1}^{(i)}
		&= \Weights_n^{(i)}
		- \frac{\lr_n}{\text{RMS}(\nabla\loss)_n}\nabla\loss(\Weights_n, Z_n)^{(i)}
		\qquad i=1,\dots,\dimension,
	\end{align*}
	where we denote the Root Mean Square
	\begin{align*}
		\text{RMS}(\nabla\loss)_n := \sqrt{\hat{\E}[\nabla\loss(\Weights_n)^{(i)}]^2+\epsilon}.
	\end{align*}

	\item[AdaDelta] \parencite{zeilerADADELTAAdaptiveLearning2012} The motivation
	for the independently developed AdaDelta starts exactly like RMSProp, but
	there is one additional step.
	``When considering the parameter updates, [\(\Delta \weights\)], being applied to
	[\(\weights\)], the units should match. That is, if the parameter had some
	hypothetical units, the changes to the parameter should be changes in those
	units as well. When considering SGD, Momentum, or ADAGRAD, we can see that
	this is not the case'' \parencite[p. 3]{zeilerADADELTAAdaptiveLearning2012}.
	They also note that the \ref{eq: newton minimum approx} does have matching
	units. So they consider the case where the Hessian is diagonal (the case we
	have been considering where the eigenvectors are the standard basis), and
	somehow deduce that one should use a similar estimator
	\(\text{RMS}(\Delta\Weights)\) for \(\sqrt{\E[\Delta\Weights^2]}\) as for
	\(\sqrt{\E[\nabla\loss^2]}\) to obtain ``AdaDelta''
	\begin{align*}
		\Weights_{n+1}^{(i)} = \Weights_n^{(i)}
		-\lr_n\frac{\text{RMS}(\Delta\Weights)_{n-1}}{\text{RMS}(\nabla\loss)_n}
		\nabla\loss(\Weights_n, Z_n)^{(i)}.
	\end{align*}

	\item[Adam] \parencite{kingmaAdamMethodStochastic2017} applies the same idea
	as RMSProp to the momentum update, i.e.
	\begin{align*}
		\Weights_{n+1} = \Weights_n + \frac{\lr_n}{\text{RMS}(\nabla\loss)_n} \momentum_{n+1}
	\end{align*}
	where we recall that momentum was defined as
	\begin{align*}
		\momentum_{n+1} = \underbrace{(1-\friction \lr_n)}_{\momCoeff_n} \momentum_n - \lr_n\nabla\Loss(\Weights_n).
	\end{align*}
	Additional to this idea, the paper also introduces the idea of a correction term
	as we have to initialize our exponential averages of the momentum term \(\momentum_n\)
	and \(\text{RMS}(\nabla\loss)_n\). Since these initializations usually are zero
	we have a bias towards zero, which Adam tries to compensate by multiplying
	the term \(\momentum_n\) with \(\frac{1}{1-\momCoeff^n}\) assuming a constant
	momentum coefficient, and similarly for \(\text{RMS}(\nabla\loss)_n\). Then
	this modified momentum and second moment estimator of the gradients are
	used for the update instead.


	\item[NAdam] \parencite{dozatIncorporatingNesterovMomentum2016}
	uses the same idea as Adam just with Nesterov momentum instead of heavy ball
	momentum.

	\item[AdaMax] \parencite{kingmaAdamMethodStochastic2017} In the same paper
	as Adam, the authors also note that the Root Mean Squared
	\(\text{RMS}(\nabla\loss)\) are basically a \(2\)-norm and one could use a
	different norm for the gradients as well.  Recall that we essentially want
	the absolute eigenvalue of the hesse matrix in the end. For that, squaring
	the gradient components works just as well as taking the absolute value (i.e.
	using the maximum norm). This idea results in the ``AdaMax'' algorithm.

	\item[AMSGrad] \parencite{reddiConvergenceAdam2019} While we have argued that
	the exponential moving average used for second moment estimation is a good
	thing, \textcite{reddiConvergenceAdam2019} find, that in the cases where
	these methods do not converge it is due to some seldomly occurring directions
	getting discounted too fast. So they suggest using a running maximum instead.
\end{description}

Since all these optimizers make component-wise adjustments which could be written
as a diagonal matrix, Assumption~\ref{assmpt: parameter in generalized linear
hull of gradients} still applies. Which means our complexity bounds from
Section~\ref{sec: complexity bounds} still apply. Therefore these algorithms
can not be an improvement over standard momentum in general as Nesterov's
momentum is already asymptotically optimal.

And the theoretical justification for these algorithms is quite thin. There
are some fantastic animation by \textcite{radfordVisualizingOptimizationAlgos2014}
which heavily imply that these ``adaptive algorithms'' are better at breaking
symmetry. But these toy problems are generally axis aligned which means their
eigenspaces will be the eigenvectors. And at that point we know that these
algorithms should behave like Newton's algorithm. So it is no surprise that they
perform really well.

On the other hand one can find examples where these adaptive methods become very
unstable
\parencite[e.g.][]{wilsonMarginalValueAdaptive2018,reddiConvergenceAdam2019}.
But these examples might be unrealistic as well.

In empirical benchmarks such as \textcite{schmidtDescendingCrowdedValley2021} no
optimizer seems to outperform the others consistently.


\section{(Quasi) Newton Methods}

\subsection{Cubic Regularization of Newton's Method}

We have motivated the Newton-Raphson method on quadratic functions where
convergence is instant. Convergence analysis for general loss functions requires
an analysis of the way the Hessian changes. In Section~\ref{sec:
lipschitz continuity of the Gradient} we used Lipschitz continuity of the
gradient to get an upper bound on our Loss (\ref{eq: lin approx + distance
penalty notion})
\begin{align*}
	\Loss(\theta)
	\le \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac{\ubound}2 \|\theta-\weights\|^2,
\end{align*}
which we then optimized over in Lemma~\ref{lem: smallest upper bound} to find
that gradient descent with learning rate \(\tfrac1\ubound\) is optimal with
regards to this bound. The Lipschitz constant \(\ubound\) bounds the second
derivative and thus the rate of change of the gradient. This allows us to
formulate for how long we can trust our gradient evaluation in some sense.

In the case of the Newton-Raphson method, we need a similar rate of change
bound on the Hessian. In this case we want to bound the rate of change of the
second derivative so we assume a Lipschitz continuous second derivative (or 
in other words bounded third derivative). This similarly results in the upper
bound
\begin{align*}
	\Loss(\theta)
	\le \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac12\langle \nabla^2\Loss(\weights)(\theta-\weights), \theta-\weights \rangle
	+ \tfrac{M}6\|\theta-\weights\|^3.
\end{align*}
The solution of which represents the cubic regularization of Newton's method
\parencite[Section 4.1]{nesterovLecturesConvexOptimization2018}.
The solution of which can unfortunately not be written explicitly.

Since this method is history agnostic again, Nesterov utilizes estimating sequences
again, to derive an ``accelerated cubic Newton scheme'' \parencite[Section
4.2]{nesterovLecturesConvexOptimization2018} similar to Nesterov momentum, i.e.
the accelerated gradient descent.

\subsection{Quasi Newton Methods}

While the analysis and regularization of Newton's method itself is important,
it can not be used directly as the calculation and storage of the Hessian is
usually too expensive at cost \(O(\dimension^2)\), and if that is not the case,
then inversion of this matrix at cost \(O(\dimension^3)\) almost certainly is,
for most machine learning applications. Additionally we have saddle point
repelling properties on our wish list.

\subsubsection{Gaus Newton}
\textcite{bottouOptimizationMethodsLargeScale2018}

\subsubsection{L-BFGS}

\subsubsection{Krylov Subspace Descent}

\textcite{vinyalsKrylovSubspaceDescent2012}

Turned into Saddle Point repelling methods by \textcite{dauphinIdentifyingAttackingSaddle2014}

\subsection{Conjugate Gradient Descent}

relation to momentum

\subsubsection{Hessian Free Gradient Descent}
\textcite{martensDeepLearningHessianfree2010}

\section{Natural Gradient Descent}

relation to Newton


\section{Geometric Methods}

\subsection{Cutting Plane Methods}

Notice how the \(\dimension-1\) dimensional hyperplane
\begin{align*}
	\mathcal{H}
	:= \{\theta : \langle \nabla\Loss(\weights), \theta - \weights \rangle = 0\}
\end{align*}
separates the set in which direction the loss is increasing
\begin{align*}
	\mathcal{H}_+
	:= \{\theta : \langle \nabla\Loss(\weights), \theta - \weights \rangle > 0\},
\end{align*}
from the set in which direction the loss is decreasing \(\mathcal{H}_-\). While
this is only a local statement, convexity implies that \(\Loss(\weights)\) is
smaller than the loss at any \(\theta\in\mathcal{H}_+\) because
\begin{align*}
	\Loss(\theta)
	\ge \Loss(\weights) + \underbrace{\langle \nabla\Loss(\weights), \theta -\weights\rangle}_{>0}
	> \Loss(\weights)
\end{align*}
Therefore we know the minimum can not be in \(\mathcal{H}_+\), which allows us
to cut away this entire set from consideration.

Cutting plane methods use this fact to iteratively shrink the considered set
until the minimum is found. As convexity is vital here, it is unlikely they
might ever work for non-convex loss functions.

\subsubsection{Center of Gravity Method}

This method simply evaluates the gradient at the center of gravity of the
remaining set in the hope that the cutting plane method cuts away roughly half
of the remaining set. Therefore convergence speed can be shown to be
linear\footnote{i.e. exponential in non numerical-analysis jargon} 
\parencite[e.g.][Theorem 2.1]{bubeckConvexOptimizationAlgorithms2015}.
Unfortunately this requires calculating the center of gravity of an arbitrary
set which is its own computational problem. 

\subsubsection{Ellipsoid Method}

To work around the center of gravity problem, the ellipsoid methods starts out
with an ellipsoid set, makes one cut through the center, and finds a new
ellipsoid around the remaining set. This new ellipsoid is larger than the
set we would actually have to consider, but at least finding its center is
tractable. The convergence speed is still linear \parencite[e.g][Theorem
2.4]{bubeckConvexOptimizationAlgorithms2015}.


\subsubsection{Vaidya's Cutting Plane Method}

This method uses polytopes instead of ellipsoids as base objects to make the
center of gravity search tractable, and seems to be more efficient
computationally \parencite[e.g.][Section
2.3]{bubeckConvexOptimizationAlgorithms2015}.

\subsection{Geometric Descent}

\textcite{bubeckGeometricAlternativeNesterov2015} realized that strong convexity
\begin{align*}
	\Loss(\theta)
	\ge \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights\rangle
	+ \tfrac{\lbound}{2}\|\theta-\weights\|^2
\end{align*}
is equivalent to
\begin{align*}
	\tfrac{\lbound}{2}\|\theta - (\weights - \tfrac1\lbound \nabla\Loss(\weights))\|^2
	\le \frac{\|\nabla\Loss(\weights)\|^2}{2\lbound} - (\Loss(\weights)-\Loss(\theta))
\end{align*}
which implies that for \(\theta=\minimum\) we know that the minimum
\(\minimum\) is inside the ball
\begin{align*}
	\minimum\in B\left(\weights- \tfrac1\lbound\nabla\Loss(\weights), 
		\frac{\|\nabla\Loss(\weights)\|^2}{\lbound^2}
		- \frac{ \Loss(\weights)-\Loss(\minimum) }{\lbound}
	\right).
\end{align*}
With Lipschitz continuity of the gradient \textcite{bubeckGeometricAlternativeNesterov2015}
further refine this ``confidence ball'' around the minimum and use it to create
an algorithm with the same rate of convergence as Nesterov's momentum.
Unfortunately it looks like this algorithm is much more dependent on
convexity than the momentum method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
