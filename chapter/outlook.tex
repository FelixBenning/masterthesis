% !TEX root = ../Masterthesis.tex
\chapter{Other Methods}\label{chap: other methods}


\section{Heuristics for Adaptive Learning Rates}
\label{sec: heuristics for adpative learning rates}

There are a number of widely used heuristics which are provided in the
default optimizer selection of machine learning frameworks such as Tensorflow.

As we have only covered \ref{eq: gradient descent} and momentum so far, we have
quite a bit of ground to cover. But at the same time most of the remaining
common optimizer \parencite[as reviewed by e.g.][]{ruderOverviewGradientDescent2017}
can be motivated in a similar way, which might or might not be new.

Recall that the \ref{eq: newton minimum approx}
\begin{align*}
	\weights_{n+1}	= \weights_n - (\nabla^2\Loss(\weights_n))^{-1}\nabla\Loss(\weights_n)
\end{align*}
finds the vertex of a quadratic loss function \(\Loss\) immediately, because we
have
\begin{align*}
	\nabla\Loss(\weights_n) = \nabla^2\Loss(\weights_n)[\weights_n - \hat{\weights}_n],
\end{align*}
where \(\hat{\weights}_n\) was the vertex of the second Taylor approximation.
To better understand how that works, note that for
\begin{align*}
	\nabla^2\Loss(\weights_n) = V \diag(\hesseEV_1,\dots,\hesseEV_\dimension)V^T
\end{align*}
we have
\begin{align*}
	\nabla^2\Loss(\weights_n)^{-1}
	= V \diag(\tfrac1\hesseEV_1,\dots,\tfrac1\hesseEV_\dimension)V^T.
\end{align*}
To simplify things, let us assume a quadratic loss function
\(H=\nabla^2\Loss(\weights_n)\) now.  In this case we found that every
eigenspace scales independently
\begin{align*}
	\langle \weights_{n+1} - \minimum, v_i \rangle
	\le (1-\lr \hesseEV_i)\langle \weights_n - \minimum, v_i \rangle
\end{align*}
So the \ref{eq: newton minimum approx} rescales the steps by dividing every
eigenspace through its respective eigenvalue. For positive eigenvalues this is
all nice and dandy. But for negative eigenvalues, which represent a maximum in
their eigenspace, the \ref{eq: newton minimum approx} multiplies the gradient by
a negative value \(\tfrac1{\hesseEV_i}\) and therefore causes movement towards
this eigenvalue.

Okay, now that we remembered these crucial problems, let us make one egregious
assumption. Let us assume that the standard basis are already the eigenvectors,
i.e.\ \(V=\identity\). Then the components of the gradient of our loss function
are proportional to their respective eigenvalues
\begin{align*}
	\nabla\Loss(\weights_n)^{(i)}
	= \hesseEV_i [\weights_n - \minimum]^{(i)}.
\end{align*}
If we define
\begin{align*}
	G_n^{(i)} = \sum_{k=0}^n [\nabla\Loss(\weights_n)^{(i)}]^2 \qquad i=1,\dots,\dimension,
\end{align*}
then the algorithm
\begin{align}
	\label{eq: almost adagrad}
	\weights_{n+1}^{(i)}
	&= \weights_n^{(i)} - \frac{\lr_n}{\sqrt{G_n^{(i)}}}\nabla\Loss(\weights_n)^{(i)}\\
	\nonumber
	&= \weights_n^{(i)} - \frac{\lr_n}{
		\sqrt{\cancel{\hesseEV_i^2}\sum_{k=0}^n([\weights_k - \minimum]^{(i)})^2}
	}
	\cancel{\hesseEV_i}[\weights_n - \minimum]^{(i)}
\end{align}
also cancels out the eigenvalues. But as we divide through the absolute value of
the eigenvalues in contrast to the \ref{eq: newton minimum approx}, we get to
keep the sign
\begin{align}\label{eq: adagrad motivation contraction}
	[\weights_{n+1}-\minimum]^{(i)} = \left(1-\frac{\lr_n\sign(\hesseEV_i)}{
			\sqrt{\sum_{k=0}^n([\weights_k - \minimum]^{(i)})^2}
		}\right)[\weights_n - \minimum]^{(i)}.
\end{align}
Of course the notion that the standard basis vectors are already the eigenvectors
is pretty ridiculous. But for some reason the following algorithms seem to
work. And they are basically based on this notion of reweighting the components
by dividing them through the squared past gradients in some sense. The intuition
provided by \textcite{ruderOverviewGradientDescent2017} is, that
``It adapts the learning rate to the parameters, performing smaller updates
(i.e.\ low learning rates) for parameters associated with frequently occurring
features, and larger updates (i.e.\ high learning rates) for parameters
associated with infrequent features.'' But even this intuition seems to assume
that these ``features'' seem to somehow correspond to the coordinates.

\begin{description}
	\item[AdaGrad] \parencite{duchiAdaptiveSubgradientMethods2011}
	We have already motivated most of AdaGrad in (\ref{eq: almost adagrad}).
	But we will use the stochastic gradients
	\begin{align*}
		G_n^{(i)} := \sum_{k=0}^n [\nabla\loss(\weights_n, Z_n)^{(i)}]^2 \qquad i=1,\dots,\dimension,
	\end{align*}
	and introduce a small term \(\epsilon\) \parencite[\(\approx 10^{-8}\)][]{ruderOverviewGradientDescent2017}
	to avoid numerical instability. This result in the ``AdaGrad'' algorithm
	\begin{align*}
		\Weights_{n+1}^{(i)}
		&= \Weights_n^{(i)}
		- \frac{\lr_n}{\sqrt{G_n^{(i)}+\epsilon}}\nabla\loss(\Weights_n, Z_n)^{(i)}
		\qquad i=1,\dots,\dimension.
	\end{align*}
	If we were to divide the sum of squared gradients \(G_n^{(i)}\) by the number
	of summands, we would get an estimator for the second moment. This second
	moment consists not only of the average squares of the actual loss function,
	but also of a variance estimator. This means that high variance can also
	reduce the learning rate. This is an additional benefit, as we have found
	when looking at SGD, that we need to reduce the learning rate when the
	variance starts to dominate.

	\item[RMSProp]\parencite[lecture 6e]{hintonNeuralNetworksMachine2012} As the sum of squares increases monotonically, we can see 
	in (\ref{eq: adagrad motivation contraction}) that this quickly reduces
	the learning rate of AdaGrad in every step. So RMSProp actually uses an
	estimator for the second moments, i.e.\ divides \(G_n^{(i)}\) by \(n\). Well,
	that is a bit of a lie. To avoid calculations it uses a recursive exponential
	average
	\begin{align*}
		\hat{\E}[(\nabla\loss(\Weights_n)^{(i)})^2]
		:= \gamma \hat{\E}[(\nabla\loss(\Weights_{n-1})^{(i)})^2]
		+ (1-\gamma)(\nabla\loss(\Weights_n, Z_n)^{(i)})^2.
	\end{align*}
	Another benefit of this exponential average is, that we weigh more recent
	gradients higher. As the Second Taylor Approximation is only locally
	accurate, so it is probably a good idea to discount older gradients further
	away. Anyway, the algorithm ``RMSProp'' is defined as
	\begin{align*}
		\Weights_{n+1}^{(i)}
		&= \Weights_n^{(i)}
		- \frac{\lr_n}{\text{RMS}(\nabla\loss)_n}\nabla\loss(\Weights_n, Z_n)^{(i)}
		\qquad i=1,\dots,\dimension,
	\end{align*}
	where we denote the Root Mean Square
	\begin{align*}
		\text{RMS}(\nabla\loss)_n := \sqrt{\hat{\E}[\nabla\loss(\Weights_n)^{(i)}]^2+\epsilon}.
	\end{align*}

	\item[AdaDelta] \parencite{zeilerADADELTAAdaptiveLearning2012} The motivation
	for the independently developed AdaDelta starts exactly like RMSProp, but
	there is one additional step.
	``When considering the parameter updates, [\(\Delta \weights\)], being applied to
	[\(\weights\)], the units should match. That is, if the parameter had some
	hypothetical units, the changes to the parameter should be changes in those
	units as well. When considering SGD, Momentum, or ADAGRAD, we can see that
	this is not the case'' \parencite[p. 3]{zeilerADADELTAAdaptiveLearning2012}.
	They also note that the \ref{eq: newton minimum approx} does have matching
	units. So they consider the case where the Hessian is diagonal (the case we
	have been considering where the eigenvectors are the standard basis), and
	somehow deduce that one should use a similar estimator
	\(\text{RMS}(\Delta\Weights)\) for \(\sqrt{\E[\Delta\Weights^2]}\) as for
	\(\sqrt{\E[\nabla\loss^2]}\) to obtain ``AdaDelta''
	\begin{align*}
		\Weights_{n+1}^{(i)} = \Weights_n^{(i)}
		-\lr_n\frac{\text{RMS}(\Delta\Weights)_{n-1}}{\text{RMS}(\nabla\loss)_n}
		\nabla\loss(\Weights_n, Z_n)^{(i)}.
	\end{align*}

	\item[Adam] \parencite{kingmaAdamMethodStochastic2017} applies the same idea
	as RMSProp to the momentum update, i.e.
	\begin{align*}
		\Weights_{n+1} = \Weights_n + \frac{\lr_n}{\text{RMS}(\nabla\loss)_n} \momentum_{n+1}
	\end{align*}
	where we recall that momentum was defined as
	\begin{align*}
		\momentum_{n+1} = \underbrace{(1-\friction \lr_n)}_{\momCoeff_n} \momentum_n - \lr_n\nabla\Loss(\Weights_n).
	\end{align*}
	Additional to this idea, the paper also introduces the idea of a correction term
	as we have to initialize our exponential averages of the momentum term \(\momentum_n\)
	and \(\text{RMS}(\nabla\loss)_n\). Since these initializations usually are zero
	we have a bias towards zero, which Adam tries to compensate by multiplying
	the term \(\momentum_n\) with \(\frac{1}{1-\momCoeff^n}\) assuming a constant
	momentum coefficient, and similarly for \(\text{RMS}(\nabla\loss)_n\). Then
	this modified momentum and second moment estimator of the gradients are
	used for the update instead.


	\item[NAdam] \parencite{dozatIncorporatingNesterovMomentum2016}
	uses the same idea as Adam just with Nesterov momentum instead of heavy ball
	momentum.

	\item[AdaMax] \parencite{kingmaAdamMethodStochastic2017} In the same paper
	as Adam, the authors also note that the Root Mean Squared
	\(\text{RMS}(\nabla\loss)\) are basically a \(2\)-norm and one could use a
	different norm for the gradients as well.  Recall that we essentially want
	the absolute eigenvalue of the Hessian in the end. For that, squaring
	the gradient components works just as well as taking the absolute value (i.e.
	using the maximum norm). This idea results in the ``AdaMax'' algorithm.

	\item[AMSGrad] \parencite{reddiConvergenceAdam2019} While we have argued that
	the exponential moving average used for second moment estimation is a good
	thing, \textcite{reddiConvergenceAdam2019} find, that in the cases where
	these methods do not converge it is due to some seldomly occurring directions
	getting discounted too fast. So they suggest using a running maximum instead.
\end{description}

Since all these optimizers make component-wise adjustments which could be written
as a diagonal matrix, Assumption~\ref{assmpt: parameter in generalized linear
hull of gradients} still applies. Which means our complexity bounds from
Section~\ref{sec: complexity bounds} still apply. Therefore these algorithms
can not be an improvement over standard momentum in general as Nesterov's
momentum is already asymptotically optimal.

And the theoretical justification for these algorithms is quite thin. There
are some fantastic animations by \textcite{radfordVisualizingOptimizationAlgos2014},
which heavily imply that these ``adaptive algorithms'' are better at breaking
symmetry. But these toy problems are generally axis aligned which means their
eigenspaces will be the eigenvectors. And at that point we know that these
algorithms should behave like Newton's algorithm. So it is no surprise that they
perform really well.

On the other hand one can find examples where these adaptive methods become very
unstable
\parencite[e.g.][]{wilsonMarginalValueAdaptive2018,reddiConvergenceAdam2019}.
But these examples might be unrealistic as well.

In empirical benchmarks such as \textcite{schmidtDescendingCrowdedValley2021} no
optimizer seems to outperform the others consistently.

While this section was mostly concerned with commonly used algorithms with little
theoretical justification, the remaining chapter will walk through approaches
from the theoretical side which are not as widely used in practice as the
methods discussed so far. In other words, there are no reference implementations
for the following methods in machine learning frameworks such as Tensorflow.
This proxy for usage is likely anything but perfect as most of these methods are
more difficult to implement and might simply be deemed too complex to maintain
reference implementations for. But beginners, especially from fields other than
mathematics, are still likely to stick to the default optimizers.

\section{Geometric Methods}\label{sec: geometric methods}

\subsection{Cutting Plane Methods}

Notice how the \(\dimension-1\) dimensional hyperplane
\begin{align*}
	\mathcal{H}
	:= \{\theta : \langle \nabla\Loss(\weights), \theta - \weights \rangle = 0\}
\end{align*}
separates the set in which direction the loss is increasing
\begin{align*}
	\mathcal{H}_+
	:= \{\theta : \langle \nabla\Loss(\weights), \theta - \weights \rangle > 0\},
\end{align*}
from the set in which direction the loss is decreasing \(\mathcal{H}_-\). While
this is only a local statement, convexity implies that \(\Loss(\weights)\) is
smaller than the loss at any \(\theta\in\mathcal{H}_+\) because
\begin{align*}
	\Loss(\theta)
	\ge \Loss(\weights) + \underbrace{\langle \nabla\Loss(\weights), \theta -\weights\rangle}_{>0}
	> \Loss(\weights)
\end{align*}
Therefore we know the minimum can not be in \(\mathcal{H}_+\), which allows us
to cut away this entire set from consideration.

Cutting plane methods use this fact to iteratively shrink the considered set
until the minimum is found. As convexity is vital here, it is unlikely they
might ever work for non-convex loss functions.

\subsubsection{Center of Gravity Method}

This method simply evaluates the gradient at the center of gravity of the
remaining set in the hope that the cutting plane method cuts away roughly half
of the remaining set. Therefore convergence speed can be shown to be
linear\footnote{i.e.\ exponential in non numerical-analysis jargon} 
\parencite[e.g.][Theorem 2.1]{bubeckConvexOptimizationAlgorithms2015}.
Unfortunately this requires calculating the center of gravity of an arbitrary
set which is its own computational problem. 

\subsubsection{Ellipsoid Method}

To work around the center of gravity problem, the ellipsoid methods starts out
with an ellipsoid set, makes one cut through the center, and finds a new
ellipsoid around the remaining set. This new ellipsoid is larger than the
set we would actually have to consider, but at least finding its center is
tractable. The convergence speed is still linear \parencite[e.g][Theorem
2.4]{bubeckConvexOptimizationAlgorithms2015}.


\subsubsection{Vaidya's Cutting Plane Method}

This method uses polytopes instead of ellipsoids as base objects to make the
center of gravity search tractable, and seems to be more efficient
computationally \parencite[e.g.][Section
2.3]{bubeckConvexOptimizationAlgorithms2015}.

\subsection{Geometric Descent}

\textcite{bubeckGeometricAlternativeNesterov2015} realized that strong convexity
\begin{align*}
	\Loss(\theta)
	\ge \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights\rangle
	+ \tfrac{\lbound}{2}\|\theta-\weights\|^2
\end{align*}
is equivalent to
\begin{align*}
	\tfrac{\lbound}{2}\|\theta - (\weights - \tfrac1\lbound \nabla\Loss(\weights))\|^2
	\le \frac{\|\nabla\Loss(\weights)\|^2}{2\lbound} - (\Loss(\weights)-\Loss(\theta)),
\end{align*}
which implies that for \(\theta=\minimum\) we know that the minimum
\(\minimum\) is inside the ball
\begin{align*}
	\minimum\in B\left(\weights- \tfrac1\lbound\nabla\Loss(\weights), 
		\frac{\|\nabla\Loss(\weights)\|^2}{\lbound^2}
		- \frac{ \Loss(\weights)-\Loss(\minimum) }{\lbound}
	\right).
\end{align*}
With Lipschitz continuity of the gradient \textcite{bubeckGeometricAlternativeNesterov2015}
further refine this ``confidence ball'' around the minimum and use it to create
an algorithm with the same rate of convergence as Nesterov's momentum.
Unfortunately this looks like the algorithm is much more dependent on
convexity than the momentum method.

\section{Conjugate Gradient Descent}\label{sec: conjugate gradient descent}

Similar to momentum, conjugate gradient descent is another attempt to solve
high condition problems. The best introduction I found so far was
\textcite{shewchukIntroductionConjugateGradient1994}. This is an attempt at a
shorter version. 

Recall that we can write
our loss function as an approximate paraboloid (cf. (\ref{eq: paraboloid
approximation of L}))
\begin{align*}
	\Loss(\theta)
	= (\theta-\weights)^T \nabla^2\Loss(\weights)(\theta-\weights)
	+ c(\weights) + o(\|\theta-\weights\|^2)
\end{align*}
If the second taylor approximation is accurate and we have a constant
positive definite Hessian \(H:=\nabla^2\Loss(\weights)\), then our loss can also
be written as
\begin{align*}
	\Loss(\weights) = \Loss(\minimum) + 
	(\weights-\minimum)^T H(\weights-\minimum)
	= \Loss(\minimum) + \|\weights-\minimum\|_H^2,
\end{align*}
where the norm \(\|\cdot\|_H\) is induced by the scalar product
\begin{align*}
	\langle x, y \rangle_H := \langle x, Hy\rangle.
\end{align*}
In the space \((\reals^d, \|\cdot\|_H)\) our problem therefore has condition one
(\(\condition=1\))!

\subsection{Excursion: The Gradient in \((\reals^\dimension, \|\cdot\|_H)\)}

We have made our problem much simpler but the space we are considering much
more complicated. The question is now, what methods we can realistically use
in this new space?

Naively one might wish to take the gradient with respect to this space. As this
gradient would point straight towards the minimum because we are in the
\(\condition=1\) case. This means, that this gradient should be just as
powerful as the Newton-Raphson method, which would be great. So let us try to
obtain this gradient. From the definition of the gradient we have
\begin{align}
	\label{eq: gradient definition equation}
	0\xeq{!}&\lim_{v\to 0}\frac{
		|\Loss(\weights + v) - \Loss(\weights) - \langle \nabla_H\Loss(\weights), v\rangle_H |
	}{\|v\|_H}\\
	\nonumber
	&=\lim_{v\to 0}\frac{
		|\Loss(\weights + v) - \Loss(\weights) -  \nabla_H\Loss(\weights)^T H v |
	}{\|v\|}
	\frac{\|v\|}{\|v\|_H}\\
	\nonumber
	&=\lim_{v\to 0}\frac{
		|\Loss(\weights + v) - \Loss(\weights) -  \langle H\nabla_H\Loss(\weights), v\rangle |
	}{\|v\|}
	\frac{\|v\|}{\|v\|_H}
\end{align}
Where we have used symmetry of the Hessian \(H\) for the last equation. Now if
we had
\begin{align*}
	H \nabla_H \Loss(\weights) = \nabla\Loss(\weights),
\end{align*}
then the first fraction would go to zero by the definition of the gradient.
The second fraction would be bounded due to norm equivalence, since all norms
are equivalent in final dimensional real vector spaces. Therefore
\begin{align*}
	\nabla_H \Loss(\weights) := H^{-1}\nabla\Loss(\weights)
\end{align*}
is a (and due to uniqueness the) solution of (\ref{eq: gradient definition
equation}). So we have found a method equivalent to Newton's method.
Unfortunately it \emph{is} Newtons's method. And since we still do not want
to invert the Hessian, we have to think of something else. Fortunately, that
something else only has to work for \(\condition=1\).

\subsection{Conjugate Directions}

Let us take any (normed) direction \(d^{(1)}\) and complete it to an H-orthogonal
basis \(d^{(1)},\dots,d^{(\dimension)}\). H-orthogonal is also known as
``conjugate''. Then we can represent any vector \(\theta\) as a linear
combination of this conjugate basis
\begin{align*}
	\theta = \sum_{k=1}^\dimension \theta^{(k)} d^{(k)}.
\end{align*}
Denoting \(\Delta\weights_n := \weights_n -\minimum\), let us now optimize over
the learning rate for a move in direction \(d^{(1)}\)
\begin{align*}
	\Delta\weights_{1} = \Delta\weights_0 + \lr_0 d^{(1)}
	= (\Delta\weights^{(1)} + \lr_0)d^{(1)}
	+ \sum_{k=2}^\dimension \Delta\weights^{(k)} d^{(k)}.
\end{align*}
Due to conjugacy we have
\begin{align*}
	\Loss(\weights_{1})
	= \Loss(\minimum) + \|\Delta\weights_{1}\|_H^2
	= \Loss(\minimum) + \sum_{k=1}^{\dimension} (\Delta\weights_{1}^{(k)})^2\|d^{(k)}\|^2.
\end{align*}
And since most of the \(\Delta\weights^{(k)}_{1}\) stay constant no matter the
learning rate \(\lr_n\), the only one we actually optimize over is
\((\Delta\weights^{(1)}_{1})^2\).
And we can make that coordinate zero with \(\lr_0 :=-\Delta\weights_0^{(1)}\).

So if we do a line search (optimize the loss over one direction, i.e.\ \(d^{(1)}\)),
then we know that we will never ever have to move in the direction \(d^{(1)}\)
again. The target \(\minimum\) lies H-orthogonal to \(d^{(1)}\).
%
\begin{figure}[h]
	\centering
	\def\svgwidth{1\textwidth}
	\input{media/conjugate_direction.pdf_tex}
	\caption{
		The method of conjugate directions only works because in the
		\(\condition=1\) case (brown contour), minimizing the loss in any direction
		eliminates that direction. This does not work for general condition numbers
		as illustrated with the blue contour lines.
	}
	\label{fig: conjugate direction depends on condition one}
\end{figure}
%
If we optimize over the next direction the same holds true. In fact we get the
globally optimal solution in the span of search directions using this method 
\begin{align*}
	\Delta\weights_n &= \sum_{k=n+1}^\dimension\Delta\weights^{(k)} d^{(k)}\\
	&= \arg\min \{
		\Loss(\weights) : \weights \in \weights_0 + \linSpan[d^{(1)},\dots, d^{(n)}]
	\} - \minimum.
\end{align*}
In this light, recall that we provided complexity bounds for weights in the
linear span of gradients shifted by \(\weights_0\) in Section~\ref{sec: complexity bounds}
(cf. Assumption~\ref{assmpt: parameter in linear hull of gradients}). So if our
search directions were the gradients, we would obtain optimal convergence for
this class of methods.

\subsection{Simplifying Gram-Schmidt}

Unfortunately our gradients are not conjugate in general. But that is no
trouble, we can just use Gram-Schmidt conjugation. Right?

So here is the problem: We need knowledge of the hessian \(H\) for this
procedure and have to remove the components from all previous directions. This
blows up time complexity making this procedure not worthwhile. But as it turns
out, the gradients do have some additional structure we can exploit to remove
these issues.

First let us summarize what we have so far. We want to obtain the same conjugate
search directions \((d^{(1)},\dots,d^{(\dimension)})\) from
\((\nabla\Loss(\weights_0),\dots,\nabla\Loss(\weights_{\dimension-1}))\)
we would get if we used Gram-Schmidt conjugation, which also implies
\begin{align*}
	\mathcal{K}_k := \linSpan\{\nabla\Loss(\weights_0),\dots,\nabla\Loss(\weights_k)\}
	= \linSpan\{d^{(1)},\dots,d^{(k+1)}\}.
\end{align*}
And we select our weights with line searches along the search directions
\begin{align*}
	\lr_n := \arg\min_\lr \Loss(\weights_n - \lr d^{(n+1)})
\end{align*}
resulting in
\begin{align*}
	\weights_{n+1} = \weights_n - \lr_n d^{(n+1)}
	= \arg\min\{ \Loss(\weights) : \weights\in\weights_0 + \mathcal{K}_n\}.
\end{align*}
This last condition implies that the gradient \(\nabla\Loss(\weights_n)\)
must necessarily be orthogonal to \(\mathcal{K}_{n-1}\) (unless we have already
converged to \(\minimum\)). Otherwise \(\weights_n\) would not be optimal. This
orthogonality also allows us to argue that \(\mathcal{K}_k\) actually has rank
\(k+1\) and its respective generators are basis, which was necessary for the
conjugate directions argument.

So if we could somehow translate this euclidean orthogonality into a
H-orthogonality we would be done. And here comes in one crucial insight:
\(\mathcal{K}_n\) is a ``Krylov subspace'', i.e.\ can be generated by repeated
application of an operator. More specifically we have
\begin{lemma}[\(\mathcal{K}_n\) is a \(H\)-Krylov subspace]
	\begin{align*}
		\mathcal{K}_n
		= \{H^1 \Delta\weights_0, \dots, H^{n+1}\Delta\weights_0\}
		= \{H^0 \nabla\Loss(\weights_0), \dots, H^n\nabla\Loss(\weights_0)\}.
	\end{align*}
\end{lemma}
\begin{proof}
	The induction start \(n=0\) is simply due to \(\nabla\Loss(\weights_0) =
	H\Delta\weights_0\) and the induction step \(n-1\to n\) starts with
	\begin{align}
		\nonumber
		\nabla\Loss(\weights_n)= H\Delta\weights_n
		&= H(\Delta\weights_{n-1} - \lr_{n-1} d^{(n)})\\
		\label{eq: represent nabla Loss(weights_n) with prev gradient and d(n)}
		&= \underbrace{\nabla\Loss(\weights_{n-1})}_{\in \mathcal{K}_{n-1}} - \lr_{n-1} H
		\underbrace{d^{(n)}}_{\in \mathcal{K}_{n-1}}.
	\end{align}
	To finish this step, we apply the induction assumption and notice that the
	second summand might now contain the vector \(H^{n+1}\Delta\weights_0\) if the
	coefficient of \(H^{n}\Delta\weights_0\) is not zero. Therefore
	\(\mathcal{K}_{n}\) is contained in the Krylov subspace. And due to the
	number of elements in the generator of the Krylov subspace it can not have
	higher rank than \(n+1\), and thus cannot be larger.
\end{proof}
This is useful, because it implies that \(H\mathcal{K}_{n-2}\) is contained
in \(\mathcal{K}_{n-1}\) to which \(\nabla\Loss(\weights_n)\) is orthogonal.
Therefore \(\nabla\Loss(\weights_n)\) is already \(H\)-orthogonal to
\(\mathcal{K}_{n-2}\) from the get go. In other words we already have
\begin{align*}
	\langle \nabla\Loss(\weights_n), d^{(k)} \rangle_H = 0 
	\qquad \forall k\le n-1.
\end{align*}
To construct a new conjugate direction \(d^{(n+1)}\) we therefore only have to
remove the previous direction \(d^{(n)}\) component from our gradient! So we
want to select
\begin{align}\label{eq: next search direction}
	d^{(n+1)} = \nabla\Loss(\weights_n)
	- \frac{\langle \nabla\Loss(\weights_n), d^{(n)}\rangle_H}{\|d^{(n)}\|_H^2}d^{(n)}.
\end{align}

\subsection{Evaluating the \(H\)-Dot-Product}

Now, we ``just'' have to find a way to evaluate this factor without actually
knowing the Hessian \(H\). Using the representation (\ref{eq: represent nabla Loss(weights_n) with
prev gradient and d(n)}) of \(\nabla\Loss(\weights_n)\), we get
\begin{align*}
	\langle \nabla\Loss(\weights_n), \nabla\Loss(\weights_n) \rangle
	= \langle \nabla\Loss(\weights_n), \nabla\Loss(\weights_{n-1}) \rangle
	- \lr_{n-1}\langle \nabla\Loss(\weights_n), H d^{(n)}\rangle.
\end{align*}
Using the definition of \(\langle\cdot,\cdot\rangle_H\) and reordering results in
\begin{align}
	\nonumber
	\langle \nabla\Loss(\weights_n), d^{(n)}\rangle_H
	&= \tfrac1{\lr_{n-1}}\left[
		\langle \nabla\Loss(\weights_n), \nabla\Loss(\weights_{n-1}) \rangle
		- \langle \nabla\Loss(\weights_n), \nabla\Loss(\weights_n) \rangle
	\right]\\
	\label{eq: evaluated H dot product}
	&= \frac{- \|\nabla\Loss(\weights_n)\|^2}{\lr_{n-1}},
\end{align}
where we have used the orthogonality of \(\nabla\Loss(\weights_n)\) to
\(\nabla\Loss(\weights_{n-1})\in\mathcal{K}_{n-1}\).
Using the optimality of the learning rate \(\lr_{n-1}\)
\begin{align*}
	0\xeq{!}\frac{d}{d\lr}\Loss(\weights_{n-1} - \lr d^{(n)})
	&= -\langle \nabla\Loss(\weights_{n-1} - \lr d^{(n)}), d^{(n)} \rangle\\
	&= -\langle H(\Delta\weights_{n-1} -\lr d^{(n)}), d^{(n)} \rangle\\
	&= \lr\|d^{(n)}\|_H^2 - \langle \nabla\Loss(\weights_{n-1}), d^{(n)}\rangle,
\end{align*}
we get
\begin{align}\label{eq: optimal learning rate CG}
	\lr_{n-1}
	= - \frac{\langle \nabla\Loss(\weights_{n-1}), d^{(n)}\rangle}{\|d^{(n)}\|_H^2}.
\end{align}
Using (\ref{eq: evaluated H dot product}) and (\ref{eq: optimal learning rate CG})
we can replace the Gram-Schmidt constant from (\ref{eq: next search direction})
with things we can actually calculate resulting in
\begin{align*}
	d^{(n+1)}
	&= \nabla\Loss(\weights_n)
	- \frac{
		\|\nabla\Loss(\weights_n)\|^2
	}{\langle \nabla\Loss(\weights_{n-1}), d^{(n)}\rangle}d^{(n)}\\
	&= \nabla\Loss(\weights_n)
	- \frac{\|\nabla\Loss(\weights_n)\|^2}{\|\nabla\Loss(\weights_{n-1})\|^2}
	d^{(n)},
\end{align*}
where the last equation is due to \(\nabla\Loss(\weights_{n-1})\) being orthogonal
to \(d^{(n-1)}\in\mathcal{K}_{n-2}\), i.e.
\begin{align*}
	\langle \nabla\Loss(\weights_{n-1}), d^{(n)} \rangle
	\xeq{(\ref{eq: next search direction})} 
	\langle \nabla\Loss(\weights_{n-1}),\nabla\Loss(\weights_{n-1}) - c d^{(n-1)}\rangle
	= \|\nabla\Loss(\weights_{n-1})\|^2.
\end{align*}

\subsection{Relation to Momentum and Discussion}

Unfortunately the assumption of a constant and positive definite Hessian
\(H=\nabla^2\Loss(\weights)\) is fairly difficult to remove here. To do so
we would somehow have to estimate the error we are making, working with the taylor
approximation. This would probably result in some form of estimating sequence
argument again. And we already found that Nesterov momentum is in some sense
optimal then.

In fact for specific selections of the momentum coefficient, the momentum method
\emph{is} the conjugate gradient method. For that consider that the movement
direction \(d^{(n+1)}\) consists of the gradient and the previous direction
\(d^{(n)}\) which would be our momentum. More specifically we have
\begin{align*}
	\lr_{n-1}d^{(n)} = \weights_{n-1} - \weights_n
\end{align*}
and therefore
\begin{align*}
	\weights_{n+1}
	&= \weights_n - \lr_n d^{(n+1)}\\
	&= \weights_n - \lr_n \left[
		\nabla\Loss(\weights_n)
		- \frac{\|\nabla\Loss(\weights_n)\|^2}{\|\nabla\Loss(\weights_{n-1})\|^2}
		d^{(n)},
	\right]\\
	&= \weights_n - \underbrace{\lr_n}_{=:\alpha_n} \nabla\Loss(\weights_n)
		+ \underbrace{\tfrac{\lr_n\|\nabla\Loss(\weights_n)\|^2}{\lr_{n-1}\|\nabla\Loss(\weights_{n-1})\|^2}}_{=:\beta_n}
		(\weights_n-\weights_{n-1}).
\end{align*}
Conjugate gradient can therefore be viewed as a special form of momentum with
a particular selection of parameters.

For a constant Hessian it has the nice property that it converges in
a finite number of steps. And quickly! It is actually a competitive method to
solve linear equations \parencite[Section 10]{shewchukIntroductionConjugateGradient1994},
e.g. by using CG on
\begin{align*}
	\Loss(\weights) = \|A\weights - b\|^2.
\end{align*}
Visualizing CG would be somewhat boring as it converges in two steps on two
dimensional loss surfaces.

\section{BFGS}

If we wish to estimate the Hessian with the change of our gradients over time,
we will soon notice that we do not get the evaluation of the Hessian at one point
but rather an average
\begin{align}\label{eq: secant equation}
	\underbrace{\nabla\Loss(\weights_n) - \nabla\Loss(\weights_{n-1})}_{=:\Delta g_n}
	= \underbrace{\int_0^1 \nabla^2\Loss(\weights_{n-1} + t(\weights_n-\weights_{n-1}))dt}_{=:H_n}
	\underbrace{(\weights_n - \weights_{n-1})}_{=:\Delta\weights_n}.
\end{align}
Using this ``average'' \(H_n\) in place of the Hessian \(\nabla^2\Loss(\weights_n)\)
to reshape the gradient results in the ``secant method''
\begin{align*}
	\tag{secant method}
	\weights_{n+1}	= \weights_n - H_n^{-1}\nabla\Loss(\weights_n).
\end{align*}
While it might seem like this averaging is undesirable at first, this averaging
can sometimes be desireable as it smooths out noisy or fluctuating Hessians
\parencite[e.g.][]{metzGradientsAreNot2021}. In one dimension this is already well
defined, as we can simply define
\begin{align*}
	H_n := \frac{\Loss'(\weights_n)-\Loss'(\weights_{n-1})}{\weights_n-\weights_{n-1}}.
\end{align*}
But there is no unique solution to the secant equation (\ref{eq: secant equation})
in \(\dimension\) dimensions. This is because both the weight difference
\(\Delta\weights_n\) and the gradient difference \(\Delta g_n\) are
\(\dimension\) dimensional vectors, (\ref{eq: secant
equation}) therefore represents \(d\) linear equations to find the
\(\frac{\dimension(\dimension+1)}{2}\) entries of the symmetric matrix \(H_n\).
And for \(\dimension>1\) we have
\begin{align*}
	\frac{\dimension(\dimension+1)}{2} > \dimension.
\end{align*}
To make \(H_n\) unique we have to add additional requirements. 

\subsection{DFP}

If we already have an estimate of the Hessian \(\hat{H}_{n-1}\) from previous
iterations, it might be sensible to use ``as much as possible'' of
\(\hat{H}_{n-1}\) and change only so much that \(\hat{H}_n\) fulfills the secant
condition (\ref{eq: secant equation}), i.e.
\begin{align}\label{eq: dfp optimization problem}
	\hat{H}_n &:=
	\arg\min_H \|H-\hat{H}_{n-1}\|
	& \text{subject to}\quad H=H^T,\quad  \Delta g_n= H\Delta\weights_n.
\end{align}
This still leaves us with the selection of the norm. A norm that turned out to 
be useful is the weighted Frobenius norm \(\|\cdot\|_W\) with
\begin{align*}
	\|A\|_W := \|W^{1/2}AW^{1/2}\|_F \quad\text{where}\quad
	\|A\|_F^2 := \text{trace}(A^TA)=\sum_{i,j=1}^{\dimension}A_{i,j}^2
\end{align*}
If we select \(W:=H_n^{-1}\), then according to
\textcite{nocedalQuasiNewtonMethods2006} the unique solution of (\ref{eq: dfp
optimization problem}) is
\begin{align}
	\tag{DFP}
	\hat{H}_n =
	\left(\identity-\frac{\Delta g_n \Delta\weights_n^T}{\langle \Delta g_n, \Delta\weights_n\rangle}\right)
	\hat{H}_{n-1}
	\left(\identity-\frac{\Delta g_n \Delta\weights_n^T}{\langle \Delta g_n, \Delta\weights_n\rangle}\right)
	+ \frac{\Delta g_n \Delta g_n^T}{\langle \Delta g_n, \Delta\weights_n\rangle}.
\end{align}
Since we really want to use \(\hat{H}_n^{-1}\) and matrix inversion is expensive
\textcite{nocedalQuasiNewtonMethods2006} remark that there is also a recursion
for its inverse
\begin{align*}
	\tag{DFP'}
	\hat{H}_n^{-1}
	= \hat{H}_{n-1}^{-1}
	- \frac{
		\hat{H}_{n-1}^{-1}\Delta g_n \Delta g_n^T \hat{H}_{n-1}^{-1}
	}{
		\langle \Delta g_n, \hat{H}_{n-1}^{-1}\Delta g_n\rangle
	}
	+ \frac{
		\Delta\weights_n \Delta\weights_n^T
	}{
		\langle \Delta g_n, \Delta\weights_n\rangle
	}.
\end{align*}
This estimation \(\hat{H}_n\) of \(H_n\) was first proposed by Davidon in 1959
and further examined and developed by Fletcher and Powell. Hence its name.

\subsection{BFGS}

Since we only really want \(\hat{H}_n^{-1}\)
\textcite{broydenConvergenceClassDoublerank1970,fletcherNewApproachVariable1970,goldfarbFamilyVariablemetricMethods1970,shannoConditioningQuasiNewtonMethods1970}
all independently came up with the following improvement to DFP, which is why
this chapter is not called DFP. First we reorder (\ref{eq: secant equation})
into
\begin{align*}
	H_n^{-1} \Delta g_n = \Delta\weights_n
\end{align*}
and now the idea is to look for the inverse directly instead of looking for
\(H_n\) first. In other words we look for
\begin{align*}
	\hat{H}_n^{-1} &:= \arg\min_{H^{-1}} \| H^{-1} - \hat{H}_{n-1}^{-1}\|
	& \text{ subject to}\quad H=H^T,\quad H^{-1}\Delta g_n =\Delta \weights_n.
\end{align*}
For the weighted Frobenius norm with weights \(W=H_n\) this results in the BFGS
recursion
\begin{align}
	\tag{BFGS}
	\hat{H}_n^{-1} :=
	\left(\identity-\tfrac{\Delta\weights_n \Delta g_n^T}{\langle \Delta g_n, \Delta\weights_n\rangle}\right)
	\hat{H}_{n-1}^{-1}
	\left(\identity-\tfrac{ \Delta\weights_n\Delta g_n^T}{\langle \Delta g_n, \Delta\weights_n\rangle}\right)
	+ \tfrac{\Delta\weights_n \Delta \weights_n^T}{\langle \Delta g_n, \Delta\weights_n\rangle}.
\end{align}
An interesting property of this update is, that it retains strict positive
definiteness of previous estimates as we have for a nonzero vector \(z\)
\begin{align*}
	z^T \hat{H}_n^{-1} z
	&= \underbrace{
		\left(z-\tfrac{
			\langle\Delta\weights_n, z\rangle \Delta g_n^T
		}{
			\langle \Delta g_n, \Delta\weights_n\rangle
		}\right)
		\hat{H}_{n-1}^{-1}
		\left(z-\tfrac{\langle \Delta\weights_n, z\rangle \Delta g_n^T}{\langle \Delta g_n, \Delta\weights_n\rangle}\right)
	}_{
	\ge \begin{cases}
		z^T \hat{H}_{n-1}^{-1} z > 0 & \langle\Delta\weights_n, z\rangle^2 =0 \\
		0 & \text{else}
	\end{cases}
	}
	+ \tfrac{\langle\Delta\weights_n, z\rangle^2}{\langle \Delta g_n, \Delta\weights_n\rangle}.
	\\
	&> 0.
\end{align*}

\subsection{L-BFGS}

Limited memory BFGS is essentially just an implementation detail of BFGS. As
we only want to apply our inverse Hessian \(\hat{H}_n^{-1}\) to some vector,
i.e. the gradient, we do not necessarily need to know or store this matrix if
we can achieve this goal without doing so. And if we use our recursion we can
apply our inverse Hessian to the vector \(v\) by calculating
\begin{align*}
	\hat{H}_n^{-1}v =
	\left(\identity-\tfrac{\Delta\weights_n \Delta g_n^T}{\langle \Delta g_n, \Delta\weights_n\rangle}\right)
	\hat{H}_{n-1}^{-1}
	\left(v-\tfrac{ \langle \Delta g_n, v\rangle}{\langle \Delta g_n, \Delta\weights_n\rangle}\Delta\weights_n\right)
	+ \tfrac{\langle \Delta \weights_n, v\rangle}{\langle \Delta g_n, \Delta\weights_n\rangle}\Delta\weights_n.
\end{align*}
Assuming \(\langle \Delta g_n, \Delta\weights_n\rangle\) is precalculated, we
only need to calculate a scalar product and subtract two vectors until we have
reached a point where we need to only apply \(\hat{H}_{n-1}^{-1}\) to a vector.
Going through this recursively and assuming \(\hat{H}_0=\identity\), we only
need \(O(n\dimension)\) operations to calculate \(\hat{H}_n^{-1}v\) this way.
For \(n\le\dimension\) this can be cheaper than matrix vector multiplication
requiring \(O(\dimension^2)\) operations. Unfortunately after some time it
becomes more and more expensive.

So limited memory BFGS simply throws away information older than \(k\) iterations,
effectively restarting the estimation of the inverse Hessian with
\(\hat{H}_{n-k}^{-1}=\identity\), guaranteeing a cost of \(O(k\dimension)=O(\dimension)\).

\section{Global Newton Methods}

So far we have only motivated the Newton-Raphson method on quadratic functions.
Assuming we are in some local area around the minimum, it can be shown that
successive \ref{eq: newton minimum approx}s result in convergence. But these
proofs rely on the fact that the Hessian is locally accurate enough. Therefore
even these proofs implicitly bound the rate of change of the curvature (Hessian).
For global Newton methods we just go about this more explicitly, like we did
with the rate of change of \ref{eq: gradient descent} (Section~\ref{sec:
lipschitz continuity of the Gradient}).

\subsection{Cubic Regularization of Newton's Method}

 In Section~\ref{sec: lipschitz continuity of the Gradient} we used Lipschitz
 continuity of the gradient to get an upper bound on our Loss (\ref{eq: lin
 approx + distance penalty notion})
\begin{align*}
	\Loss(\theta)
	\le \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac{\ubound}2 \|\theta-\weights\|^2,
\end{align*}
which we then optimized over in Lemma~\ref{lem: smallest upper bound} to find
that gradient descent with learning rate \(\tfrac1\ubound\) is optimal with
regards to this bound. The Lipschitz constant \(\ubound\) bounds the second
derivative and thus the rate of change of the gradient. This allows us to
formulate for how long we can trust our gradient evaluation in some sense.

In the case of the Newton-Raphson method, we need a similar rate of change
bound on the Hessian. In this case we want to bound the rate of change of the
second derivative so we assume a Lipschitz continuous second derivative (or 
in other words bounded third derivative). This similarly results in the upper
bound
\begin{align*}
	\Loss(\theta)
	\le \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac12\langle \nabla^2\Loss(\weights)(\theta-\weights), \theta-\weights \rangle
	+ \tfrac{M}6\|\theta-\weights\|^3.
\end{align*}
Minimizing this upper bound results in the cubic regularization of Newton's method
\parencite[Section 4.1]{nesterovLecturesConvexOptimization2018}.
The solution of which can unfortunately not be written explicitly.

Since this method is history agnostic again, Nesterov reuses estimating
sequences, to derive an ``accelerated cubic Newton scheme'' \parencite[Section
4.2]{nesterovLecturesConvexOptimization2018} similar to Nesterov momentum, i.e.
the accelerated gradient descent.

\subsection{Levenberg-Marquard Regularization}\label{subsec: levenberg-marquard regularization}

Instead of using a cubic bound motivated by a bounded third derivative, one
could still use a quadratic bound
\begin{align*}
	\Loss(\theta)
	&\le \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac12\langle \nabla^2\Loss(\weights)(\theta-\weights), \theta-\weights \rangle
	+ \tfrac{M}2\|\theta-\weights\|^2\\
	&= \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac12\langle [\nabla^2\Loss(\weights)+M\identity](\theta-\weights), \theta-\weights \rangle
\end{align*}
which might perhaps be motivated by a bounded second derivative
\begin{align*}
	\lbound\identity \precsim \nabla^2\Loss(\weights) \precsim \ubound\identity
\end{align*}
which would justify the bound \(M:=\ubound-\lbound\), as we have
\begin{align}
	\label{eq: levenberg-marquard justification}
	\Loss(\theta)
	&\le \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \underbrace{\tfrac{\ubound}2\|\theta-\weights\|^2\mathrlap{.}}_{
		\le \tfrac12\langle
			[\nabla^2\Loss(\weights)+M\identity]
		\mathrlap{
			(\theta-\weights),
				\theta-\weights
			\rangle
		}
	}
\end{align}
Of course in practice one would probably use smaller \(M\), hoping that
the changes are not so drastic that the smallest eigenvalues become the
upper bound. While minimizing the cubic bound was difficult, minimizing this
bound results in
\begin{align}\label{eq: levenberg-marquard regularization}
	\weights_{n+1}
	= \weights_n - [\nabla^2\Loss(\weights_n)+M\identity]^{-1}\nabla\Loss(\weights_n).
\end{align}
For this result one simply replaces \(\nabla^2\Loss(\weights)\) with
\([\nabla^2\Loss(\weights)+M\identity]\) in the derivation of the \ref{eq:
newton minimum approx}.

For very small \(M\) this method behaves like the Newton-Raphson method, while
for large \(M\) the step in every eigenspace is roughly \(\tfrac1M\) and the
method therefore behaves like \ref{eq: gradient descent}. Levenberg-Marquard
regularization (\ref{eq: levenberg-marquard regularization}) can therefore also
be viewed as a way to interpolate between the faster Newton-Raphson method and
the more stable \ref{eq: gradient descent}. 

It moves small eigenvalues further away from zero, making the Hessian easier to invert (without numerical errors). And it can turn negative eigenvalues into
(small) positive eigenvalues, which results in large movements in these
eigenspaces in the correct direction (away from the vertex). On the other hand
it might also fail to make large negative eigenvalues positive, turning them into
small negative eigenvalues causing big jumps towards the vertex in this
eigenspace. So if we pick \(M\) too small, this method can make things worse.
If it is too big, it will just slow down convergence and behave roughly like
gradient descent with a small learning rate. 

\subsubsection{Theoretical Guarantees}

As we have used the Lipschitz continuity to justify its bound (\ref{eq:
levenberg-marquard justification}), we are already starting out with a worse
bound than the one we have used for the convergence of gradient descent. So
this approach does not result in better convergence guarantees. It is
therefore more of a heuristic than theoretically motivated. In his motivation
for the cubic regularization Nesterov states ``Unfortunately,
none of these approaches seems to be useful in addressing the global behavior of
second-order schemes.'' \parencite[p. 242]{nesterovLecturesConvexOptimization2018}.
This also includes trust bounds (Subsection~\ref{subsec: trust bounds}) and
damped Newton (Subsection~\ref{subsec: damped newton}).

\subsubsection{Hessian Free Optimization}

Since (\ref{eq: levenberg-marquard regularization}) still requires the inversion
of a matrix, which is infeasible in high dimension,
\textcite{martensDeepLearningHessianfree2010} suggests a Hessian free variant.
He notes that the bound
\begin{align}\label{eq: levenberg-marquard upper bound}
	B_{\Loss(\weights)}(\theta) := \Loss(\weights) + \langle \nabla\Loss(\weights), \theta-\weights \rangle
	+ \tfrac12\langle [\nabla^2\Loss(\weights)+M\identity](\theta-\weights), \theta-\weights \rangle
\end{align}
we are optimizing over is a quadratic, (and hopefully convex) function and
suggests using conjugate gradient descent (Section~\ref{sec: conjugate gradient
descent}) on this bound. Conjugate gradient descent does not require inversion
of the hessian and it does not even require the hessian itself. It only
requires the ability to evaluate \(\nabla^2\Loss(\weights)v\) for any vector
\(v\). For this evaluation \textcite{martensDeepLearningHessianfree2010} suggests
using either the finite differences approximation
\begin{align*}
	\nabla^2\Loss(\weights) v
	\approx \frac{\nabla\Loss(\weights + \epsilon v) - \nabla\Loss(\weights)}{\epsilon}.
\end{align*}
or adapting automatic differentiation frameworks to this task, as discussed in
\textcite{pearlmutterFastExactMultiplication1994}. This avoids storage cost
\(O(\dimension^2)\) of the Hessian. Stopping conjugate gradient descent early,
avoids most of the computational costs as well. This is justified with the fact
that the first few iterations of conjugate gradient achieve most of the
optimization.

\subsubsection{Krylov Subspace Descent}

Let us assume we are using Hesse Free Optimization and run conjugate gradient
descent for \(m\) steps on \(\weights_n\), then we know that conjugate gradient
will find the optimal \(\weights_{n+1}\) in the Krylov subspace \(\mathcal{K}_{m-1}\)
\begin{align}\label{eq: hessian free next}
	\tag{Hessian free}
	\weights_{n+1} = \arg\min \{ B_{\Loss(\weights_n)}(\weights) : \weights \in \weights_n + \mathcal{K}_{m-1} \},
\end{align}
where \(B_{\Loss(\weights_n)}\) is the upper bound defined in (\ref{eq:
levenberg-marquard upper bound}) and the Krylov subspace is accordingly
\begin{align*}
	\mathcal{K}_m
	&= \linSpan\{
		[\nabla^2\Loss(\weights_n) + M\identity]^0 \nabla\Loss(\weights_n), \dots,
		[\nabla^2\Loss(\weights_n) + M\identity]^m \nabla\Loss(\weights_n)
	\}	\\
	&= \linSpan\{
		[\nabla^2\Loss(\weights_n)]^0 \nabla\Loss(\weights_n), \dots,
		[\nabla^2\Loss(\weights_n)]^m \nabla\Loss(\weights_n)
	\},
\end{align*}
where we get the second equation as \(M\identity \nabla\Loss(\weights_n)\) is
just a multiple of the first element and therefore the linear hulls are the
same.

Noticing this fact, \textcite{vinyalsKrylovSubspaceDescent2012} argue that we
can avoid choosing the ``correct'' regularization parameter \(M\), if we
select
\begin{align}\label{eq: krylov subspace descent}
	\tag{Krylov subspace descent}
	\weights_{n+1}
	= \arg\min \{ \Loss(\weights) : \weights \in \weights_n + \mathcal{K}_{m-1} \}
\end{align}
instead of (\ref{eq: hessian free next}) which requires \(B_{\Loss(\weights_n)}\)
and in turn the regularization parameter \(M\) for this upper bound. Of course
at this point we can not use conjugate gradient descent anymore. They then suggest
using L-BFGS to solve this optimization problem. Why not use L-BFGS in the first
place? Well, the problem
\begin{align*}
	\min \{ \Loss(\weights_n + \weights) : \weights \in \mathcal{K}_{m-1} \}
\end{align*}
can be reparametrized using an (orthogonal) basis \(d^{(1)}, \dots, d^{(m)}\)
of \(\mathcal{K}_{m-1}\) into
\begin{align*}
	\min_{\alpha} \Loss\left(\weights_n + \sum_{k=1}^m \alpha_k d^{(k)} \right)
\end{align*}
which is easier to solve as the dimensionality of the problem is greatly
reduced for \(m\ll \dimension\). So in some sense it is similar to
line search. Krylov subspace descent essentially bets on the conjecture that the
Krylov subspaces represent the ``important'' directions for optimization. Just
like the gradient direction is the optimal direction for line search.

\subsection{Trust Bounds}\label{subsec: trust bounds}

Another way to make sure that we can trust the Hessian is to have literal trust
bounds, i.e.
\begin{align*}
	\min_{ \theta\in B(\weights) }
	\underbrace{
		\Loss(\weights)
		+ \langle \nabla\Loss(\weights), \theta-\weights\rangle
		+ \langle \nabla^2\Loss(\weights)(\theta-\weights), \theta-\weights\rangle
	}_{= T_2\Loss(\theta)}
\end{align*}
where \(B(\weights)\) is some trust set around \(\weights\). For the euclidean
ball
\begin{align*}
	B(\weights) = \{ \theta : \|\theta - \weights\| \le \epsilon\}
\end{align*}
this restriction can be converted into the secondary condition
\begin{align*}
	g(\theta) := \frac{\|\theta - \weights\|^2 - \epsilon^2}{2} \le 0.
\end{align*}
The Lagrangian function to this constrained optimization problem is
\begin{align*}
	L(\theta, M) 
	&= T_2\Loss(\theta) + M g(\theta)\\
	&= \Loss(\weights) -\tfrac{\epsilon^2}{2}
	+ \langle \nabla\Loss(\weights), \theta-\weights\rangle
	+ \langle [\nabla^2\Loss(\weights) + M\identity](\theta-\weights), \theta-\weights\rangle.
\end{align*}
As the constant \(\frac{\epsilon}2\) does not change this optimization problem,
its solution is the same as in Levenberg-Marquard regularization. So trust
bounds are the more general concept.

In particular \textcite{dauphinIdentifyingAttackingSaddle2014} define the bound
\begin{align*}
	B(\weights) = \{ \theta : \|\theta-\weights\|_{|H|} \le \epsilon\}
\end{align*}
where \(|H|\) is the hessian with its eigenvalues replaced by their absolute values
and \(\|x\|_{|H|}:= x^T |H| x\). They show that this trust bound results in
\begin{align*}
	\tag{saddle-free Newton}
	\weights_{n+1}	= \weights_n - |\nabla^2\Loss(\weights_n)|^{-1}\nabla\Loss(\weights_n).
\end{align*}
This method avoids moving towards saddle points and maxima as it does not flip
the direction with negative eigenvalues in these eigenspaces. Since the
calculation of the eigenvalues of the Hessian is generally intractable for
high dimensional problems, \textcite{dauphinIdentifyingAttackingSaddle2014}
use the dimension reduction idea from Krylov subspace descent to calculate
and take the absolute value of the eigenvalues of the reduced Hessian.


\subsection{Damped Newton}\label{subsec: damped newton}

``Not moving too far away'' could also be phrased as a line search along the
direction given by the Newton-Raphson method, i.e.
\begin{align*}
	\lr_n := \arg\min_\lr \Loss\left(
		\weights_n - \lr [\nabla^2\Loss(\weights_n)]^{-1}\nabla\Loss(\weights_n)
	\right)	
\end{align*}
resulting in
\begin{align}
	\tag{damped Newton}
	\weights_{n+1}
	= \weights_n - \lr_n [\nabla^2\Loss(\weights_n)]^{-1}\nabla\Loss(\weights_n).
\end{align}


\section{Gauß-Newton Algorithm}

The Gauß-Newton Algorithm opens up the black box of our loss function to create
a relatively cheap approximation of the Hessian \(\nabla^2\Loss\).
To motivate the (classical) Gauß-Newton algorithm, we are going to assume a
squared loss
\begin{align*}
	\loss(\weights, z) := \tfrac12(\model(x) - y)^2, \quad z=(x,y).
\end{align*}
Assuming differentiation and integration can be freely swapped, we have
\begin{align}
	\label{eq: derivative gauss newton}
	\nabla\Loss(\weights)
	&= \E[\nabla\tfrac12(\model[\weights](X)-Y)^2]
	= \E[(\model[\weights](X)-Y)\nabla\model[\weights](X)],\\
	\label{eq: hessian gauss newton}
	\nabla^2\Loss(\weights)
	&=\E[\nabla[(\model[\weights](X)-Y)\nabla\model[\weights](X)]]\\
	\nonumber
	&=\E[\nabla\model(X)\nabla\model(X)^T + (\model(X)-Y)\nabla^2\model(X)].
\end{align}
Now if our model is close to the truth, i.e. we are close to the minimum, then
\(\model(X)-Y\) should be small. Therefore we should approximately have
\begin{align*}
	\nabla^2\Loss(\weights) \approx \E[\nabla\model(X)\nabla\model(X)^T].
\end{align*}
A different way to motivate this approximation, is to use the first taylor
approximation of \(\model\) inside the loss function, i.e.
\begin{align*}
	\Loss(\theta)
	&= \E[\loss(\theta, Z)]
	\approx \E[\tfrac12(\model(X) + \langle \nabla\model(X), \theta-\weights\rangle - Y)^2]\\
	&= \begin{aligned}[t]
		&\E[\tfrac12(\model(X)-Y)^2]\\
		&+ \E[(\model(X)-Y)\langle \nabla\model(X), \theta-\weights\rangle]\\
		&+ \E[\tfrac12\langle \nabla\model(X), \theta-\weights\rangle^2]
	\end{aligned}\\
	&= \Loss(\weights) + \langle\nabla\Loss(\weights), \theta-\weights\rangle
	+ \tfrac12 (\theta-\weights)^T
	\underbrace{\E[\nabla\model(X)\nabla\model(X)^T]}_{\approx\nabla^2\Loss(\weights)}
	(\theta-\weights)
\end{align*}
The nice thing about this approximation is, that it is always positive definite
since we have
\begin{align*}
	 v^T \E[\nabla\model(X)\nabla\model(X)^T] v
	&= \E[ v^T\nabla\model(X) \nabla\model(X)^T v]\\
	&= \E[\langle \nabla\model(X), v\rangle^2]\ge 0.
\end{align*}

\subsection{Levenberg-Marquard Algorithm}

Unfortunately it is not necessarily strictly positive definite. That is, if there
is a vector \(v\), which is orthogonal to \(\nabla\model(X)\) for all but a
zero probability set of \(X\), then this vector is in the eigenspace with
eigenvalue zero. For the expectation one might still be able to discuss that
away. But if one optimizes with regard to a finite sample
\(\sample=(Z_1,\dots,Z_\sampleSize)\), then we have
\begin{align*}
	\nabla^2\Loss_\sample(\weights)
	\approx \E_{Z\sim\dist_\sample}[\langle \nabla\model(X), v\rangle^2]
	= \frac1\sampleSize\sum_{k=1}^\sampleSize [\langle \nabla\model(X_k), v\rangle^2]
\end{align*}
and it is much easier to find a vector orthogonal to a finite set of gradients.
Especially if we are talking about small mini-batches and do not use full batch
gradient descent.

For this reason the Gauß-Newton algorithm is usually combined with
Levenberg-Marquard regularization (Subsection~\ref{subsec: levenberg-marquard
regularization}) to allow for inversion. With this regularization it is also
called Levenberg-Marquard algorithm going back to
\textcite{levenbergMethodSolutionCertain1944} and rediscovered by
\textcite{marquardtAlgorithmLeastSquaresEstimation1963}, since that
regularization technique was developed with the Gauß-Newton algorithm in
mind.

\subsection{Generalized Gauß-Newton}

While the classical Gauß-Newton algorithm seems to be tailored to squared losses
only, this approach can actually be generalized to more general loss functions
\parencite{schraudolphFastCurvatureMatrixVector2001}. Assuming the loss function
is of the form \(\loss(f,y)\), i.e with some abuse of notation
\begin{align*}
	\loss(\weights, z) := \loss(\model(x), y), \quad z=(x,y),
\end{align*}
then we obtain similarly to (\ref{eq: derivative gauss newton}) and (\ref{eq:
hessian gauss newton})
\begin{align}
	\label{eq: derivative generalized gauss newton}
	\tfrac{d}{d\weights}\Loss(\weights)
	&= \E[\tfrac{d}{d\weights}(\model(X), Y)]
	= \E[\tfrac{d}{df}\loss(\model(X),Y)\tfrac{d}{d\weights}\model(X)],\\
	\label{eq: hessian generalized gauss newton}
	\tfrac{d^2}{d\weights^2}\Loss(\weights)
	&=\E[\tfrac{d}{d\weights}[\tfrac{d}{df}\loss(\model(X),Y)\tfrac{d}{d\weights}\model(X)]]\\
	\nonumber
	&=\begin{aligned}[t]
		&\E[\tfrac{d}{d\weights}\model(X)^T\tfrac{d^2}{df^2}\loss(\model(X),Y)\tfrac{d}{d\weights}\model(X)]\\
		&+ \E[\tfrac{d}{df}\loss(\model(X),Y)\tfrac{d^2}{d\weights^2}\model(X)].
	\end{aligned}
\end{align}
Again we throw away the second part with the Hessian of the model \(\model\),
to obtain the approximation
\begin{align*}
	\nabla^2\Loss(\weights)
	&\approx\E[\tfrac{d}{d\weights}\model(X)\tfrac{d^2}{df^2}^T\loss(\model(X),Y)\tfrac{d}{d\weights}\model(X)].
\end{align*}
The result is quite similar to the classical Gauß-Newton method, except for the
Hessian of the loss function in the middle. The calculation of which might be a
huge or tiny problem. If the model prediction is one dimensional it is likely
not an issue. Then it would only be a constant.

One should still come up with some justification why the second part of the
Hessian part can be thrown away though, i.e. why is
\begin{align*}
	\E[\tfrac{d}{df}\loss(\model(X),Y)\tfrac{d^2}{d\weights^2}\model(X)]
\end{align*}
small? In the sqared loss case, we have used the fact that
\(\tfrac{d}{df}\loss(\model(X),Y)\) was small for good models.

\section{Natural Gradient Descent}

``Natural gradient descent'' would require a deep dive into ``information geometry''
for which we do not have time. But not mentioning this algorithm at all would do it
injustice. So to avoid playing a game of telephone, here is a direct quote of
the description by \textcite{bottouOptimizationMethodsLargeScale2018}:

``We have seen that Newton's method is invariant to linear transformations of
the parameter vector w. By contrast, the natural gradient method [5, 6] aims to
be invariant with respect to all differentiable and invertible transformations.
The essential idea consists of formulating the gradient descent algorithm in the
space of prediction functions rather than specific parameters. Of course, the
actual computation takes place with respect to the parameters, but accounts for
the anisotropic relation between the parameters and the decision function. That
is, in parameter space, the natural gradient algorithm will move the parameters
more quickly along directions that have a small impact on the decision function,
and more cautiously along directions that have a large impact on the decision
function.

We remark at the outset that many authors [119, 99] propose
quasi-natural-gradient methods that are strikingly similar to the quasi-Newton
methods described in §6.2. The natural gradient approach therefore offers a
different justification for these algorithms, one that involves qualitatively
different approximations. It should also be noted that research on the design of
methods inspired by the natural gradient is ongoing and may lead to markedly
different algorithms.''

\section{Coordinate Descent}

Coordinate descent only ever updates one coordinate. ``[I]n situations in which
\(d\) coordinate updates can be performed at cost similar to the evaluation of
one full gradient, the method is competitive with a full gradient method both
theoretically and in practice.'' \parencite[7.3, p.
72]{bottouOptimizationMethodsLargeScale2018}. This might be interesting as the
Hessian is of course much easier to handle in one dimension. A line search might
not be the right thing to though, as
\textcite{powellSearchDirectionsMinimization1973} provides an example of a three
dimensional function on which coordinate descent jumps between the
corners of the cube, never converging. Although this behavior can be fixed
with a stochastic selection of the coordinates instead of cycling through them.
In fact \textcite{bubeckConvexOptimizationAlgorithms2015} argues that coordinate
descent \emph{is} just SGD with a random filter, filtering out all
but one coordinate and disappearing in expectation. I.e. for a random index \(I\)
and standard basis vectors \(\stdBasis_1,\dots,\stdBasis_\dimension\) we can
define
\begin{align*}
	\nabla\loss(\weights, i)
	:= \tfrac1{P(I=i)} \langle \nabla\Loss(\weights), \stdBasis_i \rangle \stdBasis_i
\end{align*}
to get the original loss in expectation
\begin{align*}
	\E[\nabla\loss(\weights,I)] = \sum_{k=1}^\dimension P(I=k) \nabla\loss(\weights,k)
	= \sum_{k=1}^\dimension \langle \nabla\Loss(\weights), \stdBasis_k\rangle \stdBasis_k
	= \nabla\Loss(\weights).
\end{align*}
With conditional independence the same argument can be applied to already
stochastic losses. So we can apply our SGD convergence statements to prove
convergence of random coordinate descent.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput
