% !TEX root = ../Masterthesis.tex

\chapter{Technical Proofs}

\section{Convex Analysis}

\begin{lemma}\label{Appdx-lem: lipschitz and bounded derivative}
	If \(f\) is differentiable, then the derivative \(\nabla f\) is
	bounded (w.r.t. the operator norm) by constant \(\lipConst\) if and only if the function
	\(f\) is \(\lipConst\)-Lipschitz continuous.
\end{lemma}
\begin{proof}
	Lipschitz continuity is implied by the mean value theorem
	\begin{align*}
		\|f(x_1) - f(x_0)\|
		&\le \|\nabla f(x_0 + \xi(x_1-x_0))\| \|x_1- x_0\|\\
		&\le \lipConst \|x_1-x_0\|.
	\end{align*}
	%
	The opposite direction is implied by
	%
	\begin{align*}
		\|\nabla f(x_0)\|
		&\equiv \sup_{v} \frac{\|\nabla f(x_0)v\|}{\|v\|}
		= \sup_{v} \lim_{v\to 0}\frac{\|\nabla f(x_0)v\|}{\|v\|}\\
		&\lxle{\Delta}\sup_{v} \lim_{v\to 0}
		(
			\underbrace{
				\tfrac{\|f(x_0 + v) - f(x_0) \|}{\|v\|}
			}_{
				\le \lipConst 
			}
			+ \underbrace{
				\tfrac{\|\nabla f(x_0)v + f(x_0) - f(x_0 +v)\|}{\|v\|}
			}_{
				\to 0 \text{ (derivative definition)}
			}
		)
	\end{align*}
	%
	where we have used the scalability of the norm to multiply \(v\) with a
	decreasing factor both in the numerator and denominator in order to introduce
	the limit.
\end{proof}

\begin{lemma}
	\label{Appdx-lem: Lipschitz Gradient implies taylor inequality}
	If \(\nabla f\) is \(\ubound\)-Lipschitz continuous, then
	\begin{align*}
		|f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}
	If \(f\) is convex, then the opposite direction is also true.
\end{lemma}
\begin{proof}
	The first direction is taken from \textcite[Lemma
	1.2.3]{nesterovLecturesConvexOptimization2018}.
 \begin{align*}
		f(y) = f(x) + \int_0^1\langle\nabla f(x+\tau(y-x)), y-x \rangle d\tau
	\end{align*}
	implies (using the  Cauchy-Schwarz inequality)
	\begin{align*}
		&| f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \\
		&\le \int_0^1 | \langle\nabla f(x+\tau(y-x))-\nabla f(x), y-x\rangle | d\tau \\
		&\lxle{\text{C.S.}}
		\int_0^1 \|\langle\nabla f(x+\tau(y-x))-\nabla f(x)\| \cdot \|y-x\| d\tau\\
		&\le \int_0^1 \ubound \|\tau(y-x)\|\cdot\|y-x\| d\tau
		= \tfrac{\ubound}2 \|y-x\|^2.
		\qedhere
	\end{align*}
	The opposite direction is taken from \textcite[Lemma
	2.1.5]{nesterovLecturesConvexOptimization2018}.
	As we have only used
	\begin{align*}
		0\xle{\text{Convexity}} f(y) - f(x) - \langle \nabla f(x), y-x\rangle
		\le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}	
	in Lemma~\ref{lem: bermanDiv lower bound} (and Lemma~\ref{lem: smallest upper
	bound} which was used in the proof), we know that equation (\ref{eq:
	bergmanDiv lower bound b}) follows from it and after applying Cauchy-Schwarz
	to this equation
	\begin{align*}
		\tfrac{1}\ubound \|\nabla f(x)-\nabla f(y)\|^2
		&\le \langle \nabla f(x) - \nabla f(y), x-y\rangle \\
		&\lxle{\text{C.S.}} \|\nabla f(x) - \nabla f(y)\| \|x-y\|,
	\end{align*}
	we only have to divide both sides by \(\tfrac{1}\ubound \|\nabla f(x)-\nabla
	f(y)\|\) to get \(\ubound\)-Lipschitz continuity back.
\end{proof}

\section{Momentum Convergence}

\begin{theorem}[\cite{qianMomentumTermGradient1999}]
	\label{thm-appdx: momentum - stable set of parameters}
	Let
	\begin{align*}
		\sigma_{1/2}
		= \tfrac12 \left(
			1+\momCoeff-\lrSq\hesseEV \pm \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\right)
	\end{align*}
	then 
	\begin{enumerate}
		\item \(\max\{|\sigma_1|,|\sigma_2|\}<1\) if and only if
		\begin{align*}
			0<\lrSq\hesseEV < 2(1+\momCoeff) \qquad \text{and} \qquad |\momCoeff|<1
		\end{align*}
		\item The complex case can be characterized by either
		\begin{align*}
			0<(1-\sqrt{\lrSq\hesseEV})^2 < \momCoeff < 1
		\end{align*}		
		or alternatively \(\momCoeff>0\) and
		\begin{align*}
			(1-\sqrt{\momCoeff})^2 < \lrSq\hesseEV < (1+\sqrt{\momCoeff})^2,
		\end{align*}
		for which we have \(|\sigma_1|=|\sigma_2|=\sqrt{\momCoeff}\).
		
		\item In the real case we have \(\sigma_1>\sigma_2\) and
		\begin{align}\label{eq: when does sigma_1 or sigma_2 dominate?}
			\max\{|\sigma_1|, |\sigma_2|\} = \begin{cases}
				|\sigma_1|=\sigma_1 & \lrSq\hesseEV < 1+\momCoeff \\
				|\sigma_2|=-\sigma_2 & \lrSq\hesseEV \ge 1+\momCoeff.
			\end{cases}
		\end{align}
		Restricted to \(1>\momCoeff>0\) this results in two different	
		behaviors. For
		\begin{align*}
			0<\lrSq\hesseEV \le (1-\sqrt{\momCoeff})^2 < 1+\momCoeff
		\end{align*}
		we have \(1>\sigma_1 > \sigma_2 > 0\). For
		\begin{align*}
			1+\momCoeff < (1+\sqrt{\momCoeff})^2\le \lrSq\hesseEV < 2(1+\momCoeff)
		\end{align*}
		on the other hand, we get \(-1 < \sigma_2 < \sigma_1 < 0\).
	\end{enumerate}
\end{theorem}
\begin{proof}
	Define
	\begin{align*}
		\Delta(\momCoeff)
		&:= (1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff\\
		&= \momCoeff^2 - 2(1+\lrSq\hesseEV)\momCoeff + (1-\lrSq\hesseEV)^2
	\end{align*}
	then \(\sigma_{1/2}\) is complex iff \(\Delta(\momCoeff)<0\). Since it is a
	convex parabola this implies that \(\momCoeff\) needs to be between the roots
	\begin{align*}
		\momCoeff_{1/2}
		= (1+\lrSq\hesseEV) \pm 
		\underbrace{
			\sqrt{(1+\lrSq\hesseEV)^2 - (1-\lrSq\hesseEV)^2}
		}_{=\sqrt{4\lrSq\hesseEV}=\mathrlap{2\sqrt{\lrSq\hesseEV}}}
		= (1\pm \sqrt{\lrSq\hesseEV})^2	
	\end{align*}
	of \(\Delta\). Assuming those roots are real which requires \(\lrSq\hesseEV>0\).
	But if \(\lrSq\hesseEV\le0\) then we would have
	\begin{align*}
		\sigma_1
		= \tfrac12 \Big(
			\underbrace{1+\momCoeff+|\lrSq\hesseEV|}_{\ge 1+\momCoeff}
			+ \sqrt{\underbrace{(1+\momCoeff+|\lrSq\hesseEV|)^2 - 4\momCoeff}_{\smash{\ge (1-\momCoeff)^2}}}
		\Big)
		\ge \frac{1+\momCoeff + |1-\momCoeff|}2 \ge 1
	\end{align*}
	which means that \(\max\{|\sigma_1|,|\sigma_2|\}<1\) implies \(\lrSq\hesseEV>0\).
	The roots \(\momCoeff_{1/2}\) are therefore real no matter the direction we
	wish to prove.
	\begin{description}[wide, labelindent=0pt]
	\item[Complex Case:]
		\(\sigma_{1/2}\) are complex iff	
		\begin{align}\label{eq: complex case}
			0 < (1-\sqrt{\lrSq\hesseEV})^2 < \momCoeff < (1+\sqrt{\lrSq\hesseEV})^2.
		\end{align}
		In that case we have
		\begin{align*}
			|\sigma_1| = |\sigma_2|
			&= \sqrt{|\Re(\sigma_1)|^2 + |\Im(\sigma_1)|^2}\\
			&= \tfrac12 \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 + 4\momCoeff - (1+\momCoeff-\lrSq\hesseEV)^2}
			= \sqrt{\momCoeff}.
		\end{align*}
		So in this complex case the condition
		\(\momCoeff<1\) is necessary and sufficient for \(\max\{|\sigma_1|,|\sigma_2|\}<1\).
		And since we have seen that \(\lrSq\hesseEV>0\) is necessary and \(0<\momCoeff\)
		from (\ref{eq: complex case}) implies the condition \(|\momCoeff|<1\), we only
		need to show that \(\lrSq\hesseEV <2(1+\momCoeff)\) is necessary in the complex
		case to have this case covered. Using \((\sqrt{\lrSq\hesseEV}-1)^2 < \momCoeff\)
		from (\ref{eq: complex case}) and \(ab \le a^2 + b^2\) we get
		\begin{align*}
			\sqrt{\lrSq\hesseEV} - 1
			< \momCoeff \implies \lrSq\hesseEV < (1+\sqrt{\momCoeff})^2
			\le 2(1+\momCoeff).
		\end{align*}

		The remaining characterization of the complex case follows from
		restricting (\ref{eq: complex case}) to \(\momCoeff<1\) and reordering.
		\item[Real Case:] Since we have \(\sigma_2 \le \sigma_1\),
		\(\max\{|\sigma_1|,|\sigma_2|\}<1\) is equivalent to
		\begin{align*}
			-1 < \sigma_2 \qquad \text{and} \qquad \sigma_1 < 1
		\end{align*}
		By subtracting the part before the "\(\pm\)" from the equation \(\sigma_1<1\)
		after multiplying it by two, we get
		\begin{align}\label{eq: sigma1 condition}
			0\xle{\text{Real Case}} \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 -4\momCoeff} < 1-\momCoeff +\lrSq\hesseEV.
		\end{align}
		And since \((1+\sqrt{\lrSq\hesseEV})^2 \le \momCoeff\) leads to a contradiction:
		\begin{align*}
			0 \le 1-\momCoeff+\lrSq\hesseEV \le 1-(1+\sqrt{\lrSq\hesseEV})^2 +\lrSq\hesseEV
			= -2\sqrt{\lrSq\hesseEV} \ge 0
		\end{align*}
		the real case is restricted to the case \(\momCoeff < (1-\sqrt{\lrSq\hesseEV})^2\)
		as we are otherwise in the complex case (\ref{eq: complex case}).
		This means that due to
		\begin{align*}
			1-\momCoeff+\lrSq\hesseEV
			> 1 - (1-\sqrt{\lrSq\hesseEV})^2 + \lrSq\hesseEV
			= 2\sqrt{\lrSq\hesseEV}>0
		\end{align*}
		the inequality (\ref{eq: sigma1 condition}) is (in the real case) equivalent to
		\begin{align*}
			&(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff < (1-\momCoeff+\lrSq\hesseEV)^2\\
			&\iff \cancel{1^2} + 2(\momCoeff-\lrSq\hesseEV) + 
			\cancel{(\momCoeff-\lrSq\hesseEV)^2} - 4\momCoeff
			< \cancel{1^2} - 2(\momCoeff-\lrSq\hesseEV) + 
			\cancel{(\momCoeff-\lrSq\hesseEV)^2}\\
			&\iff 0 < 4\lrSq\momCoeff.
		\end{align*}
		Which is a given for positive learning rates in the convex	
		case. This is not surprising if one remembers the proof from gradient
		decent. The real issue is not overshooting \(-1\) with large learning
		rates. So multiplying \(-1<\sigma_2\) by two, adding 2 and moving the
		root to the other side we get
		\begin{align}\label{eq: from -1<sigma_2 followed}
			0 \xle{\text{Real Case}} \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
			< 3+\momCoeff-\lrSq\hesseEV
		\end{align}
		Now if we assume \(\lrSq\hesseEV < 3+\momCoeff\), then this condition is
		equivalent to 
		\begin{align}
			\nonumber
			&(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff < (3+\momCoeff-\lrSq\hesseEV)\\
			\nonumber
			&\iff 1^2 + 2(\momCoeff-\lrSq\hesseEV) + \cancel{(\momCoeff-\lrSq\hesseEV)^2} - 4\momCoeff
			< 3^2 + 6(\momCoeff-\lrSq\hesseEV) + \cancel{(\momCoeff-\lrSq\hesseEV)^2}\\
			\label{eq: momentum upper bound derivation}
			&\iff 4\lrSq\hesseEV < 8 + 8\momCoeff
			\iff \lrSq\hesseEV < 2(1+\momCoeff)
		\end{align}
		which is the important requirement on the learning rate we are looking for.
		Now for \(\momCoeff<1\) this requirement is actually stronger than
		\(\lrSq\hesseEV <3+\momCoeff\) which means that we have proven that our
		requirements are sufficient for \(\max\{|\sigma_1|,|\sigma_2|\}<1\).
		What is left to show is that \(|\momCoeff|<1\) is also necessary in the real
		case. Now from (\ref{eq: momentum upper bound derivation}) we immediately
		get
		\begin{align*}
			0 \le \tfrac{\lrSq\hesseEV}2 < 1+\momCoeff
		\end{align*}
		which implies \(-1 < \momCoeff\). At first glance it might look like
		\(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) is already sufficient for the upper
		bound, but for \(\lrSq\hesseEV>4\) this bound is greater than one again.
		So let us assume \(\lrSq\hesseEV>2\) and \(\momCoeff>0\), then
		\(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) implies a contradiction:
		\begin{align*}
			\sqrt{\momCoeff} < \sqrt{\lrSq\momCoeff} -1
			&\implies \smash{\overbrace{(1+\sqrt{\momCoeff})^2}^{1+2\sqrt{\momCoeff} + \momCoeff}}
			< \lrSq\hesseEV
			\stackrel{(\ref{eq: from -1<sigma_2 followed})}{<} 3+\momCoeff \\
			&\implies -2 +2\sqrt{\momCoeff} < 0
			\implies \sqrt{\momCoeff} < 1
		\end{align*}

		For the \emph{characterization of the real case} one only needs to observe
		that for any \(b>0\)
		\begin{align}
			\max|a\pm b| = \begin{cases}
				a + b & a \ge 0\\
				-(a-b) & a < 0
			\end{cases}.
		\end{align}
		In our case \(b\) is a square root in the real case, so by definition
		positive and \(a>0\) is equivalent to \(1+\momCoeff - \lrSq\hesseEV >0\)
		and thus implies (\ref{eq: when does sigma_1 or sigma_2 dominate?}).
		
		For \(\momCoeff>0\) one only needs to keep in mind that for
		 \(\lrSq\hesseEV < (1+\sqrt{\momCoeff})^2 <1+\momCoeff\)
		\begin{align*}
			1+\momCoeff-\lrSq\hesseEV = \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2}
			> \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\end{align*}
		ensures that \(\sigma_2 >0\). On the other hand for \(\momCoeff>0\)
		and \(\lrSq\hesseEV > (1-\sqrt{\momCoeff})^2> 1+\momCoeff\)
		\begin{align*}
			- (1+\momCoeff-\lrSq\hesseEV) = \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2}
			> \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\end{align*}
		implies \(\sigma_1 < 0\). \qedhere
 \end{description}
\end{proof}

\begin{lemma}
	\begin{align*}
		\frac{d}{d\momCoeff}\max\{|\sigma_1|,|\sigma_2|\} < 0 \qquad
		\forall -1<\momCoeff <(1-\sqrt{\lrSq\hesseEV})^2
	\end{align*}
\end{lemma}
\begin{proof}
	Using the characterizations of the real case
	from Theorem~\ref{thm: momentum - stable set of parameters} we get
	\begin{align*}
		\frac{d}{d\momCoeff}\max\{|\sigma_1|,|\sigma_2|\}
		= \begin{cases}
			\frac{d\sigma_1}{d\momCoeff} & \momCoeff \ge \lrSq\hesseEV -1 \\
			-\frac{d\sigma_2}{d\momCoeff} & \momCoeff \le \lrSq\hesseEV -1 
		\end{cases}
	\end{align*}
	This allows us to treat those cases separately. For the first case we use
	the same definition for \(\Delta\) as in Theorem~\ref{thm: momentum - stable
	set of parameters}. Calculating its derivative
	\begin{align*}
		\Delta'(\momCoeff) = 2(1+\momCoeff-\lrSq\hesseEV)	-4
	\end{align*}
	allows us to represent \(\sigma_1\) as
	\begin{align*}
		1>\sigma_1
		= 1 + \frac{\Delta'(\momCoeff)}{4} + \tfrac12 \sqrt{\Delta(\momCoeff)},
	\end{align*}
	which results in the inequality
	\begin{align*}
		0 > \tfrac14\Delta'(\momCoeff) + \tfrac12\sqrt{\Delta(\momCoeff)}
		\implies -\tfrac12 > \tfrac14\frac{\Delta'(\momCoeff)}{\sqrt{\Delta{\momCoeff}}}
	\end{align*}
	We can now use this inequality to show that\footnote{
		I want to apologize for the lack of insight in this proof upon which I
		stumbled by accident playing around with the \(\Delta(\momCoeff)\)
		representation of \(\sigma_{1/2}\). Its only redeeming merit is its brevity.
	}
	\begin{align*}
		\frac{d\sigma_1}{d\momCoeff}
		= \frac12 + \frac14 \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}} < 0.
	\end{align*}
	The second case on the other hand requires us to show that
	\begin{align*}
		\frac{d\sigma_2}{d\momCoeff}
		= \frac12 - \frac14 \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}} > 0
		\iff 2 > \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}}.
	\end{align*}
	Which is true due to
	\begin{align*}
		2\sqrt{\Delta(\momCoeff)} > 0 > -4\sqrt{\lrSq\hesseEV}
		&= 2\overbrace{
			(1-\lrSq\hesseEV + (1-\sqrt{\lrSq\hesseEV})^2)
		}^{\smash{=2-2\sqrt{\lrSq\hesseEV}}} -4\\
		&= \Delta'((1-\sqrt{\lrSq\hesseEV})^2) > \Delta'(\momCoeff),
	\end{align*}
	since \(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) and \(\Delta'\) is monotonously
	increasing in \(\momCoeff\).
 \end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput