
% !TEX root = ../Masterthesis.tex

\chapter{Technical Proofs}

\section{Convex Analysis}

\begin{lemma}\label{Appdx-lem: lipschitz and bounded derivative}
	If \(f\) is differentiable, then the derivative \(\nabla f\) is
	bounded (w.r.t. the operator norm) by constant \(\lipConst\) if and only if the function
	\(f\) is \(\lipConst\)-Lipschitz continuous.
\end{lemma}
\begin{proof}
	Lipschitz continuity is implied by the mean value theorem
	\begin{align*}
		\|f(x_1) - f(x_0)\|
		&\le \|\nabla f(x_0 + \xi(x_1-x_0))\| \|x_1- x_0\|\\
		&\le \lipConst \|x_1-x_0\|.
	\end{align*}
	%
	The opposite direction is implied by
	%
	\begin{align*}
		\|\nabla f(x_0)\|
		&\equiv \sup_{v} \frac{\|\nabla f(x_0)v\|}{\|v\|}
		= \sup_{v} \lim_{v\to 0}\frac{\|\nabla f(x_0)v\|}{\|v\|}\\
		&\lxle{\Delta}\sup_{v} \lim_{v\to 0}
		(
			\underbrace{
				\tfrac{\|f(x_0 + v) - f(x_0) \|}{\|v\|}
			}_{
				\le \lipConst 
			}
			+ \underbrace{
				\tfrac{\|\nabla f(x_0)v + f(x_0) - f(x_0 +v)\|}{\|v\|}
			}_{
				\to 0 \text{ (derivative definition)}
			}
		)
	\end{align*}
	%
	where we have used the scalability of the norm to multiply \(v\) with a
	decreasing factor both in the numerator and denominator in order to introduce
	the limit.
\end{proof}

\begin{lemma}
	\label{Appdx-lem: Lipschitz Gradient implies taylor inequality}
	If \(\nabla f\) is \(\ubound\)-Lipschitz continuous, then
	\begin{align*}
		|f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}
	If \(f\) is convex, then the opposite direction is also true.
\end{lemma}
\begin{proof}
	The first direction is taken from \textcite[Lemma
	1.2.3]{nesterovLecturesConvexOptimization2018}.
 \begin{align*}
		f(y) = f(x) + \int_0^1\langle\nabla f(x+\tau(y-x)), y-x \rangle d\tau
	\end{align*}
	implies (using the  Cauchy-Schwarz inequality)
	\begin{align*}
		&| f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \\
		&\le \int_0^1 | \langle\nabla f(x+\tau(y-x))-\nabla f(x), y-x\rangle | d\tau \\
		&\lxle{\text{C.S.}}
		\int_0^1 \|\langle\nabla f(x+\tau(y-x))-\nabla f(x)\| \cdot \|y-x\| d\tau\\
		&\le \int_0^1 \ubound \|\tau(y-x)\|\cdot\|y-x\| d\tau
		= \tfrac{\ubound}2 \|y-x\|^2.
		\qedhere
	\end{align*}
	The opposite direction is taken from \textcite[Lemma
	2.1.5]{nesterovLecturesConvexOptimization2018}.
	As we have only used
	\begin{align*}
		0\xle{\text{Convexity}} f(y) - f(x) - \langle \nabla f(x), y-x\rangle
		\le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}	
	in Lemma~\ref{lem: bermanDiv lower bound} (and Lemma~\ref{lem: smallest upper
	bound} which was used in the proof), we know that equation (\ref{eq:
	bergmanDiv lower bound b}) follows from it and after applying Cauchy-Schwarz
	to this equation
	\begin{align*}
		\tfrac{1}\ubound \|\nabla f(x)-\nabla f(y)\|^2
		&\le \langle \nabla f(x) - \nabla f(y), x-y\rangle \\
		&\lxle{\text{C.S.}} \|\nabla f(x) - \nabla f(y)\| \|x-y\|,
	\end{align*}
	we only have to divide both sides by \(\tfrac{1}\ubound \|\nabla f(x)-\nabla
	f(y)\|\) to get \(\ubound\)-Lipschitz continuity back.
 \end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput