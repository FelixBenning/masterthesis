% !TEX root = ../Masterthesis.tex

\chapter{Technical Proofs}

\section{Convex Analysis}

\begin{lemma}\label{Appdx-lem: lipschitz and bounded derivative}
	If \(f\) is differentiable, then the derivative \(\nabla f\) is
	bounded (w.r.t. the operator norm) by constant \(\lipConst\) if and only if the function
	\(f\) is \(\lipConst\)-Lipschitz continuous.
\end{lemma}
\begin{proof}
	Lipschitz continuity is implied by the mean value theorem
	\begin{align*}
		\|f(x_1) - f(x_0)\|
		&\le \|\nabla f(x_0 + \xi(x_1-x_0))\| \|x_1- x_0\|\\
		&\le \lipConst \|x_1-x_0\|.
	\end{align*}
	%
	The opposite direction is implied by
	%
	\begin{align*}
		\|\nabla f(x_0)\|
		&\equiv \sup_{v} \frac{\|\nabla f(x_0)v\|}{\|v\|}
		= \sup_{v} \lim_{v\to 0}\frac{\|\nabla f(x_0)v\|}{\|v\|}\\
		&\lxle{\Delta}\sup_{v} \lim_{v\to 0}
		(
			\underbrace{
				\tfrac{\|f(x_0 + v) - f(x_0) \|}{\|v\|}
			}_{
				\le \lipConst 
			}
			+ \underbrace{
				\tfrac{\|\nabla f(x_0)v + f(x_0) - f(x_0 +v)\|}{\|v\|}
			}_{
				\to 0 \text{ (derivative definition)}
			}
		)
	\end{align*}
	%
	where we have used the scalability of the norm to multiply \(v\) with a
	decreasing factor both in the numerator and denominator in order to introduce
	the limit.
\end{proof}

\begin{lemma}
	\label{Appdx-lem: Lipschitz Gradient implies taylor inequality}
	If \(\nabla f\) is \(\ubound\)-Lipschitz continuous, then
	\begin{align*}
		|f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}
	If \(f\) is convex, then the opposite direction is also true.
\end{lemma}
\begin{proof}
	The first direction is taken from \textcite[Lemma
	1.2.3]{nesterovLecturesConvexOptimization2018}.
 \begin{align*}
		f(y) = f(x) + \int_0^1\langle\nabla f(x+\tau(y-x)), y-x \rangle d\tau
	\end{align*}
	implies (using the  Cauchy-Schwarz inequality)
	\begin{align*}
		&| f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \\
		&\le \int_0^1 | \langle\nabla f(x+\tau(y-x))-\nabla f(x), y-x\rangle | d\tau \\
		&\lxle{\text{C.S.}}
		\int_0^1 \|\langle\nabla f(x+\tau(y-x))-\nabla f(x)\| \cdot \|y-x\| d\tau\\
		&\le \int_0^1 \ubound \|\tau(y-x)\|\cdot\|y-x\| d\tau
		= \tfrac{\ubound}2 \|y-x\|^2.
		\qedhere
	\end{align*}
	The opposite direction is taken from \textcite[Lemma
	2.1.5]{nesterovLecturesConvexOptimization2018}.
	As we have only used
	\begin{align*}
		0\xle{\text{Convexity}} f(y) - f(x) - \langle \nabla f(x), y-x\rangle
		\le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}	
	in Lemma~\ref{lem: bermanDiv lower bound} (and Lemma~\ref{lem: smallest upper
	bound} which was used in the proof), we know that equation (\ref{eq:
	bergmanDiv lower bound b}) follows from it and after applying Cauchy-Schwarz
	to this equation
	\begin{align*}
		\tfrac{1}\ubound \|\nabla f(x)-\nabla f(y)\|^2
		&\le \langle \nabla f(x) - \nabla f(y), x-y\rangle \\
		&\lxle{\text{C.S.}} \|\nabla f(x) - \nabla f(y)\| \|x-y\|,
	\end{align*}
	we only have to divide both sides by \(\tfrac{1}\ubound \|\nabla f(x)-\nabla
	f(y)\|\) to get \(\ubound\)-Lipschitz continuity back.
\end{proof}

\section{Momentum Convergence}

\subsection{Eigenvalue Analysis}

\begin{theorem}[\cite{qianMomentumTermGradient1999}]
	\label{thm-appdx: momentum - stable set of parameters}
	Let
	\begin{align*}
		\sigma_{1/2}
		= \tfrac12 \left(
			1+\momCoeff-\lrSq\hesseEV \pm \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\right)
	\end{align*}
	then 
	\begin{enumerate}
		\item \(\max\{|\sigma_1|,|\sigma_2|\}<1\) if and only if
		\begin{align*}
			0<\lrSq\hesseEV < 2(1+\momCoeff) \qquad \text{and} \qquad |\momCoeff|<1
		\end{align*}
		\item The complex case can be characterized by either
		\begin{align*}
			0<(1-\sqrt{\lrSq\hesseEV})^2 < \momCoeff < 1
		\end{align*}		
		or alternatively \(\momCoeff>0\) and
		\begin{align*}
			(1-\sqrt{\momCoeff})^2 < \lrSq\hesseEV < (1+\sqrt{\momCoeff})^2,
		\end{align*}
		for which we have \(|\sigma_1|=|\sigma_2|=\sqrt{\momCoeff}\).
		
		\item In the real case we have \(\sigma_1>\sigma_2\) and
		\begin{align}\label{eq: when does sigma_1 or sigma_2 dominate?}
			\max\{|\sigma_1|, |\sigma_2|\} = \begin{cases}
				|\sigma_1|=\sigma_1 & \lrSq\hesseEV < 1+\momCoeff \\
				|\sigma_2|=-\sigma_2 & \lrSq\hesseEV \ge 1+\momCoeff.
			\end{cases}
		\end{align}
		Restricted to \(1>\momCoeff>0\) this results in two different	
		behaviors. For
		\begin{align*}
			0<\lrSq\hesseEV \le (1-\sqrt{\momCoeff})^2 < 1+\momCoeff
		\end{align*}
		we have \(1>\sigma_1 > \sigma_2 > 0\). For
		\begin{align*}
			1+\momCoeff < (1+\sqrt{\momCoeff})^2\le \lrSq\hesseEV < 2(1+\momCoeff)
		\end{align*}
		on the other hand, we get \(-1 < \sigma_2 < \sigma_1 < 0\).
	\end{enumerate}
\end{theorem}
\begin{proof}
	Define
	\begin{align*}
		\Delta(\momCoeff)
		&:= (1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff\\
		&= \momCoeff^2 - 2(1+\lrSq\hesseEV)\momCoeff + (1-\lrSq\hesseEV)^2
	\end{align*}
	then \(\sigma_{1/2}\) is complex iff \(\Delta(\momCoeff)<0\). Since it is a
	convex parabola this implies that \(\momCoeff\) needs to be between the roots
	\begin{align*}
		\momCoeff_{1/2}
		= (1+\lrSq\hesseEV) \pm 
		\underbrace{
			\sqrt{(1+\lrSq\hesseEV)^2 - (1-\lrSq\hesseEV)^2}
		}_{=\sqrt{4\lrSq\hesseEV}=\mathrlap{2\sqrt{\lrSq\hesseEV}}}
		= (1\pm \sqrt{\lrSq\hesseEV})^2	
	\end{align*}
	of \(\Delta\). Assuming those roots are real which requires \(\lrSq\hesseEV>0\).
	But if \(\lrSq\hesseEV\le0\) then we would have
	\begin{align*}
		\sigma_1
		= \tfrac12 \Big(
			\underbrace{1+\momCoeff+|\lrSq\hesseEV|}_{\ge 1+\momCoeff}
			+ \sqrt{\underbrace{(1+\momCoeff+|\lrSq\hesseEV|)^2 - 4\momCoeff}_{\smash{\ge (1-\momCoeff)^2}}}
		\Big)
		\ge \frac{1+\momCoeff + |1-\momCoeff|}2 \ge 1
	\end{align*}
	which means that \(\max\{|\sigma_1|,|\sigma_2|\}<1\) implies \(\lrSq\hesseEV>0\).
	The roots \(\momCoeff_{1/2}\) are therefore real no matter the direction we
	wish to prove.
	\begin{description}[wide, labelindent=0pt]
	\item[Complex Case:]
		\(\sigma_{1/2}\) are complex iff	
		\begin{align}\label{eq: complex case}
			0 < (1-\sqrt{\lrSq\hesseEV})^2 < \momCoeff < (1+\sqrt{\lrSq\hesseEV})^2.
		\end{align}
		In that case we have
		\begin{align*}
			|\sigma_1| = |\sigma_2|
			&= \sqrt{|\Re(\sigma_1)|^2 + |\Im(\sigma_1)|^2}\\
			&= \tfrac12 \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 + 4\momCoeff - (1+\momCoeff-\lrSq\hesseEV)^2}
			= \sqrt{\momCoeff}.
		\end{align*}
		So in this complex case the condition
		\(\momCoeff<1\) is necessary and sufficient for \(\max\{|\sigma_1|,|\sigma_2|\}<1\).
		And since we have seen that \(\lrSq\hesseEV>0\) is necessary and \(0<\momCoeff\)
		from (\ref{eq: complex case}) implies the condition \(|\momCoeff|<1\), we only
		need to show that \(\lrSq\hesseEV <2(1+\momCoeff)\) is necessary in the complex
		case to have this case covered. Using \((\sqrt{\lrSq\hesseEV}-1)^2 < \momCoeff\)
		from (\ref{eq: complex case}) and \(ab \le a^2 + b^2\) we get
		\begin{align*}
			\sqrt{\lrSq\hesseEV} - 1
			< \momCoeff \implies \lrSq\hesseEV < (1+\sqrt{\momCoeff})^2
			\le 2(1+\momCoeff).
		\end{align*}

		The remaining characterization of the complex case follows from
		restricting (\ref{eq: complex case}) to \(\momCoeff<1\) and reordering.
		\item[Real Case:] Since we have \(\sigma_2 \le \sigma_1\),
		\(\max\{|\sigma_1|,|\sigma_2|\}<1\) is equivalent to
		\begin{align*}
			-1 < \sigma_2 \qquad \text{and} \qquad \sigma_1 < 1
		\end{align*}
		By subtracting the part before the "\(\pm\)" from the equation \(\sigma_1<1\)
		after multiplying it by two, we get
		\begin{align}\label{eq: sigma1 condition}
			0\xle{\text{Real Case}} \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 -4\momCoeff} < 1-\momCoeff +\lrSq\hesseEV.
		\end{align}
		And since \((1+\sqrt{\lrSq\hesseEV})^2 \le \momCoeff\) leads to a contradiction:
		\begin{align*}
			0 \le 1-\momCoeff+\lrSq\hesseEV \le 1-(1+\sqrt{\lrSq\hesseEV})^2 +\lrSq\hesseEV
			= -2\sqrt{\lrSq\hesseEV} \ge 0
		\end{align*}
		the real case is restricted to the case \(\momCoeff < (1-\sqrt{\lrSq\hesseEV})^2\)
		as we are otherwise in the complex case (\ref{eq: complex case}).
		This means that due to
		\begin{align*}
			1-\momCoeff+\lrSq\hesseEV
			> 1 - (1-\sqrt{\lrSq\hesseEV})^2 + \lrSq\hesseEV
			= 2\sqrt{\lrSq\hesseEV}>0
		\end{align*}
		the inequality (\ref{eq: sigma1 condition}) is (in the real case) equivalent to
		\begin{align*}
			&(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff < (1-\momCoeff+\lrSq\hesseEV)^2\\
			&\iff \cancel{1^2} + 2(\momCoeff-\lrSq\hesseEV) + 
			\cancel{(\momCoeff-\lrSq\hesseEV)^2} - 4\momCoeff
			< \cancel{1^2} - 2(\momCoeff-\lrSq\hesseEV) + 
			\cancel{(\momCoeff-\lrSq\hesseEV)^2}\\
			&\iff 0 < 4\lrSq\momCoeff.
		\end{align*}
		Which is a given for positive learning rates in the convex	
		case. This is not surprising if one remembers the proof from gradient
		decent. The real issue is not overshooting \(-1\) with large learning
		rates. So multiplying \(-1<\sigma_2\) by two, adding 2 and moving the
		root to the other side we get
		\begin{align}\label{eq: from -1<sigma_2 followed}
			0 \xle{\text{Real Case}} \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
			< 3+\momCoeff-\lrSq\hesseEV
		\end{align}
		Now if we assume \(\lrSq\hesseEV < 3+\momCoeff\), then this condition is
		equivalent to 
		\begin{align}
			\nonumber
			&(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff < (3+\momCoeff-\lrSq\hesseEV)\\
			\nonumber
			&\iff 1^2 + 2(\momCoeff-\lrSq\hesseEV) + \cancel{(\momCoeff-\lrSq\hesseEV)^2} - 4\momCoeff
			< 3^2 + 6(\momCoeff-\lrSq\hesseEV) + \cancel{(\momCoeff-\lrSq\hesseEV)^2}\\
			\label{eq: momentum upper bound derivation}
			&\iff 4\lrSq\hesseEV < 8 + 8\momCoeff
			\iff \lrSq\hesseEV < 2(1+\momCoeff)
		\end{align}
		which is the important requirement on the learning rate we are looking for.
		Now for \(\momCoeff<1\) this requirement is actually stronger than
		\(\lrSq\hesseEV <3+\momCoeff\) which means that we have proven that our
		requirements are sufficient for \(\max\{|\sigma_1|,|\sigma_2|\}<1\).
		What is left to show is that \(|\momCoeff|<1\) is also necessary in the real
		case. Now from (\ref{eq: momentum upper bound derivation}) we immediately
		get
		\begin{align*}
			0 \le \tfrac{\lrSq\hesseEV}2 < 1+\momCoeff
		\end{align*}
		which implies \(-1 < \momCoeff\). At first glance it might look like
		\(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) is already sufficient for the upper
		bound, but for \(\lrSq\hesseEV>4\) this bound is greater than one again.
		So let us assume \(\lrSq\hesseEV>2\) and \(\momCoeff>0\), then
		\(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) implies a contradiction:
		\begin{align*}
			\sqrt{\momCoeff} < \sqrt{\lrSq\momCoeff} -1
			&\implies \smash{\overbrace{(1+\sqrt{\momCoeff})^2}^{1+2\sqrt{\momCoeff} + \momCoeff}}
			< \lrSq\hesseEV
			\stackrel{(\ref{eq: from -1<sigma_2 followed})}{<} 3+\momCoeff \\
			&\implies -2 +2\sqrt{\momCoeff} < 0
			\implies \sqrt{\momCoeff} < 1
		\end{align*}

		For the \emph{characterization of the real case} one only needs to observe
		that for any \(b>0\)
		\begin{align}
			\max|a\pm b| = \begin{cases}
				a + b & a \ge 0\\
				-(a-b) & a < 0
			\end{cases}.
		\end{align}
		In our case \(b\) is a square root in the real case, so by definition
		positive and \(a>0\) is equivalent to \(1+\momCoeff - \lrSq\hesseEV >0\)
		and thus implies (\ref{eq: when does sigma_1 or sigma_2 dominate?}).
		
		For \(\momCoeff>0\) one only needs to keep in mind that for
		 \(\lrSq\hesseEV < (1+\sqrt{\momCoeff})^2 <1+\momCoeff\)
		\begin{align*}
			1+\momCoeff-\lrSq\hesseEV = \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2}
			> \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\end{align*}
		ensures that \(\sigma_2 >0\). On the other hand for \(\momCoeff>0\)
		and \(\lrSq\hesseEV > (1-\sqrt{\momCoeff})^2> 1+\momCoeff\)
		\begin{align*}
			- (1+\momCoeff-\lrSq\hesseEV) = \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2}
			> \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\end{align*}
		implies \(\sigma_1 < 0\). \qedhere
 \end{description}
\end{proof}

\begin{lemma}\label{lem-appdx: just at the border of complex case is best beta}
	\begin{align*}
		\frac{d}{d\momCoeff}\max\{|\sigma_1|,|\sigma_2|\} < 0 \qquad
		\forall -1<\momCoeff <(1-\sqrt{\lrSq\hesseEV})^2
	\end{align*}
\end{lemma}
\begin{proof}
	Using the characterizations of the real case
	from Theorem~\ref{thm: momentum - stable set of parameters} we get
	\begin{align*}
		\frac{d}{d\momCoeff}\max\{|\sigma_1|,|\sigma_2|\}
		= \begin{cases}
			\frac{d\sigma_1}{d\momCoeff} & \momCoeff \ge \lrSq\hesseEV -1 \\
			-\frac{d\sigma_2}{d\momCoeff} & \momCoeff \le \lrSq\hesseEV -1 
		\end{cases}
	\end{align*}
	This allows us to treat those cases separately. For the first case we use
	the same definition for \(\Delta\) as in Theorem~\ref{thm: momentum - stable
	set of parameters}. Calculating its derivative
	\begin{align*}
		\Delta'(\momCoeff) = 2(1+\momCoeff-\lrSq\hesseEV)	-4
	\end{align*}
	allows us to represent \(\sigma_1\) as
	\begin{align*}
		1>\sigma_1
		= 1 + \frac{\Delta'(\momCoeff)}{4} + \tfrac12 \sqrt{\Delta(\momCoeff)},
	\end{align*}
	which results in the inequality
	\begin{align*}
		0 > \tfrac14\Delta'(\momCoeff) + \tfrac12\sqrt{\Delta(\momCoeff)}
		\implies -\tfrac12 > \tfrac14\frac{\Delta'(\momCoeff)}{\sqrt{\Delta{\momCoeff}}}
	\end{align*}
	We can now use this inequality to show that\footnote{
		I want to apologize for the lack of insight in this proof upon which I
		stumbled by accident playing around with the \(\Delta(\momCoeff)\)
		representation of \(\sigma_{1/2}\). Its only redeeming merit is its brevity.
	}
	\begin{align*}
		\frac{d\sigma_1}{d\momCoeff}
		= \frac12 + \frac14 \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}} < 0.
	\end{align*}
	The second case on the other hand requires us to show that
	\begin{align*}
		\frac{d\sigma_2}{d\momCoeff}
		= \frac12 - \frac14 \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}} > 0
		\iff 2 > \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}}.
	\end{align*}
	Which is true due to
	\begin{align*}
		2\sqrt{\Delta(\momCoeff)} > 0 > -4\sqrt{\lrSq\hesseEV}
		&= 2\overbrace{
			(1-\lrSq\hesseEV + (1-\sqrt{\lrSq\hesseEV})^2)
		}^{\smash{=2-2\sqrt{\lrSq\hesseEV}}} -4\\
		&= \Delta'((1-\sqrt{\lrSq\hesseEV})^2) > \Delta'(\momCoeff),
	\end{align*}
	since \(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) and \(\Delta'\) is monotonously
	increasing in \(\momCoeff\).
\end{proof}

\subsection{Nesterov's Momentum Convergence}

\begin{lemma}[Dumpster Dive]
	\label{lem-appendix: dumpster dive}
	The conditions
	\begin{align*}
		\ubound\gamma_n^2 \le \lbound_n
	\end{align*}
	and
	\begin{align*}
		y_n
		&= \tfrac{\lbound_n\weights_{n-1} + \lbound_{n-1}\gamma_n z_{n-1}}{\gamma_n\lbound + \lbound_{n-1}}
	\end{align*}
	are sufficient for lower bounding
	\begin{align*}
		J := (1-\gamma_n)\left(
			\langle \Loss(y_n), \weights_{n-1} - y_n\rangle
			+ \tfrac{\lbound_{n-1}}{2}\|y_n - z_{n-1}\|^2
		\right)
		- \tfrac{\lbound_n}{2}\|y_n - z_n\|^2
	\end{align*}
	by \(- \tfrac{1}{2\ubound}\|\nabla\Loss(y_n)\|^2\).
\end{lemma}
\begin{proof}
	By definition \(z_n\) is the unique minimum of \(\Phi_n\) so we can find
	it with
	\begin{align*}
		0\xeq{!}\Phi'_n(z_n)
		= \gamma_n\underbrace{\phi_n'(z_n)}_{
			= \nabla\Loss(y_n) \mathrlap{+ \lbound(z_n-y_n)}
		} +(1-\gamma_n)
		\underbrace{\Phi'_{n-1}(z_n)}_{
			=\lbound_{n-1}\mathrlap{(z_n-z_{n-1})}
		},
	\end{align*}
	where we have used the representation (\ref{eq: centered Phi representation})
	for \(\Phi_{n-1}\). It is also quite reassuring that the gradient at \(y_n\)
	which we ultimately need makes its first appearance here. Collecting all
	the \(z_n\) we get
	\begin{align}\label{eq: center of Phi recursion}
		\overbrace{\lbound_n}^{
			=\mathrlap{\gamma_n\lbound + (1-\gamma_n)\lbound_{n-1}}
		}z_n
		= \gamma_n\lbound y_n + (1-\gamma_n)\lbound_n z_{n-1}
		- \gamma_n\nabla\Loss(y_n).
	\end{align}
	Subtracting \(\lbound_n y_n\) results in
	\begin{align*}
		\lbound_n(z_n-y_n)
		= (1-\gamma_n)\lbound_{n-1}(z_{n-1}-y_n)
		- \gamma_n\nabla\Loss(y_n).
	\end{align*}
	Plugging this into the last part of our ``junk'', we get
	\begin{align*}
		\tfrac{\lbound_n}2\|y_n-z_n\|^2
		&= \tfrac1{2\lbound_n}
		\underbrace{
			\|{\scriptstyle (1-\gamma_n)\lbound_{n-1}}(z_{n-1}-y_n)
			- {\scriptstyle \gamma_n}\nabla\Loss(y_n)\|^2
		}_{
		\begin{aligned}
			=&{\scriptstyle (1-\gamma_n)^2\lbound_{n-1}^2}\|z_{n-1}-y_n\|^2 \\
			&- {\scriptstyle 2(1-\gamma_n)\lbound_{n-1}\gamma_n}\langle
			\nabla\Loss(y_n), z_{n-1}-y_n\rangle \\
			&+ {\scriptstyle\gamma_n^2}\|\nabla\Loss(y_n)\|^2
		\end{aligned}
		}
	\end{align*}
	Therefore our ``junk'' is equal to
	\begin{align*}
		-\tfrac{\gamma_n^2}{2\lbound_n}\|\nabla\Loss(y_n)\|^2
		\begin{aligned}[t]
			&+ {\scriptstyle(1-\gamma_n)}\left\langle \Loss(y_n),
			(\weights_n -y_n)
			+ \tfrac{\lbound_{n-1}\gamma_n}{\lbound_n}(z_{n-1} - y_n)
			\right\rangle\\
			&+\tfrac{(1-\gamma_n)\lbound_{n-1}}{2}
			\underbrace{\left(1-\tfrac{(1-\gamma_n)\lbound_{n-1}}{\lbound_n}\right)}_{
				= \tfrac{\gamma_n \lbound}{\lbound_n} \ge 0
			}
			\|y_n - z_{n-1}\|^2.
		\end{aligned}
	\end{align*}
	As the last summand is positive we can discard it and end up with two equations
	sufficient to achieve our lower bound
	\begin{align*}
		-\tfrac{\gamma_n^2}{\lbound_n}
		&\ge -\tfrac{1}{\ubound} \iff \ubound\gamma_n^2 \le \lbound_n
		\\
		0
		&= (\weights_{n-1} -y_n) + \tfrac{\lbound_n\gamma_n}{\lbound_n}(z_{n-1} - y_n)
		\xiff{\text{reorder}}
		y_n
		= \tfrac{\lbound_n\weights_{n-1} + \lbound_{n-1}\gamma_n z_{n-1}}{\gamma_n\lbound + \lbound_{n-1}}
		\qedhere
	\end{align*}
\end{proof}

\begin{lemma}\label{lem-appendix: streamline general schema of optimal methods}
	If we use the recursion
	\begin{align}
		\weights_n = y_n - \tfrac1\ubound\Loss(y_n)
	\end{align}
	in Algorithm~\ref{algo: general schema of optimal methods} line~\ref{line:
	selecting next position}, then we can simplify \(z_n\) and \(y_n\)
	to
	\begin{align}
		\label{eq: z_n recursion}
		z_n
		&= \weights_{n-1} + \tfrac1{\gamma_n}(\weights_n-\weights_{n-1})
		\qquad \forall n\ge 1\\
		y_{n+1}
		&=\weights_n + \momCoeff_n(\weights_n-\weights_{n-1}) \qquad \forall n\ge 0
	\end{align}
	for \(\weights_{-1}:=\weights_0=z_0\) and
	\begin{align}
		\momCoeff_n
		= \frac{\lbound_n \gamma_{n+1}(1-\gamma_n)}{\gamma_n(\gamma_{n+1}\lbound +\lbound_n)}
		= \frac{\gamma_n(1-\gamma_n)}{\gamma_n^2 + \gamma_{n+1}}
	\end{align}
\end{lemma}
\begin{proof}
	After the assignment in line~\ref{line: definition of y_n} we can reorder it
	as an equation to get
	\begin{align*}
		\lbound_{n-1} z_{n-1}
		= \tfrac{1}{\gamma_n} [(\gamma_n\lbound + \lbound_{n-1})y_n - \lbound_n\weights_{n-1}]
	\end{align*}
	plugging this into our definition of \(z_n\) results in our first claim
	\begin{align*}
		z_n
		&= \tfrac{1}{\lbound_n}
		\left[
			\gamma_n\lbound y_n
			+ \tfrac{(1-\gamma_n)}{\gamma_n}
			[(\gamma_n\lbound + \lbound_{n-1})y_n - \lbound_n\weights_{n-1}]
			- \gamma_n\nabla\Loss(y_n)
		\right]\\
		&=\tfrac{1}{\lbound_n} \underbrace{\Big[
			\underbrace{\gamma_n\lbound + (1-\gamma_n)\lbound}_{=\lbound}
			+ \tfrac{1-\gamma_n}{\gamma_n}\lbound_{n-1}
		\Big]}_{
			=\lbound_n/\gamma_n
		}y_n
		-\tfrac{1-\gamma_n}{\gamma_n}\weights_{n-1}
		-\underbrace{\tfrac{\gamma_n}{\lbound_n}}_{
			=\frac{1}{\ubound\gamma_n} \mathrlap{\ (L\gamma_n^2 = \lbound_n)}
		}\nabla\Loss(y_n)\\
		&= \weights_{n-1} + \tfrac{1}{\gamma_n}(y_n - \weights_{n-1})
		- \tfrac{1}{\ubound\gamma_n}\nabla\Loss(y_n)\\
		&\lxeq{(\ref{eq-lem-assmpt: weigth recursion})}
		\weights_{n-1} + \tfrac{1}{\gamma_n}(\weights_n-\weights_{n-1})
	\end{align*}
	Now we are going to rewrite \(y_{n+1}\):
	\begin{align*}
		y_{n+1}
		&= \frac{
			\overbrace{[\gamma_{n+1} \lbound + (1-\gamma_{n+1})\lbound_n]}^{\lbound_{n+1}}\weights_n
			+ \lbound_n \gamma_{n+1} z_n
		}{\gamma_{n+1}\lbound +\lbound_n}
		=\weights_n
		+ \frac{\lbound_n \gamma_{n+1} (z_n - \weights_n)}{\gamma_{n+1}\lbound +\lbound_n}
	\end{align*}
	For \(n\ge1\) we can apply our previous result (\ref{eq: z_n recursion})
	\begin{align*}
		z_n - \weights_n
		= \left(\tfrac1{\gamma_n} -1\right)(\weights_n-\weights_{n-1}),
	\end{align*}
	for \(n=0\) both sides are zero and we therefore obtain our second recursion
	\begin{align*}
		y_{n+1}
		=\weights_n
		+ \underbrace{
			\frac{\lbound_n \gamma_{n+1}(1-\gamma_n)}{\gamma_n(\gamma_{n+1}\lbound +\lbound_n)}
		}_{=\momCoeff_n}
		(\weights_n-\weights_{n-1}).
	\end{align*}
	Now we just use the identity \(\gamma_{n+1}\lbound = \ubound\gamma_{n+1}^2 -
	(1-\gamma_{n+1})\lbound_n\) from the definition of \(\gamma_{n+1}\) to simplify our
	momentum coefficient
	\begin{align*}
		\momCoeff_n
		&= \frac{\lbound_n \gamma_{n+1}(1-\gamma_n)}{
			\gamma_n([\ubound\gamma_{n+1}^2 - (\cancel{1}-\gamma_{n+1})\lbound_n] +\cancel{\lbound_n})
		}
		= \frac{\lbound_n\cancel{\gamma_{n+1}}(1-\gamma_n)}{
			\gamma_n\cancel{\gamma_{n+1}}(\ubound\gamma_{n+1} + \underbrace{\lbound_n}_{=\ubound\gamma_n^2})
		}\\
		&\lxeq{\bcancel{\ubound\gamma_n}} \frac{\gamma_n(1-\gamma_n)}{\gamma_{n+1} + \gamma_n^2}
		\qedhere
	\end{align*}
\end{proof}

\begin{lemma}[{\cite[Lemma 2.2.4]{nesterovLecturesConvexOptimization2018}}]
	\label{lem-appendix: convergence rate bounds for estimating sequences}
	For \(\lbound_0\in (\lbound, 3\ubound + \lbound]\) we have
	\begin{align*}
		\Gamma^n
		\le \frac{4}{\left(\frac{\condition}{\condition_0}-1\right)\left[
			\exp\left(\frac{n+1}{2\sqrt{\condition}}\right)
			-\exp\left(-\frac{n+1}{2\sqrt{\condition}}\right)
		\right]}
		\le \frac{4}{(\condition_0^{-1}-\condition^{-1})(n+1)^2}
	\end{align*}
\end{lemma}
\begin{proof}

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput