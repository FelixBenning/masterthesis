% !TEX root = ../Masterthesis.tex

\chapter{Technical Proofs}

\section{Non-Trivial Basics}

\subsection{Convex Analysis}

\begin{lemma}\label{lem-appendix: lipschitz and bounded derivative}
	If \(f\) is differentiable, then the derivative \(\nabla f\) is
	bounded (w.r.t. the operator norm) by constant \(\lipConst\) if and only if the function
	\(f\) is \(\lipConst\)-Lipschitz continuous.
\end{lemma}
\begin{proof}
	Lipschitz continuity is implied by the mean value theorem
	\begin{align*}
		\|f(x_1) - f(x_0)\|
		&\le \|\nabla f(x_0 + \xi(x_1-x_0))\| \|x_1- x_0\|\\
		&\le \lipConst \|x_1-x_0\|.
	\end{align*}
	%
	The opposite direction is implied by
	%
	\begin{align*}
		\|\nabla f(x_0)\|
		&\equiv \sup_{v} \frac{\|\nabla f(x_0)v\|}{\|v\|}
		= \sup_{v} \lim_{\lambda\to 0}\frac{|\lambda|\|\nabla f(x_0) v\|}{|\lambda|\|v\|}\\
		&\lxle{\Delta}\sup_{v} \lim_{\lambda\to 0}
		(
			\underbrace{
				\tfrac{\|\nabla f(x_0)\lambda v + f(x_0) - f(x_0 +\lambda v)\|}{\|\lambda v\|}
			}_{
				\to 0 \text{ (derivative definition)}
			}
			+ \underbrace{
				\tfrac{\|f(x_0 + \lambda v) - f(x_0) \|}{\|\lambda v\|}
			}_{
				\le \lipConst 
			}
		)
	\end{align*}
	%
	where we have used the scalability of the norm to multiply \(v\) with a
	decreasing factor both in the numerator and denominator in order to introduce
	the limit.
\end{proof}

\begin{lemma}
	\label{Appdx-lem: Lipschitz Gradient implies taylor inequality}
	If \(\nabla f\) is \(\ubound\)-Lipschitz continuous, then
	\begin{align*}
		|f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}
	If \(f\) is convex, then the opposite direction is also true.
\end{lemma}
\begin{proof}
	The first direction is taken from \textcite[Lemma
	1.2.3]{nesterovLecturesConvexOptimization2018}.
 \begin{align*}
		f(y) = f(x) + \int_0^1\langle\nabla f(x+\tau(y-x)), y-x \rangle d\tau
	\end{align*}
	implies (using the  Cauchy-Schwarz inequality)
	\begin{align*}
		&| f(y) - f(x) - \langle \nabla f(x), y-x\rangle | \\
		&\le \int_0^1 | \langle\nabla f(x+\tau(y-x))-\nabla f(x), y-x\rangle | d\tau \\
		&\lxle{\text{C.S.}}
		\int_0^1 \|\langle\nabla f(x+\tau(y-x))-\nabla f(x)\| \cdot \|y-x\| d\tau\\
		&\le \int_0^1 \ubound \|\tau(y-x)\|\cdot\|y-x\| d\tau
		= \tfrac{\ubound}2 \|y-x\|^2.
	\end{align*}
	The opposite direction is taken from \textcite[Lemma
	2.1.5]{nesterovLecturesConvexOptimization2018}.
	As we have only used
	\begin{align*}
		0\xle{\text{Convexity}} f(y) - f(x) - \langle \nabla f(x), y-x\rangle
		\le \tfrac{\ubound}2 \|y-x\|^2
	\end{align*}	
	in Lemma~\ref{lem: bregmanDiv lower bound} (and Lemma~\ref{lem: smallest upper
	bound} which was used in the proof), we know that equation (\ref{eq:
	bregmanDiv lower bound b}) follows from it and after applying Cauchy-Schwarz
	to this equation
	\begin{align*}
		\tfrac{1}\ubound \|\nabla f(x)-\nabla f(y)\|^2
		&\le \langle \nabla f(x) - \nabla f(y), x-y\rangle \\
		&\lxle{\text{C.S.}} \|\nabla f(x) - \nabla f(y)\| \|x-y\|,
	\end{align*}
	we only have to divide both sides by \(\tfrac{1}\ubound \|\nabla f(x)-\nabla
	f(y)\|\) to get \(\ubound\)-Lipschitz continuity back.
\end{proof}

\subsection{Sequences}

\begin{theorem}[Cesàro-Stolz]\label{thm-appendix: cesaro-stolz}
	Let \((x_n, n\in\naturals),(y_n, n\in\naturals)\) be real sequences with
	\(\lim_{n\to\infty}y_n\to\infty\), then for \(\Delta x_n:=x_{n+1}-x_n\)
	\begin{align}
		\liminf_n \frac{\Delta x_n}{\Delta y_n}
		\le \liminf_n \frac{x_n}{y_n} 
		\le \limsup_n \frac{x_n}{y_n} 
		\le \limsup_n \frac{\Delta x_n}{\Delta y_n}
	\end{align}
	In particular if the limit \(\lim_n\frac{\Delta x_n}{\Delta y_n}\) exists
	we have equality. Thereby it becomes a discrete version of L'Hôpital's
	rule.
\end{theorem}
\begin{proof}[{Proof \parencite{IMOmathHopitalTheorem}}]
	To show the first inequality let us assume we have
	\begin{align*}
		\gamma < \liminf_n \frac{\Delta x_n}{\Delta y_n}.
	\end{align*}
	Then by definition of the limes inferior there exists \(N\) such that
	\begin{align*}
		\gamma \Delta y_n < \Delta x_n \qquad \forall n\ge N
	\end{align*}
	Using telescoping sums this implies
	\begin{align*}
		\gamma (y_m- y_N)
		= \gamma \sum_{n=N}^{m-1} \Delta y_n
		< \sum_{n=N}^{m-1} \Delta x_n
		= x_m- x_N.
	\end{align*}
	Dividing both sides by \(y_m\) and taking the limes inferior keeping in
	mind that \(y_m\to\infty\) results in
	\begin{align*}
		\gamma = \liminf_m \gamma\left(\frac{y_m}{y_m} - \frac{y_N}{y_m}\right)
		< \liminf_m \frac{x_m}{y_m} - \frac{x_N}{y_m}
		= \liminf_m \frac{x_m}{y_m}.
	\end{align*}
	Since \(\gamma\) was arbitrary this proves the first inequality. The second
	inequality is true by definition of the limes inferior (superior) and the
	third inequality can be proved in the same way as the first starting with
	\(\limsup_n \frac{\Delta x_n}{\Delta y_n} < \gamma\).
\end{proof}
\begin{lemma}
	\label{lem-appendix: diminishing contraction}
	Let \(a_0 \in [0, 1/q)\) for \(q>0\) and assume 
	\begin{align*}
		a_{n+1} = (1-q a_n)a_n \quad \forall n \ge 0,
	\end{align*}
	then we have
	\begin{align}
		\lim_{n\to\infty} n a_n = 1/q
	\end{align}
\end{lemma}
\begin{proof}
	The proof follows \textcite{israelHowWorkOut2012}. By induction we have
	\begin{align*}
		0\le a_n \le a_{n-1} < 1/q.
	\end{align*}
	Therefore \(a_n\) is a falling bounded sequences and thus converges to
	some \(a\in[0,1/q)\) which necessarily fulfills
		\(a(1-qa) = a\)
	which implies
	\begin{align*}
		\lim_{n\to\infty} a_n = a = 0
	\end{align*}
	Thus for \(y_n := 1/a_n\) and \(x_n:=n\) we have
	\begin{align*}
		\frac{\Delta x_n}{\Delta y_n}
		= \frac{1}{\frac{1}{a_{n+1}}-\frac1{a_n}}
		= \frac{a_{n+1}a_n}{a_n - a_{n+1}}
		= \frac{(1-q a_n)a_n}{1 - (1-q a_n)}
		= \frac{1-q a_n}{q} \to 1/q
	\end{align*}
	and since \(y_n\to\infty\) we can apply Theorem~\ref{thm-appendix: cesaro-stolz}
	to get
	\begin{align*}
		\lim_{n\to\infty} n a_n
		&= \lim_{n\to\infty} \frac{x_n}{y_n}
		= \lim_{n\to\infty} \frac{\Delta x_n}{\Delta y_n}
		= 1/q.
		\qedhere
	\end{align*}
\end{proof}

\begin{lemma}[Discrete Gr\"onwall]\label{lem-appendix: discrete gronwall}
	Let \(g_n, a_n, b_n \ge 0\), \(g_0=0\) and
	\begin{align*}
		g_{n+1} \le a_n g_n + b_n
	\end{align*}
	then we have
	\begin{align*}
		g_n \le \sum_{k=0}^{n-1}b_k\prod_{i=k+1}^{n-1}a_i
	\end{align*}
\end{lemma}
\begin{proof}
	By induction where \(n=0\) is trivial interpreting the empty sum as zero, we have	
	\begin{align*}
		g_{n+1}
		&\le a_n g_n + b_n
		\xle{\text{ind.}} a_n \left(\sum_{k=0}^{n-1}b_k\prod_{i=k+1}^{n-1}a_i\right) + b_n
		\le \sum_{k=0}^{n}b_k\prod_{i=k+1}^{n}a_i
		\qedhere
	\end{align*}
\end{proof}

\subsection{Operators}

\begin{lemma}[Absolute Values Inside Operator Norms]
	\label{lem-appdx: absolute value inside operator norms}
	For complex matrices \(A^{(1)},\dots,A^{(n)}\in\complex^{\dimension\times\dimension}\)
	we define \(B^{(1)},\dots,B^{(n)}\in\complex^{\dimension\times\dimension}\)
	with
	\begin{align*}
		B^{(k)}_{ij} := |A^{(k)}_{ij}| \qquad \forall k,i,j.
	\end{align*}
	Then we have
	\begin{align*}
		\Bigg\|\prod_{k=1}^n A^{(k)}\Bigg\| \le \Bigg\|\prod_{k=1}^n B^{(k)}\Bigg\|.
	\end{align*}
\end{lemma}
\begin{proof}
	Let \(x\in\complex^\dimension\) be an arbitrary vector and define \(y\in\complex^\dimension\)
	with
	\begin{align*}
		y_i = |x_i| \qquad \forall i=1,\dots,\dimension.
	\end{align*}
	Then we have
	\begin{align*}
		\Bigg\| \prod_{k=1}^n A^{(k)} x \Bigg\|
		&= \sum_{j=1}^{d} \Big| \sum_{1\le i_1,\dots, i_n \le d}
		A^{(1)}_{j i_1} A^{(2)}_{i_1 i_2} \dots A^{(n)}_{i_{n-1} i_n} x_{i_n} \Big|^2
		\\
		&\le \sum_{j=1}^{d} \Big| \sum_{1\le i_1,\dots, i_n \le d}
		B^{(1)}_{j i_1} B^{(2)}_{i_1 i_2} \dots B^{(n)}_{i_{n-1} i_n} y_{i_n} \Big|^2
		\\
		&= \Bigg\| \prod_{k=1}^n B^{(k)} y \Bigg\|
	\end{align*}
	Since we have \(\|x\|=\|y\|\) as the norm takes the absolute value anyway, we
	get our claim
	\begin{align*}
		\Bigg\|\prod_{k=1}^n A^{(k)}\Bigg\|
		&= \sup_{\|x\|=1}
		\Bigg\|\prod_{k=1}^n A^{(k)} x\Bigg\|
		\le \sup_{\|y\|=1}
		\Bigg\|\prod_{k=1}^n B^{(k)} y\Bigg\|
		= \Bigg\|\prod_{k=1}^n B^{(k)}\Bigg\|.
		\qedhere
	\end{align*}
\end{proof}

\section{Momentum Convergence}

\subsection{Eigenvalue Analysis}

\begin{theorem}[\cite{qianMomentumTermGradient1999}]
	\label{thm-appdx: momentum - stable set of parameters}
	Let
	\begin{align*}
		\momEV_{1/2}
		= \tfrac12 \left(
			1+\momCoeff-\lrSq\hesseEV \pm \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\right)
	\end{align*}
	then 
	\begin{enumerate}
		\item \(\max\{|\momEV_1|,|\momEV_2|\}<1\) if and only if
		\begin{align*}
			0<\lrSq\hesseEV < 2(1+\momCoeff) \qquad \text{and} \qquad |\momCoeff|<1
		\end{align*}
		\item The complex case can be characterized by either
		\begin{align*}
			0<(1-\sqrt{\lrSq\hesseEV})^2 < \momCoeff < 1
		\end{align*}		
		or alternatively \(\momCoeff>0\) and
		\begin{align*}
			(1-\sqrt{\momCoeff})^2 < \lrSq\hesseEV < (1+\sqrt{\momCoeff})^2,
		\end{align*}
		for which we have \(|\momEV_1|=|\momEV_2|=\sqrt{\momCoeff}\).
		
		\item In the real case we have \(\momEV_1>\momEV_2\) and
		\begin{align}\label{eq: when does sigma_1 or sigma_2 dominate?}
			\max\{|\momEV_1|, |\momEV_2|\} = \begin{cases}
				|\momEV_1|=\momEV_1 & \lrSq\hesseEV < 1+\momCoeff \\
				|\momEV_2|=-\momEV_2 & \lrSq\hesseEV \ge 1+\momCoeff.
			\end{cases}
		\end{align}
		Restricted to \(1>\momCoeff>0\) this results in two different	
		behaviors. For
		\begin{align*}
			0<\lrSq\hesseEV \le (1-\sqrt{\momCoeff})^2 < 1+\momCoeff
		\end{align*}
		we have \(1>\momEV_1 > \momEV_2 > 0\). For
		\begin{align*}
			1+\momCoeff < (1+\sqrt{\momCoeff})^2\le \lrSq\hesseEV < 2(1+\momCoeff)
		\end{align*}
		on the other hand, we get \(-1 < \momEV_2 < \momEV_1 < 0\).
	\end{enumerate}
\end{theorem}
\begin{proof}
	Define
	\begin{align*}
		\Delta(\momCoeff)
		&:= (1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff\\
		&= \momCoeff^2 - 2(1+\lrSq\hesseEV)\momCoeff + (1-\lrSq\hesseEV)^2
	\end{align*}
	then \(\momEV_{1/2}\) is complex iff \(\Delta(\momCoeff)<0\). Since it is a
	convex parabola this implies that \(\momCoeff\) needs to be between the roots
	\begin{align*}
		\momCoeff_{1/2}
		= (1+\lrSq\hesseEV) \pm 
		\underbrace{
			\sqrt{(1+\lrSq\hesseEV)^2 - (1-\lrSq\hesseEV)^2}
		}_{=\sqrt{4\lrSq\hesseEV}=\mathrlap{2\sqrt{\lrSq\hesseEV}}}
		= (1\pm \sqrt{\lrSq\hesseEV})^2	
	\end{align*}
	of \(\Delta\). Assuming those roots are real which requires \(\lrSq\hesseEV>0\).
	But if \(\lrSq\hesseEV\le0\) then we would have
	\begin{align*}
		\momEV_1
		= \tfrac12 \Big(
			\underbrace{1+\momCoeff+|\lrSq\hesseEV|}_{\ge 1+\momCoeff}
			+ \sqrt{\underbrace{(1+\momCoeff+|\lrSq\hesseEV|)^2 - 4\momCoeff}_{\smash{\ge (1-\momCoeff)^2}}}
		\Big)
		\ge \frac{1+\momCoeff + |1-\momCoeff|}2 \ge 1
	\end{align*}
	which means that \(\max\{|\momEV_1|,|\momEV_2|\}<1\) implies \(\lrSq\hesseEV>0\).
	The roots \(\momCoeff_{1/2}\) are therefore real no matter the direction we
	wish to prove.
	\begin{description}[wide, labelindent=0pt]
	\item[Complex Case:]
		\(\momEV_{1/2}\) are complex iff	
		\begin{align}\label{eq: complex case}
			0 < (1-\sqrt{\lrSq\hesseEV})^2 < \momCoeff < (1+\sqrt{\lrSq\hesseEV})^2.
		\end{align}
		In that case we have
		\begin{align*}
			|\momEV_1| = |\momEV_2|
			&= \sqrt{|\Re(\momEV_1)|^2 + |\Im(\momEV_1)|^2}\\
			&= \tfrac12 \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 + 4\momCoeff - (1+\momCoeff-\lrSq\hesseEV)^2}
			= \sqrt{\momCoeff}.
		\end{align*}
		So in this complex case the condition
		\(\momCoeff<1\) is necessary and sufficient for \(\max\{|\momEV_1|,|\momEV_2|\}<1\).
		And since we have seen that \(\lrSq\hesseEV>0\) is necessary and \(0<\momCoeff\)
		from (\ref{eq: complex case}) implies the condition \(|\momCoeff|<1\), we only
		need to show that \(\lrSq\hesseEV <2(1+\momCoeff)\) is necessary in the complex
		case to have this case covered. Using \((\sqrt{\lrSq\hesseEV}-1)^2 < \momCoeff\)
		from (\ref{eq: complex case}) and \(ab \le a^2 + b^2\) we get
		\begin{align*}
			\sqrt{\lrSq\hesseEV} - 1
			< \momCoeff \implies \lrSq\hesseEV < (1+\sqrt{\momCoeff})^2
			\le 2(1+\momCoeff).
		\end{align*}

		The remaining characterization of the complex case follows from
		restricting (\ref{eq: complex case}) to \(\momCoeff<1\) and reordering.
		\item[Real Case:] Since we have \(\momEV_2 \le \momEV_1\),
		\(\max\{|\momEV_1|,|\momEV_2|\}<1\) is equivalent to
		\begin{align*}
			-1 < \momEV_2 \qquad \text{and} \qquad \momEV_1 < 1
		\end{align*}
		By subtracting the part before the "\(\pm\)" from the equation \(\momEV_1<1\)
		after multiplying it by two, we get
		\begin{align}\label{eq: sigma1 condition}
			0\xle{\text{Real Case}} \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 -4\momCoeff} < 1-\momCoeff +\lrSq\hesseEV.
		\end{align}
		And since \((1+\sqrt{\lrSq\hesseEV})^2 \le \momCoeff\) leads to a contradiction:
		\begin{align*}
			0 \le 1-\momCoeff+\lrSq\hesseEV \le 1-(1+\sqrt{\lrSq\hesseEV})^2 +\lrSq\hesseEV
			= -2\sqrt{\lrSq\hesseEV} \ge 0
		\end{align*}
		the real case is restricted to the case \(\momCoeff < (1-\sqrt{\lrSq\hesseEV})^2\)
		as we are otherwise in the complex case (\ref{eq: complex case}).
		This means that due to
		\begin{align*}
			1-\momCoeff+\lrSq\hesseEV
			> 1 - (1-\sqrt{\lrSq\hesseEV})^2 + \lrSq\hesseEV
			= 2\sqrt{\lrSq\hesseEV}>0
		\end{align*}
		the inequality (\ref{eq: sigma1 condition}) is (in the real case) equivalent to
		\begin{align*}
			&(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff < (1-\momCoeff+\lrSq\hesseEV)^2\\
			&\iff \cancel{1^2} + 2(\momCoeff-\lrSq\hesseEV) + 
			\cancel{(\momCoeff-\lrSq\hesseEV)^2} - 4\momCoeff
			< \cancel{1^2} - 2(\momCoeff-\lrSq\hesseEV) + 
			\cancel{(\momCoeff-\lrSq\hesseEV)^2}\\
			&\iff 0 < 4\lrSq\momCoeff.
		\end{align*}
		Which is a given for positive learning rates in the convex	
		case. This is not surprising if one remembers the proof from gradient
		descent. The real issue is not overshooting \(-1\) with large learning
		rates. So multiplying \(-1<\momEV_2\) by two, adding 2 and moving the
		root to the other side we get
		\begin{align}\label{eq: from -1<sigma_2 followed}
			0 \xle{\text{Real Case}} \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
			< 3+\momCoeff-\lrSq\hesseEV
		\end{align}
		Now if we assume \(\lrSq\hesseEV < 3+\momCoeff\), then this condition is
		equivalent to 
		\begin{align}
			\nonumber
			&(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff < (3+\momCoeff-\lrSq\hesseEV)\\
			\nonumber
			&\iff 1^2 + 2(\momCoeff-\lrSq\hesseEV) + \cancel{(\momCoeff-\lrSq\hesseEV)^2} - 4\momCoeff
			< 3^2 + 6(\momCoeff-\lrSq\hesseEV) + \cancel{(\momCoeff-\lrSq\hesseEV)^2}\\
			\label{eq: momentum upper bound derivation}
			&\iff 4\lrSq\hesseEV < 8 + 8\momCoeff
			\iff \lrSq\hesseEV < 2(1+\momCoeff)
		\end{align}
		which is the important requirement on the learning rate we are looking for.
		Now for \(\momCoeff<1\) this requirement is actually stronger than
		\(\lrSq\hesseEV <3+\momCoeff\) which means that we have proven that our
		requirements are sufficient for \(\max\{|\momEV_1|,|\momEV_2|\}<1\).
		What is left to show is that \(|\momCoeff|<1\) is also necessary in the real
		case. Now from (\ref{eq: momentum upper bound derivation}) we immediately
		get
		\begin{align*}
			0 \le \tfrac{\lrSq\hesseEV}2 < 1+\momCoeff
		\end{align*}
		which implies \(-1 < \momCoeff\). At first glance it might look like
		\(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) is already sufficient for the upper
		bound, but for \(\lrSq\hesseEV>4\) this bound is greater than one again.
		So let us assume \(\lrSq\hesseEV>2\) and \(\momCoeff>0\), then
		\(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) implies a contradiction:
		\begin{align*}
			\sqrt{\momCoeff} < \sqrt{\lrSq\momCoeff} -1
			&\implies \smash{\overbrace{(1+\sqrt{\momCoeff})^2}^{1+2\sqrt{\momCoeff} + \momCoeff}}
			< \lrSq\hesseEV
			\stackrel{(\ref{eq: from -1<sigma_2 followed})}{<} 3+\momCoeff \\
			&\implies -2 +2\sqrt{\momCoeff} < 0
			\implies \sqrt{\momCoeff} < 1
		\end{align*}

		For the \emph{characterization of the real case} one only needs to observe
		that for any \(b>0\)
		\begin{align}
			\max|a\pm b| = \begin{cases}
				a + b & a \ge 0\\
				-(a-b) & a < 0
			\end{cases}.
		\end{align}
		In our case \(b\) is a square root in the real case, so by definition
		positive and \(a>0\) is equivalent to \(1+\momCoeff - \lrSq\hesseEV >0\)
		and thus implies (\ref{eq: when does sigma_1 or sigma_2 dominate?}).
		
		For \(\momCoeff>0\) one only needs to keep in mind that for
		 \(\lrSq\hesseEV < (1+\sqrt{\momCoeff})^2 <1+\momCoeff\)
		\begin{align*}
			1+\momCoeff-\lrSq\hesseEV = \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2}
			> \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\end{align*}
		ensures that \(\momEV_2 >0\). On the other hand for \(\momCoeff>0\)
		and \(\lrSq\hesseEV > (1-\sqrt{\momCoeff})^2> 1+\momCoeff\)
		\begin{align*}
			- (1+\momCoeff-\lrSq\hesseEV) = \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2}
			> \sqrt{(1+\momCoeff-\lrSq\hesseEV)^2 - 4\momCoeff}
		\end{align*}
		implies \(\momEV_1 < 0\). \qedhere
 \end{description}
\end{proof}

\begin{lemma}\label{lem-appdx: just at the border of complex case is best beta}
	\begin{align*}
		\frac{d}{d\momCoeff}\max\{|\momEV_1|,|\momEV_2|\} < 0 \qquad
		\forall -1<\momCoeff <(1-\sqrt{\lrSq\hesseEV})^2
	\end{align*}
\end{lemma}
\begin{proof}
	Using the characterizations of the real case
	from Theorem~\ref{thm: momentum - stable set of parameters} we get
	\begin{align*}
		\frac{d}{d\momCoeff}\max\{|\momEV_1|,|\momEV_2|\}
		= \begin{cases}
			\frac{d\momEV_1}{d\momCoeff} & \momCoeff \ge \lrSq\hesseEV -1 \\
			-\frac{d\momEV_2}{d\momCoeff} & \momCoeff \le \lrSq\hesseEV -1 
		\end{cases}
	\end{align*}
	This allows us to treat those cases separately. For the first case we use
	the same definition for \(\Delta\) as in Theorem~\ref{thm: momentum - stable
	set of parameters}. Calculating its derivative
	\begin{align*}
		\Delta'(\momCoeff) = 2(1+\momCoeff-\lrSq\hesseEV)	-4
	\end{align*}
	allows us to represent \(\momEV_1\) as
	\begin{align*}
		1>\momEV_1
		= 1 + \frac{\Delta'(\momCoeff)}{4} + \tfrac12 \sqrt{\Delta(\momCoeff)},
	\end{align*}
	which results in the inequality
	\begin{align*}
		0 > \tfrac14\Delta'(\momCoeff) + \tfrac12\sqrt{\Delta(\momCoeff)}
		\implies -\tfrac12 > \tfrac14\frac{\Delta'(\momCoeff)}{\sqrt{\Delta{\momCoeff}}}
	\end{align*}
	We can now use this inequality to show that\footnote{
		I want to apologize for the lack of insight in this proof upon which I
		stumbled by accident playing around with the \(\Delta(\momCoeff)\)
		representation of \(\momEV_{1/2}\). Its only redeeming merit is its brevity.
	}
	\begin{align*}
		\frac{d\momEV_1}{d\momCoeff}
		= \frac12 + \frac14 \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}} < 0.
	\end{align*}
	The second case on the other hand requires us to show that
	\begin{align*}
		\frac{d\momEV_2}{d\momCoeff}
		= \frac12 - \frac14 \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}} > 0
		\iff 2 > \frac{\Delta'(\momCoeff)}{\sqrt{\Delta(\momCoeff)}}.
	\end{align*}
	Which is true due to
	\begin{align*}
		2\sqrt{\Delta(\momCoeff)} > 0 > -4\sqrt{\lrSq\hesseEV}
		&= 2\overbrace{
			(1-\lrSq\hesseEV + (1-\sqrt{\lrSq\hesseEV})^2)
		}^{\smash{=2-2\sqrt{\lrSq\hesseEV}}} -4\\
		&= \Delta'((1-\sqrt{\lrSq\hesseEV})^2) > \Delta'(\momCoeff),
	\end{align*}
	since \(\momCoeff<(1-\sqrt{\lrSq\hesseEV})^2\) and \(\Delta'\) is monotonously
	increasing in \(\momCoeff\).
\end{proof}

\subsection{Nesterov's Momentum Convergence}

\begin{lemma}[Dumpster Dive]
	\label{lem-appendix: dumpster dive}
	The conditions
	\begin{align*}
		\ubound\gamma_n^2 \le \lbound_n
	\end{align*}
	and
	\begin{align*}
		y_n
		&= \tfrac{\lbound_n\weights_{n-1} + \lbound_{n-1}\gamma_n z_{n-1}}{\gamma_n\lbound + \lbound_{n-1}}
	\end{align*}
	are sufficient for lower bounding
	\begin{align*}
		J := (1-\gamma_n)\left(
			\langle \Loss(y_n), \weights_{n-1} - y_n\rangle
			+ \tfrac{\lbound_{n-1}}{2}\|y_n - z_{n-1}\|^2
		\right)
		- \tfrac{\lbound_n}{2}\|y_n - z_n\|^2
	\end{align*}
	by \(- \tfrac{1}{2\ubound}\|\nabla\Loss(y_n)\|^2\).
\end{lemma}
\begin{proof}
	By definition \(z_n\) is the unique minimum of \(\Phi_n\) so we can find
	it with
	\begin{align*}
		0\xeq{!}\Phi'_n(z_n)
		= \gamma_n\underbrace{\phi_n'(z_n)}_{
			= \nabla\Loss(y_n) \mathrlap{+ \lbound(z_n-y_n)}
		} +(1-\gamma_n)
		\underbrace{\Phi'_{n-1}(z_n)}_{
			=\lbound_{n-1}\mathrlap{(z_n-z_{n-1})}
		},
	\end{align*}
	where we have used the representation (\ref{eq: centered Phi representation})
	for \(\Phi_{n-1}\). It is also quite reassuring that the gradient at \(y_n\)
	which we ultimately need makes its first appearance here. Collecting all
	the \(z_n\) we get
	\begin{align}\label{eq: center of Phi recursion}
		\overbrace{\lbound_n}^{
			=\mathrlap{\gamma_n\lbound + (1-\gamma_n)\lbound_{n-1}}
		}z_n
		= \gamma_n\lbound y_n + (1-\gamma_n)\lbound_n z_{n-1}
		- \gamma_n\nabla\Loss(y_n).
	\end{align}
	Subtracting \(\lbound_n y_n\) results in
	\begin{align*}
		\lbound_n(z_n-y_n)
		= (1-\gamma_n)\lbound_{n-1}(z_{n-1}-y_n)
		- \gamma_n\nabla\Loss(y_n).
	\end{align*}
	Plugging this into the last part of our ``junk'', we get
	\begin{align*}
		\tfrac{\lbound_n}2\|y_n-z_n\|^2
		&= \tfrac1{2\lbound_n}
		\underbrace{
			\|{\scriptstyle (1-\gamma_n)\lbound_{n-1}}(z_{n-1}-y_n)
			- {\scriptstyle \gamma_n}\nabla\Loss(y_n)\|^2
		}_{
		\begin{aligned}
			=&{\scriptstyle (1-\gamma_n)^2\lbound_{n-1}^2}\|z_{n-1}-y_n\|^2 \\
			&- {\scriptstyle 2(1-\gamma_n)\lbound_{n-1}\gamma_n}\langle
			\nabla\Loss(y_n), z_{n-1}-y_n\rangle \\
			&+ {\scriptstyle\gamma_n^2}\|\nabla\Loss(y_n)\|^2
		\end{aligned}
		}
	\end{align*}
	Therefore our ``junk'' is equal to
	\begin{align*}
		-\tfrac{\gamma_n^2}{2\lbound_n}\|\nabla\Loss(y_n)\|^2
		\begin{aligned}[t]
			&+ {\scriptstyle(1-\gamma_n)}\left\langle \Loss(y_n),
			(\weights_n -y_n)
			+ \tfrac{\lbound_{n-1}\gamma_n}{\lbound_n}(z_{n-1} - y_n)
			\right\rangle\\
			&+\tfrac{(1-\gamma_n)\lbound_{n-1}}{2}
			\underbrace{\left(1-\tfrac{(1-\gamma_n)\lbound_{n-1}}{\lbound_n}\right)}_{
				= \tfrac{\gamma_n \lbound}{\lbound_n} \ge 0
			}
			\|y_n - z_{n-1}\|^2.
		\end{aligned}
	\end{align*}
	As the last summand is positive we can discard it and end up with two equations
	sufficient to achieve our lower bound
	\begin{align*}
		-\tfrac{\gamma_n^2}{\lbound_n}
		&\ge -\tfrac{1}{\ubound} \iff \ubound\gamma_n^2 \le \lbound_n
		\\
		0
		&= (\weights_{n-1} -y_n) + \tfrac{\lbound_n\gamma_n}{\lbound_n}(z_{n-1} - y_n)
		\xiff{\text{reorder}}
		y_n
		= \tfrac{\lbound_n\weights_{n-1} + \lbound_{n-1}\gamma_n z_{n-1}}{\gamma_n\lbound + \lbound_{n-1}}
		\qedhere
	\end{align*}
\end{proof}

\begin{lemma}\label{lem-appendix: streamline general schema of optimal methods}
	If we use the recursion
	\begin{align}
		\weights_n = y_n - \tfrac1\ubound\Loss(y_n)
	\end{align}
	in Algorithm~\ref{algo: general schema of optimal methods} line~\ref{line:
	selecting next position}, then we can simplify \(z_n\) and \(y_n\)
	to
	\begin{align}
		\label{eq: z_n recursion}
		z_n
		&= \weights_{n-1} + \tfrac1{\gamma_n}(\weights_n-\weights_{n-1})
		\qquad \forall n\ge 1\\
		y_{n+1}
		&=\weights_n + \momCoeff_n(\weights_n-\weights_{n-1}) \qquad \forall n\ge 0
	\end{align}
	for \(\weights_{-1}:=\weights_0=z_0\) and
	\begin{align}
		\momCoeff_n
		= \frac{\lbound_n \gamma_{n+1}(1-\gamma_n)}{\gamma_n(\gamma_{n+1}\lbound +\lbound_n)}
		= \frac{\gamma_n(1-\gamma_n)}{\gamma_n^2 + \gamma_{n+1}}
	\end{align}
\end{lemma}
\begin{proof}
	After the assignment in line~\ref{line: definition of y_n} we can reorder it
	as an equation to get
	\begin{align*}
		\lbound_{n-1} z_{n-1}
		= \tfrac{1}{\gamma_n} [(\gamma_n\lbound + \lbound_{n-1})y_n - \lbound_n\weights_{n-1}]
	\end{align*}
	plugging this into our definition of \(z_n\) results in our first claim
	\begin{align*}
		z_n
		&= \tfrac{1}{\lbound_n}
		\left[
			\gamma_n\lbound y_n
			+ \tfrac{(1-\gamma_n)}{\gamma_n}
			[(\gamma_n\lbound + \lbound_{n-1})y_n - \lbound_n\weights_{n-1}]
			- \gamma_n\nabla\Loss(y_n)
		\right]\\
		&=\tfrac{1}{\lbound_n} \underbrace{\Big[
			\underbrace{\gamma_n\lbound + (1-\gamma_n)\lbound}_{=\lbound}
			+ \tfrac{1-\gamma_n}{\gamma_n}\lbound_{n-1}
		\Big]}_{
			=\lbound_n/\gamma_n
		}y_n
		-\tfrac{1-\gamma_n}{\gamma_n}\weights_{n-1}
		-\underbrace{\tfrac{\gamma_n}{\lbound_n}}_{
			=\frac{1}{\ubound\gamma_n} \mathrlap{\ (L\gamma_n^2 = \lbound_n)}
		}\nabla\Loss(y_n)\\
		&= \weights_{n-1} + \tfrac{1}{\gamma_n}(y_n - \weights_{n-1})
		- \tfrac{1}{\ubound\gamma_n}\nabla\Loss(y_n)\\
		&\lxeq{(\ref{eq-lem-assmpt: weigth recursion})}
		\weights_{n-1} + \tfrac{1}{\gamma_n}(\weights_n-\weights_{n-1})
	\end{align*}
	Now we are going to rewrite \(y_{n+1}\):
	\begin{align*}
		y_{n+1}
		&= \frac{
			\overbrace{[\gamma_{n+1} \lbound + (1-\gamma_{n+1})\lbound_n]}^{\lbound_{n+1}}\weights_n
			+ \lbound_n \gamma_{n+1} z_n
		}{\gamma_{n+1}\lbound +\lbound_n}
		=\weights_n
		+ \frac{\lbound_n \gamma_{n+1} (z_n - \weights_n)}{\gamma_{n+1}\lbound +\lbound_n}
	\end{align*}
	For \(n\ge1\) we can apply our previous result (\ref{eq: z_n recursion})
	\begin{align*}
		z_n - \weights_n
		= \left(\tfrac1{\gamma_n} -1\right)(\weights_n-\weights_{n-1}),
	\end{align*}
	for \(n=0\) both sides are zero and we therefore obtain our second recursion
	\begin{align*}
		y_{n+1}
		=\weights_n
		+ \underbrace{
			\frac{\lbound_n \gamma_{n+1}(1-\gamma_n)}{\gamma_n(\gamma_{n+1}\lbound +\lbound_n)}
		}_{=\momCoeff_n}
		(\weights_n-\weights_{n-1}).
	\end{align*}
	Now we just use the identity \(\gamma_{n+1}\lbound = \ubound\gamma_{n+1}^2 -
	(1-\gamma_{n+1})\lbound_n\) from the definition of \(\gamma_{n+1}\) to simplify our
	momentum coefficient
	\begin{align*}
		\momCoeff_n
		&= \frac{\lbound_n \gamma_{n+1}(1-\gamma_n)}{
			\gamma_n([\ubound\gamma_{n+1}^2 - (\cancel{1}-\gamma_{n+1})\lbound_n] +\cancel{\lbound_n})
		}
		= \frac{\lbound_n\cancel{\gamma_{n+1}}(1-\gamma_n)}{
			\gamma_n\cancel{\gamma_{n+1}}(\ubound\gamma_{n+1} + \underbrace{\lbound_n}_{=\ubound\gamma_n^2})
		}\\
		&\lxeq{\bcancel{\ubound\gamma_n}} \frac{\gamma_n(1-\gamma_n)}{\gamma_{n+1} + \gamma_n^2}
		\qedhere
	\end{align*}
\end{proof}

\begin{lemma}[{\cite[Lemma 2.2.4]{nesterovLecturesConvexOptimization2018}}]
	\label{lem-appendix: convergence rate bounds for estimating sequences}
	For \(\condition_0^{-1}\in (\condition^{-1}, 3 + \condition^{-1}]\) we have
	\begin{align*}
		\Gamma^n
		\le \frac{4\condition^{-1}}{(\condition_0^{-1}-\condition^{-1})\left[
			\exp\left(\frac{n+1}{2\sqrt{\condition}}\right)
			-\exp\left(-\frac{n+1}{2\sqrt{\condition}}\right)
		\right]^2}
		\le \frac{4}{(\condition_0^{-1}-\condition^{-1})(n+1)^2}
	\end{align*}
\end{lemma}
\begin{proof}
	We are interested in finding a function fulfilling
	\begin{align}\label{eq: bounding function properties}
		\Gamma^n \le \frac{1}{f(n)^2} \iff f(n) \le \frac1{\sqrt{\Gamma^n}}
	\end{align}
	Since the weaker inequality would require \(f(n)\sim n\) it might be a
	good idea to look at the increments.
	% \begin{align*}
	% 	f(0) + \sum_{k=1}^n f(k) - f(k-1) = f(n)
	% 	\le \frac{1}{\sqrt{\Gamma^n}}
	% 	= \underbrace{\frac1{\sqrt{\Gamma^0}}}_{=1}
	% 	+ \sum_{k=1}^n\frac{1}{\sqrt{\Gamma^k}}-\frac{1}{\sqrt{\Gamma^{k-1}}}.
	% \end{align*}
	In particular it would be sufficient if we had \(f(0) \le
	\frac1{\sqrt{\Gamma^0}}=1\) and
	\begin{align*}
		f(k) - f(k-1) \le \frac{1}{\sqrt{\Gamma^k}}-\frac{1}{\sqrt{\Gamma^{k-1}}}
		\qquad \forall k > 0.
	\end{align*}
	for our weaker inequality this requires the right side to be lower bounded
	by a constant. Now if we had not started out looking for a squared \(f\) we
	would have instead ended up with the increment
	\begin{align*}
		\frac{1}{\Gamma^k}-\frac{1}{\Gamma^{k-1}}.
	\end{align*}
	This provides some intuition how Nesterov might have come up with the following	
	idea. Let us consider the increment above and multiply it by \(\Gamma^k\).
	Then we get
	\begin{align*}
		1-\frac{\Gamma^k}{\Gamma^{k-1}}
		&\lxeq{\text{def.}} \gamma_k
		\xeq{(\ref{eq: morphing speed equation})}
		\sqrt{\frac{\lbound_k}{\ubound}} \xeq{(\ref{eq: lbound recursion})}
		\sqrt{\frac{\Gamma^k \lbound_0 + (1-\Gamma^k)\lbound}{\ubound}}\\
		&\lxeq{\text{def.}} \sqrt{\condition^{-1} + \Gamma^k(\condition_0^{-1}-\condition^{-1})}.
	\end{align*}
	Now we not only need to return to the increments by dividing by \(\Gamma^k\)
	again, we also need to somehow add roots. To do that we are going to
	use the third binomial formula
	\begin{align}
		\nonumber
		\tfrac1{\sqrt{\Gamma^k}}
		\sqrt{\tfrac{\condition^{-1}}{\Gamma^k} + \condition_0^{-1}-\condition^{-1}}
		&= \frac{1}{\Gamma^k}-\frac{1}{\Gamma^{k-1}}\\
		\nonumber
		&\lxeq{\text{bin.}} 
		\left(
			\frac{1}{\sqrt{\Gamma^k}}+\frac{1}{\sqrt{\Gamma^{k-1}}}
		\right)
		\left(
			\frac{1}{\sqrt{\Gamma^k}}-\frac{1}{\sqrt{\Gamma^{k-1}}}
		\right) \\
		&\le \frac{2}{\sqrt{\Gamma^k}}
		\left(
			\frac{1}{\sqrt{\Gamma^k}}-\frac{1}{\sqrt{\Gamma^{k-1}}}
		\right)
		\label{eq: inverse gamma increment}
	\end{align}
	where we have used \(\Gamma^k\le\Gamma^{k-1}\) for the inequality. From
	this we get the sufficient condition
	\begin{align*}
		f(k) - f(k-1)
		\le \tfrac12\sqrt{\tfrac{\condition^{-1}}{\Gamma^k} + \condition_0^{-1}-\condition^{-1}}
		= \tfrac12\underbrace{\sqrt{\condition_0^{-1}-\condition^{-1}}}_{=:c}
		\sqrt{\tfrac{\condition^{-1}}{\Gamma^k}c^{-2} + 1}
	\end{align*}
	In particular we could throw away the complicated positive part in the root
	to get the sufficient condition
	\begin{align*}
		f(k) - f(k-1) := \tfrac{c}2\quad \forall k>0 \quad \text{and}
		\quad f(0):=\tfrac{c}2 = \tfrac{\sqrt{\condition_0^{-1}-\condition^{-1}}}2
		\le \tfrac{\sqrt{3}}{2}<1.
	\end{align*}
	And this immediately leads to \(f(n) = \tfrac{c}{2}(n+1)\) which provides us
	with the weaker second bound. Note that the ``complicated'' part we have
	thrown away is zero in case of \(\condition^{-1}\) i.e.\ when we do not
	have strong convexity this is the best we can do.
	
	But for strong convexity we need to find a tighter upper bound \(f\). 
	If we shuffle around (\ref{eq: inverse gamma increment}) a bit we get
	\begin{align*}
		\frac{1}{\sqrt{\Gamma^{k-1}}}
		\le \frac{1}{\sqrt{\Gamma^k}}
		- \frac{c}{2}\sqrt{\tfrac{\condition^{-1}}{c^2}\left(\tfrac{1}{\sqrt{\Gamma^k}}\right)^2+1}
		=: g\left(\tfrac1{\sqrt{\Gamma^k}}\right)
	\end{align*}
	Now here is the trick: \(g\) is strictly monotonously increasing (its
	derivative is positive). So if \(n\) was the first number violating
	our requirement (\ref{eq: bounding function properties}), (i.e.
	\(\tfrac{1}{\sqrt{\Gamma^n}}< f(n)\)), then due to
	\begin{align*}
		f(n-1)
		\xle{\text{(\ref{eq: bounding function properties})}} \frac{1}{\sqrt{\Gamma^{n-1}}}
		\le g\left(\tfrac1{\sqrt{\Gamma^n}}\right) 
		< g(f(n))
	\end{align*}
	it would be enough for
	\begin{align}\label{eq: weird contradiction requirement}
		g(f(n))\le f(n-1)
	\end{align}
	to be true, to obtain a
	contradiction. So if \(f(0)\le 1\) is a given, \(f(n)\) would then fulfill
	(\ref{eq: bounding function properties}) by induction\footnote{Unfortunately,
	I have no idea how Nesterov came up with this trick, nor how he eventually
	found the function \(f\) that satisfies the requirements}. As we know the
	result we can define
	\begin{align*}
		f(n)
		:=\tfrac{c}{2\sqrt{\condition^{-1}}}\left[
			\exp\left(\tfrac{n+1}{2\sqrt{\condition}}\right)
			-\exp\left(-\tfrac{n+1}{2\sqrt{\condition}}\right)
		\right]
		=\frac{c}{4\delta}\left[
			e^{(n+1)\delta}- e^{-(n+1)\delta}
		\right]
	\end{align*}
	for \(\delta:=\sqrt{\condition^{-1}}/2\le 1/2\). Since \(f(0)\) is increasing in \(\delta\)
	we have the induction start 
	\begin{align*}
		f(0)=\tfrac{c}{4\delta}\left[e^\delta-e^{-\delta}\right]
		\le \tfrac{c}2\left[e^{1/2}-e^{-1/2}\right]
		\le \tfrac{c}{\sqrt{3}} \le 1.
	\end{align*}
	Now we only need to show (\ref{eq: weird contradiction requirement}) to be
	true to finish the proof:
	\begin{align*}
		g(f(n))
		&= f(n) - \frac{c}{2}\sqrt{\tfrac{\condition^{-1}}{c^2}f(n)^2 + 1}\\
		&= f(n) - \frac{c}{2}\sqrt{
			\tfrac{1}{2^2}\left[e^{(n+1)\delta}- e^{-(n+1)\delta}\right]^2
			+ 1
		} \\
		&= f(n) - \frac{c}{2}\sqrt{
			\tfrac{1}{2}\left[
				\tfrac12 e^{2(n+1)\delta}
				-\smash{\underbrace{e^{(n+1)\delta}e^{-(n+1)\delta}}_{=1}}
				+\tfrac12e^{-2(n+1)\delta}
			\right]
			+ 1
		}\\
		&= f(n) - \underbrace{\frac{c}{4}\left[e^{(n+1)\delta}+ e^{-(n+1)\delta}\right]}_{=f'(n)}\\
		&= f(n) + \langle f'(n), n - (n-1) \rangle \xle{f\text{ convex}} f(n-1)
		\qedhere
	\end{align*}
\end{proof}
\begin{lemma}\label{lem-appendix: derivative loss difference constant in gamma}
	\begin{align*}
		c(\gamma_1)
		=2\ubound\frac{1-\gamma_1(1+\condition^{-1}) + \gamma_1^2}{\gamma_1^2 -\condition^{-1}}
		\qquad \gamma_1\in(\sqrt{\condition^{-1}}, 1).
	\end{align*}
	is monotonously decreasing.
\end{lemma}
\begin{proof}
	We are going to discard \(2\ubound\) without loss of generality to get	
	\begin{align*}
		\frac{dc}{d\gamma_1}
		&= \frac{
			(-(1+\condition^{-1})+2\gamma_1)(\gamma_1^2 -\condition^{-1})
			- (1-\gamma_1(1+\condition^{-1})+\gamma_1^2)2\gamma_1
		}{(\gamma_1^2-\condition^{-1})^2}.
	\end{align*}
	Since we are interested in the sign of the derivative we can discard the
	denominator and look at just the enumerator
	\begin{align*}
		&2\gamma_1(\cancel{\gamma_1^2}-\condition^{-1}) - (1+\condition^{-1})(\gamma_1^2-\condition^{-1})
		- (1-\gamma_1(1+\condition^{-1})+\cancel{\gamma_1^2})2\gamma_1\\
		&= -2\gamma_1\condition^{-1} - (1+\condition^{-1})(\gamma_1^2 + \condition^{-1})
		-2\gamma_1 + 2\gamma_1^2(1+\condition^{-1})\\
		&= (1+\condition^{-1})\underbrace{(\condition^{-1}-2\gamma_1 + 2\gamma_1^2)}_{=:f(\gamma_1)}.
	\end{align*}
	In which we only need to consider the parabola \(f\). Now since the parabola
	is convex, it is negative for all points between two ends where it
	is negative. And we have
	\begin{align*}
		f(\sqrt{\condition^{-1}}) &= 2(\condition^{-1}-\sqrt{\condition^{-1}}) < 0\\
		f(1) &= \condition^{-1} -1 < 0
		\qedhere
	\end{align*}
\end{proof}

\chapter{Code for Visualizations}

The Programming Language Julia \parencite{bezansonJuliaFreshApproach2017} was
used to generate all the visualizations without citation in this work. The code
can be found in various .jl files on GitHub at
\url{https://github.com/FelixBenning/masterthesis/tree/main/media}. While
these are normal Julia files which can be run as-is, they were written for
the \href{https://github.com/fonsp/Pluto.jl}{Pluto.jl} notebook, which is
required for the interactive features of these plots.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput