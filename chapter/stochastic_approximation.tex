% !TEX root = ../Masterthesis.tex

\newcommand{\dist}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}

\chapter{Classical Optimization}

Let \(f_\theta(X)\) be a (parametrized) prediction for \(Y\), where \(Z=(X,Y)\)
is drawn from a distribution \(\dist\) of real world examples. The prediction
error of our parameter \(\theta\) is a lossfunction \(l(\theta, X, Y)\), e.g. 
%
\begin{align*}
	l(\theta, x, y) := (f_\theta(x) - y)^2.
\end{align*}
%
And we \emph{want} to minimize the theoretical loss over all examples (risk)
%
\begin{align*}
	L(\theta) := \E_{X,Y\sim \dist} [l(\theta, X, Y)].
\end{align*}

\section{Zero Order Optimization}

Let us set aside the stochasticity of the problem for a moment and assume we
could evaluate \(L\) at any point \(\theta\). In other words: we have a "zero
order oracle" which can tell us \(L(\theta)\) for any \(\theta\) for a fixed
computational cost.

If we assume an infinite number of possible inputs \(\theta\), it does not matter
how much and where we sample \(L\) there is always a different \(\theta\) where
\(L\) might be arbitrarily lower. This notion is formalized in various "No-Free-Lunch
Theorems". We must therefore start making assumptions about \(L\) to make this
problem tractable.

\subsection{Gridsearch}

While Lipschitz continuity 
%
\begin{align*}
	| L(\theta_1) - L(\theta_2) | < K_0 | \theta_1 - \theta_2 |
\end{align*}
%
is sufficient to find an \(\epsilon\)-solution \(\hat{\theta}\) inside a unit
(hyper-)cube
%
\begin{align*}
	L(\hat{\theta}) \le \inf_{\theta: 0\le\theta_i\le 1} L(\theta) + \epsilon
\end{align*}
%
in finite time, it still requires a full \emph{grid search} with time complexity
%
\begin{align*}
	\left(\left\lfloor \frac{K_0}{2\epsilon}\right\rfloor + 1\right)^n
\end{align*}
%
where \(n\) is the dimension of \(\theta\). The fact that grid search is
sufficient can be seen by tiling the space into a grid and asking the oracle
for the value of the loss function at each center. If the minimum is placed in
some tile, the Lipschitz continuity forces the center of the tile to be \(\epsilon\)
close to it \parencite[cf.][p. 11]{nesterovLecturesConvexOptimization2018}.

The fact that this is necessary can easily be seen by imagining a "resisting
oracle" which places the minimum in just the last gridelement where we look
\parencite[cf.][p. 13]{nesterovLecturesConvexOptimization2018}. The exponential
increase of complexity in the dimension makes the problem intractable for even
mild dimensions and precisions (e.g. \(n=11\), \(\epsilon=0.01\), \(K_0=2\)
implies millions of years of computation). So we have to make further
assumptions. \fxnote{annealing? evolutionary algorithms?}

\section{First Order Optimization}

A step up from zero order methods (such as grid search) are first order methods
using the first derivative (the gradient). This requires a "first order oracle"
which can provide us with \(L(\theta)\) and \(\nabla L(\theta)\) at any point
\(\theta\).

\subsection{Gradient Decent}

One immediately obvious way to utilize this information is to
incrementally move in the direction of steepest decent, i.e.
%
\begin{align*}
	\theta_{n+1} = \theta_n - \eta\nabla L(\theta_n).
\end{align*}
%
A useful way to look at this equation is to notice that it is the
discretization of an ordinary differential equation (ODE)
%
\begin{align*}
	\dot{\theta}_n \approx \frac{\theta_{n+1} - \theta_n}{\eta}
	= - \nabla L(\theta_n).
\end{align*}
%
Here \(\eta\) is the time delta between discretizations \(t_n\) and \(t_{n+1}\).
This of course implies \(t_n = n\eta\). And for \(\eta\to\infty\), we arrive
at the ODE
%
\begin{align*}
	\dot{\theta}(t) = -\nabla L(\theta(t)).
\end{align*}
%
If you are familiar with Lyapunov functions you will not be surprised by the next
few arguments:
%
\begin{align*}
	L(\theta(t_1)) - L(\theta(t_0))
	&= \int_{t_0}^{t_1} \nabla L(\theta(s)) \cdot \dot{\theta}(s) ds
	= \int_{t_0}^{t_1} -\|\nabla L(\theta(s))\|^2 ds
	\le 0\\
\end{align*}
%
immediately implies
\begin{align*}
	L(\theta(t_0)) \ge L(\theta(t_1)) \ge \dots \ge 0
\end{align*}
which implies convergence. But not necessarily a convergent \(\theta(t)\).

\subsection{Convergence of the Gradient}

The first thing we might want to ensure, is that the gradient \(\nabla L(\theta(t))\)
actually converges to zero. If we know that the gradient being larger \(\epsilon\)
implies it being larger \(\epsilon/2\) in a \(\delta\) environment around any
such occurance, we can immediately see that the number of times the gradient
sticks its head above \(\epsilon\) must be finite, as
%
\begin{align}\label{bounded gradient integral}
	0 \le \liminf_{t\to\infty} L(\theta(t))
	=  L(\theta(t_0)) - \int_{t_0}^\infty \|\nabla L(\theta(s))\|^2 ds.
\end{align}
%
So uniform continuity is sufficient for convergence of \(\nabla L(\theta(t))\).
But that still does not ensure that \(\theta(t)\) converges, only that its
derivative \(\dot{\theta}(t) = -\nabla L(\theta(t))\) converges. The logarithm
is an obvious example where this goes wrong.

Additionally we are currently talking about a theoretical ODE where the timestep
is infinitesimal. So let us first return to a discretized gradient. Under the
assumption that \(\nabla L\) is Lipschitz continuous
%
\begin{align*}
	\| \nabla L(\theta) - \nabla L(\tilde{\theta})\| \le K_1 \|\theta-\tilde{\theta}\|
\end{align*}
%
we have for \(\theta_{n+1}=\theta_n-\eta\nabla L(\theta_n)\)
%
\begin{align}
	L(\theta_{n+1})
	&\lxle{\ref{Lipschitz Gradient implies taylor inequality}} L(\theta_n) 
	+ \langle\nabla L(\theta_n),\theta_{n+1} - \theta_n\rangle
	+ \tfrac{K_1}{2} \| \theta_{n+1} - \theta_n\|^2 
	\label{bound increment}\\
	&= L(\theta_n)
	+ \langle\nabla L(\theta_n), -\eta\nabla L(\theta_n)\rangle
	+ \tfrac{K_1}{2}\eta^2\| \nabla L(\theta_n)\|^2
	\nonumber\\
	&=L(\theta_n) - \eta(1-\tfrac{\eta K_1}{2})\|\nabla L(\theta_n)\|^2
	\nonumber
\end{align}
%
this means that for \(\eta=\tfrac{2\alpha}{K_1}\), \(\alpha \in (0,1)\)
parameterizes all the \(\eta\) for which we can guarantee a positive decrease
%
\begin{align*}
	L(\theta_{n+1}) - L(\theta_n)
	\le - \tfrac{2}{K_1}\alpha (1-\alpha)\|\nabla L(\theta_n)\|^2
\end{align*}
%
with the optimal choice of \(\eta=\tfrac{1}{K_1}\) (\(\alpha=1/2\)) resulting
in 
%
\begin{align*}
	L(\theta_{n+1}) - L(\theta_n)
	\le - \tfrac{1}{2K_1}\|\nabla L(\theta_n)\|^2.
\end{align*}
%
Summing over these increments results in a very similar equation to
(\ref{bounded gradient integral})
%
\begin{align*}
	\inf_{\theta} L(\theta) \le L(\theta_n) 
	\le L(\theta_0) - \tfrac{1}{2K_1} \sum_{k=0}^{n-1}\|\nabla L(\theta_k)\|^2
\end{align*}
%
Since \(\eta=\tfrac{1}{K_1}\) is the time increment between the \(\theta_k\)
we have only lost the factor \(1/2\) in our estimation (\ref{bound increment})
compared to the precise integral version.

In particular we get
%
\begin{align*}
	\tfrac{n}{2K_1}
	\underbrace{\tfrac{1}{n} \sum_{k=0}^n \|\nabla L(\theta_k)\|^2}_{
		=: \overline{\|\nabla L(\theta_n)\|^2}
	}
	\le \tfrac{1}{2K_1}\sum_{k=0}^{n-1} \|\nabla L(\theta_k)\|^2 
	\le L(\theta_0) - \inf_\theta L(\theta)
\end{align*}
%
which implies that the average gradient square converges with \(n\) to zero
%
\begin{align*}
	\overline{\|\nabla L(\theta_n)\|^2}
	\le \tfrac{1}{n}2 K_1 (L(\theta_0) - \inf_\theta L(\theta)) \in O(\tfrac{1}{n}).
\end{align*}
%
It is important to note the "square" part of the gradient squares. If we had the
series of unsquared gradient norms being finite as well, we would have
\(\theta_n\) being a cauchy sequence
%
\begin{align*}
	\|\theta_n - \theta_m \|
	\le \sum_{k=m}^{n-1} \|\theta_{k+1} - \theta_k\|
	\le \tfrac{1}{2K_1} \sum_{k=m}^{n-1} \|\nabla L(\theta_k)\|.
\end{align*}
%
This provides us with some intuition how a situation might look like when the
gradient converges but not the sequence of \(\theta_n\). The gradient would
behave something like the harmonic series, as its squares converges and the
\(\theta_n\) would behave like the partial sums of the harmonic series which
behaves in the limit like the logarithm.

It is difficult to formulate an example in finite space, but
%
\begin{align*}
	L(\theta) &= \exp(-\theta) \\
	\dot{\theta} &= -\nabla L(\theta)
\end{align*}
%
implies
%
\begin{align*}
	\theta(t) &= \log(t)\\
	\nabla L(t) &= -\tfrac1t
\end{align*}
%
which provides intuition how "flat" a minima has to be to cause such behaviour.
The minimum at \(\infty\) has an infinitely wide basin which flattens out
more and more. If we wanted such an example in a bounded space we would have
to try and coil up such an infinite slope into a spiral, which spirals outwards
to avoid convergence.

\subsection{All Roads Lead to Convexity}

Before we further tighten our restrictions on \(L\) to ensure convergene, let
me point out the following lemma
%
\begin{lemma}
	If \(f\) is differentiable, then derivative \(\nabla f\) is
	bounded (w.r.t. the operator norm) by constant \(K\) iff the function
	\(f\) is Lipschitz continuous with Lipschitz constant \(K\).
\end{lemma}
\begin{proof}
	Lipschitz continuity is implied by the mean value theorem
	\begin{align*}
		\|f(x_1) - f(x_0)\|
		&\le \|\nabla f(x_0 + \xi(x_1-x_0))\| \|x_1- x_0\|\\
		&\le K \|x_1-x_0\|.
	\end{align*}
	%
	The opposite direction is implied by
	%
	\begin{align*}
		\|\nabla f(x_0)\|
		&\equiv \sup_{v} \frac{\|\nabla f(x_0)v\|}{\|v\|}
		= \sup_{v} \lim_{v\to 0}\frac{\|\nabla f(x_0)v\|}{\|v\|}\\
		&\lxle{\Delta}\sup_{v} \lim_{v\to 0}
		(
			\underbrace{
				\tfrac{\|f(x_0 + v) - f(x_0) \|}{\|v\|}
			}_{
				\le K
			}
			+ \underbrace{
				\tfrac{\|\nabla f(x_0)v + f(x_0) - f(x_0 +v)\|}{\|v\|}
			}_{
				\to 0 \text{ (derivative definition)}
			}
		)
	\end{align*}
	%
	where we have used the scalability of the norm to multiply \(v\) with a
	decreasing factor both in the numerator and denominator in order to introduce
	the limit.
\end{proof}

Lipschitz continuity of \(\nabla L\) is thus equivalent to boundedness of the
Hesse matrix \(\nabla^2 L\). And since the Hesse matrix is symmetric and thus
diagnoizable with an orthonormal basis, the operatornorm of it is equal to its
largest absolute eigenvalue. The Lipschitz constant thus bounds the speed at
which the gradient changes in the direction of an eigenvector corresponding to
this largest absolute eigenvalue.

The slower the gradient changes, the further we can walk before we have to
take another measurement. For this reason the optimal learning rate is
inversely proportional to the Lipschitz constant.

The Lipschitz constant \(K_1\) restricts how sharp the minima can be, 
this ensures that we do not overshoot it and bounce around aimlessly with a 
learning rate that is much too high. But as we have seen above, a very flat
minima might decelerate gradient decent so much that our prarmeter \(\theta_n\)
never converges.

So it is a natural conclusion to bound the Hesse matrix from below as well.
Now if there is a local minima somewhere, then it can not have negative
eigenvalues in its Hessian. Those would imply a direction in which the gradient
\emph{decreases} below zero and thus that the minima is in fact not a minima.
So if we assume that the absolute eigenvalues are bounded from below, assume
that \emph{a} local minima exists, and assume that the second derivative is
continuous, then the positive eigenvalues around that local minima can
never become negative due to the lower bound on the aboslute eigenvalues and
continuity. In other words: We are assuming strong, global convexity. Which of
course is anything but a natural assumption.

Alternatively one could make this assumption only about the local minimum and
make continuity assumptions about the second derivative, then there is a local
space around the minimum where this bound holds and we can get convergence
statments conditioned on starting in this local space.

Since neither option resembles real applications, we will focus on the intuition
similarly to \textcite{gohWhyMomentumReally2017} instead of mathematical rigor.
For a proper treatment of the local case see \textcite[Theorem
1.2.4]{nesterovLecturesConvexOptimization2018} and for the global case see
\textcite[Theorem 2.1.15]{nesterovLecturesConvexOptimization2018}.

\subsection{Visualizing the Second Order Taylor Approximation}
If we assume our Hesse matrix \(\nabla^2 L(\theta)\) to be positive definite
(all eigenvalues positive), then for a second order Taylor approximation
%
\begin{align*}
	L(\theta+v) \approx T_2L(\theta+v)
	= L(\theta) + v^T \nabla L(\theta) + \tfrac12 v^T \nabla^2 L(\theta) v
\end{align*}
%
we can find the minimum 
\begin{align*}
	v^* = -(\nabla^2 L(\theta))^{-1}\nabla L(\theta)
\end{align*}
by setting the first derivative to zero
%
\begin{align*}
	\nabla T_2L(\theta+v) = \nabla L(\theta) + \nabla^2L(\theta) v \xeq{!} 0.
\end{align*}
%
This minimum allows us not only to rewrite the taylor derivative
%
\begin{align*}
	\nabla T_2L(\theta+v) = \nabla^2 L(\theta)(v-v^*),
\end{align*}
%
but also the original taylor approximation
%
\begin{align*}
	T_2L(\theta+v)
	&= L(\theta) - v^T \nabla^2 L(\theta) v^* + \tfrac12 v^T \nabla^2 L(\theta) v \\
	&= \underbrace{L(\theta) - \tfrac12 v^* \nabla^2 L(\theta) v^*}_{=: \alpha \text{ (const)}}
	+ \tfrac12 (v-v^*)^T \nabla^2 L(\theta)(v-v^*).
\end{align*}
%
To get absolute instead of relative coordinates to \(\theta\) we set
%
\begin{align*}
	\theta^* := \theta + v^* = \theta -(\nabla^2 L(\theta))^{-1}\nabla L(\theta),
\end{align*}
%
and obtain the notion of a paraboloid centered around \(\theta^*\)
%
\begin{align}
	L(y) = \tfrac12 (y- \theta^*) \nabla^2 L(\theta) (y-\theta^*) + \alpha + o(\|y-\theta\|^2)
\end{align}
%
To fully appreciate Figure~\ref{fig: 2d paraboloid}, we only need to realize
that the diagonizability of \(\nabla^2 L(\theta)\)
%
\begin{align}\label{diagnalization of the Hesse matrix}
	V \nabla^2 L(\theta) V^T
	= \text{diag}(\lambda_1,\dots,\lambda_n), \qquad V=(v_1,\dots, v_n)
\end{align}
%
\begin{wrapfigure}{O}{0.6\textwidth}
	\centering
	\def\svgwidth{0.58\textwidth}
	\input{media/contour.pdf_tex}
	\caption{Assuming \(\theta^*=0\), \(\lambda_1=1, \lambda_2=2\), \(v_1=(\sin(1), \cos(1))\)}
	\label{fig: 2d paraboloid}
\end{wrapfigure}
%
implies that once we have selected the center \(\theta^*\) and the direction of
one eigenspace in two dimensions, the other eigenspace has to be
orthogonal to the first one which fully determines its direction at this point. 

\subsection{From Eigenvalues to Convergence Rate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endinput