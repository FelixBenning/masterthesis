
@online{alainNegativeEigenvaluesHessian2019,
  title = {Negative Eigenvalues of the {{Hessian}} in Deep Neural Networks},
  author = {Alain, Guillaume and Roux, Nicolas Le and Manzagol, Pierre-Antoine},
  date = {2019-02-06},
  url = {http://arxiv.org/abs/1902.02366},
  urldate = {2021-06-17},
  abstract = {The loss function of deep networks is known to be non-convex but the precise nature of this nonconvexity is still an active area of research. In this work, we study the loss landscape of deep networks through the eigendecompositions of their Hessian matrix. In particular, we examine how important the negative eigenvalues are and the benefits one can observe in handling them appropriately.},
  archiveprefix = {arXiv},
  eprint = {1902.02366},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Alain et al\\Alain et al_2019_Negative eigenvalues of the Hessian in deep neural networks.pdf;C\:\\gdrive\\ZoteroPaper\\2019_Alain et al\\Alain et al_2019_Negative eigenvalues of the Hessian in deep neural networks2.pdf;C\:\\Users\\felix\\Zotero\\storage\\PSVMGMJV\\1902.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  langid = {english},
  primaryclass = {cs, math, stat}
}

@book{amariDifferentialGeometricalMethodsStatistics1985,
  title = {Differential-{{Geometrical Methods}} in {{Statistics}}},
  author = {Amari, Shun-ichi},
  date = {1985},
  volume = {28},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4612-5056-2},
  abstract = {From the reviews: "In this Lecture Note volume the author describes his differential-geometric approach to parametrical statistical problems summarizing the results he had published in a series of papers in the last five years. The author provides a geometric framework for a special class of test and estimation procedures for curved exponential families. ... ... The material and ideas presented in this volume are important and it is recommended to everybody interested in the connection between statistics and geometry ..." \#Metrika\#1 "More than hundred references are given showing the growing interest in differential geometry with respect to statistics. The book can only strongly be recommended to a geodesist since it offers many new insights into statistics on a familiar ground." \#Manuscripta Geodaetica\#2},
  editorb = {Berger, J. and Fienberg, S. and Gani, J. and Krickeberg, K. and Olkin, I. and Singer, B.},
  editorbtype = {redactor},
  file = {C\:\\gdrive\\ZoteroPaper\\1985_Amari\\Amari_1985_Differential-Geometrical Methods in Statistics.pdf},
  isbn = {978-0-387-96056-2 978-1-4612-5056-2},
  langid = {english},
  series = {Lecture {{Notes}} in {{Statistics}}}
}

@article{amariNaturalGradientWorks1998,
  title = {Natural {{Gradient Works Efficiently}} in {{Learning}}},
  author = {Amari, Shun-ichi},
  date = {1998-02-15},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {10},
  pages = {251--276},
  issn = {0899-7667},
  doi = {10.1162/089976698300017746},
  abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.},
  file = {C\:\\gdrive\\ZoteroPaper\\1998_Amari\\Amari_1998_Natural Gradient Works Efficiently in Learning.pdf;C\:\\Users\\felix\\Zotero\\storage\\JC5KCTNQ\\Natural-Gradient-Works-Efficiently-in-Learning.html},
  number = {2}
}

@online{andrychowiczLearningLearnGradient2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
  date = {2016-11-30},
  url = {http://arxiv.org/abs/1606.04474},
  urldate = {2021-06-17},
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archiveprefix = {arXiv},
  eprint = {1606.04474},
  eprinttype = {arxiv},
  file = {C\:\\Users\\felix\\Zotero\\storage\\AHGTIWPJ\\Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf;C\:\\Users\\felix\\Zotero\\storage\\M4UMDP6S\\1606.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  options = {useprefix=true},
  primaryclass = {cs}
}

@online{ayadiStochasticRungeKuttaMethods2020,
  title = {Stochastic {{Runge}}-{{Kutta}} Methods and Adaptive {{SGD}}-{{G2}} Stochastic Gradient Descent},
  author = {Ayadi, Imen and Turinici, Gabriel},
  date = {2020-02-20},
  url = {http://arxiv.org/abs/2002.09304},
  urldate = {2021-04-17},
  abstract = {The minimization of the loss function is of paramount importance in deep neural networks. On the other hand, many popular optimization algorithms have been shown to correspond to some evolution equation of gradient flow type. Inspired by the numerical schemes used for general evolution equations we introduce a second order stochastic Runge Kutta method and show that it yields a consistent procedure for the minimization of the loss function. In addition it can be coupled, in an adaptive framework, with a Stochastic Gradient Descent (SGD) to adjust automatically the learning rate of the SGD, without the need of any additional information on the Hessian of the loss functional. The adaptive SGD, called SGD-G2, is successfully tested on standard datasets.},
  archiveprefix = {arXiv},
  eprint = {2002.09304},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2020_Ayadi_Turinici\\Ayadi_Turinici_2020_Stochastic Runge-Kutta methods and adaptive SGD-G2 stochastic gradient descent.pdf;C\:\\Users\\felix\\Zotero\\storage\\6FAJEVZG\\2002.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@online{bachNonstronglyconvexSmoothStochastic2013,
  title = {Non-Strongly-Convex Smooth Stochastic Approximation with Convergence Rate {{O}}(1/n)},
  author = {Bach, Francis and Moulines, Eric},
  date = {2013-06-10},
  url = {http://arxiv.org/abs/1306.2119},
  urldate = {2021-07-15},
  abstract = {We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/n\^\{1/2\}). We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. For least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments on standard machine learning benchmarks showing that they often outperform existing approaches.},
  archiveprefix = {arXiv},
  eprint = {1306.2119},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2013_Bach_Moulines\\Bach_Moulines_2013_Non-strongly-convex smooth stochastic approximation with convergence rate O(1-n).pdf;C\:\\Users\\felix\\Zotero\\storage\\CFCQP8CT\\1306.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@article{benkoNystromMethodsSingular2008,
  title = {Nyström Methods and Singular Second-Order Differential Equations},
  author = {Benko, David and Biles, Daniel C. and Robinson, Mark P. and Spraker, John S.},
  date = {2008-10-01},
  journaltitle = {Computers \& Mathematics with Applications},
  shortjournal = {Computers \& Mathematics with Applications},
  volume = {56},
  pages = {1975--1980},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2008.04.023},
  abstract = {Chawla, Jain and Subramanian studied the application of Nyström methods to a class of singular initial value problems. Following their approach, we generalize this class by applying the Nyström method to the initial value problem for an equation of the form y″+p(t)y′+q(t,y(t))=0,t∈(0,1] where p has a certain specified type of singularity and q is sufficiently differentiable, and then we determine the order of convergence.~This is followed by computational evidence.},
  file = {C\:\\gdrive\\ZoteroPaper\\2008_Benko et al\\Benko et al_2008_Nyström methods and singular second-order differential equations.pdf;C\:\\Users\\felix\\Zotero\\storage\\BRE8SCJA\\S0898122108002757.html},
  keywords = {Lane–Emden equation,Numerical approximation,Nyström method,Second-order differential equation,Singular differential equations},
  langid = {english},
  number = {8}
}

@online{bottouOptimizationMethodsLargeScale2018,
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  date = {2018-02-08},
  url = {http://arxiv.org/abs/1606.04838},
  urldate = {2021-06-08},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  archiveprefix = {arXiv},
  eprint = {1606.04838},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Bottou et al\\Bottou et al_2018_Optimization Methods for Large-Scale Machine Learning.pdf;C\:\\Users\\felix\\Zotero\\storage\\5PK2UUXE\\1606.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@article{bouttierConvergenceRateSimulated2019,
  title = {Convergence {{Rate}} of a {{Simulated Annealing Algorithm}} with {{Noisy Observations}}},
  author = {Bouttier, Clément and Gavra, Ioana},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  pages = {1--45},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/16-588.html},
  urldate = {2021-07-05},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Bouttier_Gavra\\Bouttier_Gavra_2019_Convergence Rate of a Simulated Annealing Algorithm with Noisy Observations.pdf;C\:\\Users\\felix\\Zotero\\storage\\3EVBRBLD\\16-588.html},
  number = {4}
}

@article{brayStatisticsCriticalPoints2007,
  title = {The Statistics of Critical Points of {{Gaussian}} Fields on Large-Dimensional Spaces},
  author = {Bray, Alan J. and Dean, David S.},
  date = {2007-04-10},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {98},
  pages = {150201},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.98.150201},
  abstract = {We calculate the average number of critical points of a Gaussian field on a high-dimensional space as a function of their energy and their index. Our results give a complete picture of the organization of critical points and are of relevance to glassy and disordered systems, and to landscape scenarios coming from the anthropic approach to string theory.},
  archiveprefix = {arXiv},
  eprint = {cond-mat/0611023},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2007_Bray_Dean\\Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces.pdf;C\:\\Users\\felix\\Zotero\\storage\\RV2A6TST\\0611023.html},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  number = {15}
}

@online{bubeckConvexOptimizationAlgorithms2015,
  title = {Convex {{Optimization}}: {{Algorithms}} and {{Complexity}}},
  shorttitle = {Convex {{Optimization}}},
  author = {Bubeck, Sébastien},
  date = {2015-11-16},
  url = {http://arxiv.org/abs/1405.4980},
  urldate = {2021-07-12},
  abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
  archiveprefix = {arXiv},
  eprint = {1405.4980},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2015_Bubeck\\Bubeck_2015_Convex Optimization.pdf;C\:\\Users\\felix\\Zotero\\storage\\CLBZ7TQV\\1405.html},
  keywords = {Computer Science - Computational Complexity,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@online{bubeckGeometricAlternativeNesterov2015,
  title = {A Geometric Alternative to {{Nesterov}}'s Accelerated Gradient Descent},
  author = {Bubeck, Sébastien and Lee, Yin Tat and Singh, Mohit},
  date = {2015-06-26},
  url = {http://arxiv.org/abs/1506.08187},
  urldate = {2021-07-07},
  abstract = {We propose a new method for unconstrained optimization of a smooth and strongly convex function, which attains the optimal rate of convergence of Nesterov’s accelerated gradient descent. The new algorithm has a simple geometric interpretation, loosely inspired by the ellipsoid method. We provide some numerical evidence that the new method can be superior to Nesterov’s accelerated gradient descent.},
  archiveprefix = {arXiv},
  eprint = {1506.08187},
  eprinttype = {arxiv},
  file = {C\:\\Users\\felix\\Zotero\\storage\\GRJP99VL\\Bubeck et al. - 2015 - A geometric alternative to Nesterov's accelerated .pdf},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  langid = {english},
  primaryclass = {cs, math}
}

@article{chaudhariEntropysgdBiasingGradient2019,
  title = {Entropy-Sgd: {{Biasing}} Gradient Descent into Wide Valleys},
  shorttitle = {Entropy-Sgd},
  author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  date = {2019},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2019},
  pages = {124018},
  publisher = {{IOP Publishing}},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Chaudhari et al\\Chaudhari et al_2019_Entropy-sgd.pdf},
  number = {12}
}

@online{choromanskaLossSurfacesMultilayer2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
  date = {2015-01-21},
  url = {http://arxiv.org/abs/1412.0233},
  urldate = {2021-06-16},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  archiveprefix = {arXiv},
  eprint = {1412.0233},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2015_Choromanska et al\\Choromanska et al_2015_The Loss Surfaces of Multilayer Networks.pdf;C\:\\Users\\felix\\Zotero\\storage\\2AF42JU2\\1412.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@online{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  date = {2014},
  archiveprefix = {arXiv},
  eprint = {1406.2572},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2014_Dauphin et al\\Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional.pdf;C\:\\Users\\felix\\Zotero\\storage\\ZICT2QCI\\1406.html}
}

@online{dinhSharpMinimaCan2017,
  title = {Sharp {{Minima Can Generalize For Deep Nets}}},
  author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  date = {2017-05-15},
  url = {http://arxiv.org/abs/1703.04933},
  urldate = {2021-06-16},
  abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
  archiveprefix = {arXiv},
  eprint = {1703.04933},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Dinh et al\\Dinh et al_2017_Sharp Minima Can Generalize For Deep Nets.pdf;C\:\\Users\\felix\\Zotero\\storage\\JT5F9JLJ\\1703.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@online{dontlooWhatDifferenceMomentum2016,
  title = {What's the Difference between Momentum Based Gradient Descent and {{Nesterov}}'s Accelerated Gradient Descent?},
  author = {{dontloo}, (https://stats.stackexchange.com/users/95569/dontloo)},
  date = {2016-01-21},
  url = {https://stats.stackexchange.com/q/191727},
  eprint = {https://stats.stackexchange.com/q/191727},
  howpublished = {Cross Validated}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  date = {2011-07-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {12},
  pages = {2121--2159},
  issn = {1532-4435},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  file = {C\:\\gdrive\\ZoteroPaper\\2011_Duchi et al\\Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf},
  issue = {null}
}

@online{dudaImprovingSGDConvergence2019,
  title = {Improving {{SGD}} Convergence by Online Linear Regression of Gradients in Multiple Statistically Relevant Directions},
  author = {Duda, Jarek},
  date = {2019-04-14},
  url = {http://arxiv.org/abs/1901.11457},
  urldate = {2021-06-17},
  abstract = {Deep neural networks are usually trained with stochastic gradient descent (SGD), which minimizes objective function using very rough approximations of gradient, only averaging to the real gradient. Standard approaches like momentum or ADAM only consider a single direction, and do not try to model distance from extremum - neglecting valuable information from calculated sequence of gradients, often stagnating in some suboptimal plateau. Second order methods could exploit these missed opportunities, however, beside suffering from very large cost and numerical instabilities, many of them attract to suboptimal points like saddles due to negligence of signs of curvatures (as eigenvalues of Hessian).},
  archiveprefix = {arXiv},
  eprint = {1901.11457},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Duda\\Duda_2019_Improving SGD convergence by online linear regression of gradients in multiple.pdf},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryclass = {cs, stat}
}

@online{dziugaiteComputingNonvacuousGeneralization2017,
  title = {Computing {{Nonvacuous Generalization Bounds}} for {{Deep}} ({{Stochastic}}) {{Neural Networks}} with {{Many More Parameters}} than {{Training Data}}},
  author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
  date = {2017-10-18},
  url = {http://arxiv.org/abs/1703.11008},
  urldate = {2021-06-16},
  abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this "deep learning" regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
  archiveprefix = {arXiv},
  eprint = {1703.11008},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Dziugaite_Roy\\Dziugaite_Roy_2017_Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural.pdf;C\:\\Users\\felix\\Zotero\\storage\\LLHIHT9F\\1703.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@online{flammarionAveragingAccelerationThere2015,
  title = {From {{Averaging}} to {{Acceleration}}, {{There}} Is {{Only}} a {{Step}}-Size},
  author = {Flammarion, Nicolas and Bach, Francis},
  date = {2015-04-07},
  url = {http://arxiv.org/abs/1504.01577},
  urldate = {2021-06-21},
  abstract = {We show that accelerated gradient descent, averaged gradient descent and the heavyball method for non-strongly-convex problems may be reformulated as constant parameter second-order difference equation algorithms, where stability of the system is equivalent to convergence at rate O(1/n2), where n is the number of iterations. We provide a detailed analysis of the eigenvalues of the corresponding linear dynamical system, showing various oscillatory and non-oscillatory behaviors, together with a sharp stability result with explicit constants. We also consider the situation where noisy gradients are available, where we extend our general convergence result, which suggests an alternative algorithm (i.e., with different step sizes) that exhibits the good aspects of both averaging and acceleration.},
  archiveprefix = {arXiv},
  eprint = {1504.01577},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2015_Flammarion_Bach\\Flammarion_Bach_2015_From Averaging to Acceleration, There is Only a Step-size.pdf},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  langid = {english},
  primaryclass = {math, stat}
}

@online{freemanTopologyGeometryHalfRectified2017,
  title = {Topology and {{Geometry}} of {{Half}}-{{Rectified Network Optimization}}},
  author = {Freeman, C. Daniel and Bruna, Joan},
  date = {2017-06-01},
  url = {http://arxiv.org/abs/1611.01540},
  urldate = {2021-06-16},
  abstract = {The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model. In this work, we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our theoretical work quantifies and formalizes two important \textbackslash emph\{folklore\} facts: (i) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones, and (ii) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay. The conditioning of gradient descent is the next challenge we address. We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks. Our empirical results show that these level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays, in accordance to what is observed in practice with very low curvature attractors.},
  archiveprefix = {arXiv},
  eprint = {1611.01540},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Freeman_Bruna\\Freeman_Bruna_2017_Topology and Geometry of Half-Rectified Network Optimization.pdf;C\:\\Users\\felix\\Zotero\\storage\\52A8JMKD\\1611.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{garipovLossSurfacesMode2018,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2018-10-30},
  url = {http://arxiv.org/abs/1802.10026},
  urldate = {2021-06-15},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
  archiveprefix = {arXiv},
  eprint = {1802.10026},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Garipov et al\\Garipov et al_2018_Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs.pdf;C\:\\Users\\felix\\Zotero\\storage\\SETXQE29\\1802.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{gdawiecRobustNewtonMethod2021,
  title = {On the Robust {{Newton}}’s Method with the {{Mann}} Iteration and the Artistic Patterns from Its Dynamics},
  author = {Gdawiec, Krzysztof and Kotarski, Wiesław and Lisowska, Agnieszka},
  date = {2021-03-01},
  journaltitle = {Nonlinear Dynamics},
  shortjournal = {Nonlinear Dyn},
  volume = {104},
  pages = {297--331},
  issn = {1573-269X},
  doi = {10.1007/s11071-021-06306-5},
  abstract = {There are two main aims of this paper. The first one is to show some improvement of the robust Newton’s method (RNM) introduced recently by Kalantari. The RNM is a generalisation of the well-known Newton’s root finding method. Since the base method is undefined at critical points, the RNM allows working also at such points. In this paper, we improve the RNM method by applying the Mann iteration instead of the standard Picard iteration. This leads to an essential decrease in the number of root finding steps without visible destroying the sharp boundaries among the basins of attractions presented in polynomiographs. Furthermore, we investigate visually the dynamics of the RNM with the Mann iteration together with the basins of attraction for varying Mann’s iteration parameter with the help of polynomiographs for several polynomials. The second aim of this paper is to present the intriguing polynomiographs obtained from the dynamics of the RNM with the Mann iteration under various sequences used in this iteration. The obtained polynomiographs differ considerably from the ones obtained with the RNM and are interesting from the artistic perspective. Moreover, they can easily find applications in wallpaper or fabric design.},
  file = {C\:\\gdrive\\ZoteroPaper\\2021_Gdawiec et al\\Gdawiec et al_2021_On the robust Newton’s method with the Mann iteration and the artistic patterns.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{geEscapingSaddlePoints2015,
  title = {Escaping {{From Saddle Points}} — {{Online Stochastic Gradient}} for {{Tensor Decomposition}}},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  date = {2015-06-26},
  pages = {797--842},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v40/Ge15.html},
  urldate = {2021-06-17},
  abstract = {We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradi...},
  eventtitle = {Conference on {{Learning Theory}}},
  file = {C\:\\Users\\felix\\Zotero\\storage\\DGPALN9K\\Ge et al. - 2015 - Escaping From Saddle Points — Online Stochastic Gr.pdf;C\:\\Users\\felix\\Zotero\\storage\\9RW3A2KU\\Ge15.html},
  langid = {english}
}

@inproceedings{ghadimiGlobalConvergenceHeavyball2015,
  title = {Global Convergence of the {{Heavy}}-Ball Method for Convex Optimization},
  booktitle = {2015 {{European Control Conference}} ({{ECC}})},
  author = {Ghadimi, Euhanna and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  date = {2015-07},
  pages = {310--315},
  doi = {10.1109/ECC.2015.7330562},
  abstract = {This paper establishes global convergence and provides global bounds of the rate of convergence for the Heavy-ball method for convex optimization. When the objective function has Lipschitz-continuous gradient, we show that the Cesáro average of the iterates converges to the optimum at a rate of O(1/k) where k is the number of iterations. When the objective function is also strongly convex, we prove that the Heavy-ball iterates converge linearly to the unique optimum. Numerical examples validate our theoretical findings.},
  eventtitle = {2015 {{European Control Conference}} ({{ECC}})},
  file = {C\:\\gdrive\\ZoteroPaper\\2015_Ghadimi et al\\Ghadimi et al_2015_Global convergence of the Heavy-ball method for convex optimization.pdf;C\:\\Users\\felix\\Zotero\\storage\\W2IV5QKI\\7330562.html},
  keywords = {Acceleration,Algorithm design and analysis,Convergence,Convex functions,Gradient methods,Linear programming,Radio frequency}
}

@online{gibianskyHessianFreeOptimization2014,
  title = {Hessian {{Free Optimization}}},
  author = {Gibiansky, Andrew},
  date = {2014-02-13},
  url = {https://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/},
  urldate = {2021-06-07},
  abstract = {Training a neural network involves minimizing its error with respect to its parameters, and since larger neural networks may have millions of parameters, this poses quite a challenge. Surprisingly, many of the recent advances in neural networks come not from improved data processing, neural network architectures, or other machine learning tricks, but instead from incredibly powerful methods for function minimization. In this notebook, we'll go through several optimization methods and work our way up to Hessian Free Optimization, a powerful optimization method adapted to neural networks by James Martens in 2010.},
  file = {C\:\\Users\\felix\\Zotero\\storage\\BNVPEN9L\\hessian-free-optimization.html},
  organization = {{Andrew Gibiansky :: Math -{$>$} [Code]}}
}

@article{gohWhyMomentumReally2017,
  title = {Why {{Momentum Really Works}}},
  author = {Goh, Gabriel},
  date = {2017-04-04},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  pages = {e6},
  issn = {2476-0757},
  doi = {10.23915/distill.00006},
  abstract = {We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.},
  file = {C\:\\Users\\felix\\Zotero\\storage\\SQTH5E9A\\momentum.html},
  langid = {english},
  number = {4}
}

@online{haghighiNumericalOptimizationUnderstanding2014,
  title = {Numerical {{Optimization}}: {{Understanding L}}-{{BFGS}}},
  shorttitle = {Numerical {{Optimization}}},
  author = {Haghighi, Aria},
  date = {2014-12-02},
  url = {http://aria42.com/blog/2014/12/understanding-lbfgs},
  urldate = {2021-06-17},
  abstract = {Numerical optimization is at the core of much of machine learning. In this post, we derive the L-BFGS algorithm, commonly used in batch machine learning applications.},
  file = {C\:\\Users\\felix\\Zotero\\storage\\YPWGNHR8\\understanding-lbfgs.html},
  organization = {{aria42}}
}

@book{hairerSolvingOrdinaryDifferential1993,
  title = {Solving {{Ordinary Differential Equations I}}: {{Nonstiff Problems}}},
  shorttitle = {Solving {{Ordinary Differential Equations I}}},
  author = {Hairer, Ernst},
  date = {1993},
  edition = {Second Revised Edition},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78862-1},
  abstract = {This book deals with methods for solving nonstiff ordinary differential equations. The first chapter describes the historical development of the classical theory from Newton, Leibniz, Euler, and Hamilton to limit cycles and strange attractors. In a second chapter a modern treatment of Runge-Kutta and extrapolation methods is given. Also included are continuous methods for dense output, parallel Runge-Kutta methods, special methods for Hamiltonian systems, second order differential equations and delay equations. The third chapter begins with the classical theory of multistep methods, and concludes with the theory of general linear methods. Many applications from physics, chemistry, biology, and astronomy together with computer programs and numerical comparisons are presented. The book will be immensely useful to graduate students and researchers in numerical analysis and scientific computing, and to scientists in the fields mentioned above. "This is the revised version of the first edition of Vol. I published in 1987. ….Vols. I and II (SSCM 14) of Solving Ordinary Differential Equations together are the standard text on numerical methods for ODEs. ..This book is well written and is together with Vol. II, the most comprehensive modern text on numerical integration methods for ODEs. It may serve a a text book for graduate courses, ..and also as a reference book for all those who have to solve ODE problems numerically." Zeitschrift für Angewandte Mathematik und Physik},
  editora = {Wanner, Gerhard and Nørsett, Syvert P.},
  editoratype = {collaborator},
  file = {C\:\\gdrive\\ZoteroPaper\\1993_Hairer\\Hairer_1993_Solving Ordinary Differential Equations I.pdf},
  isbn = {978-3-540-78862-1},
  keywords = {Global analysis (Mathematics); Numerical analysis; Mathematics},
  langid = {english},
  number = {8},
  pagetotal = {xv+528},
  series = {Springer {{Series}} in {{Computational Mathematics}}}
}

@online{hardtTrainFasterGeneralize2016,
  title = {Train Faster, Generalize Better: {{Stability}} of Stochastic Gradient Descent},
  shorttitle = {Train Faster, Generalize Better},
  author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  date = {2016-02-07},
  url = {http://arxiv.org/abs/1509.01240},
  urldate = {2021-04-19},
  abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.},
  archiveprefix = {arXiv},
  eprint = {1509.01240},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2016_Hardt et al\\Hardt et al_2016_Train faster, generalize better.pdf;C\:\\Users\\felix\\Zotero\\storage\\Z4N4PGBE\\1509.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@incollection{hendersonTheoryPracticeSimulated2003,
  title = {The {{Theory}} and {{Practice}} of {{Simulated Annealing}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Henderson, Darrall and Jacobson, Sheldon H. and Johnson, Alan W.},
  editor = {Glover, Fred and Kochenberger, Gary A.},
  date = {2003},
  pages = {287--319},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/0-306-48056-5_10},
  abstract = {Simulated annealing is a popular local search meta-heuristic used to address discrete and, to a lesser extent, continuous optimization problems. The key feature of simulated annealing is that it provides a means to escape local optima by allowing hill-climbing moves (i.e., moves which worsen the objective function value) in hopes of finding a global optimum. A brief history of simulated annealing is presented, including a review of its application to discrete and continuous optimization problems. Convergence theory for simulated annealing is reviewed, as well as recent advances in the analysis of finite time performance. Other local search algorithms are discussed in terms of their relationship to simulated annealing. The chapter also presents practical guidelines for the implementation of simulated annealing in terms of cooling schedules, neighborhood functions, and appropriate applications.},
  file = {C\:\\gdrive\\ZoteroPaper\\2003_Henderson et al\\Henderson et al_2003_The Theory and Practice of Simulated Annealing.pdf},
  isbn = {978-0-306-48056-0},
  keywords = {Heuristics,Local Search Algorithms,Meta-heuristics,Simulated Annealing},
  langid = {english},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}}
}

@online{heuselGANsTrainedTwo2018,
  title = {{{GANs Trained}} by a {{Two Time}}-{{Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2018-01-12},
  url = {http://arxiv.org/abs/1706.08500},
  urldate = {2021-06-08},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  archiveprefix = {arXiv},
  eprint = {1706.08500},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Heusel et al\\Heusel et al_2018_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf;C\:\\Users\\felix\\Zotero\\storage\\IF4TQMVR\\1706.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{hochreiterFlatMinima1997,
  title = {Flat {{Minima}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-01-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {9},
  pages = {1--42},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.1.1},
  abstract = {We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a “flat” minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to “simple” networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a “good” weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and “optimal brain surgeon/optimal brain damage.”},
  file = {C\:\\gdrive\\ZoteroPaper\\1997_Hochreiter_Schmidhuber\\Hochreiter_Schmidhuber_1997_Flat Minima.pdf;C\:\\gdrive\\ZoteroPaper\\1997_Hochreiter_Schmidhuber\\Hochreiter_Schmidhuber_1997_Flat Minima2.pdf;C\:\\Users\\felix\\Zotero\\storage\\T24KCAGB\\Flat-Minima.html},
  number = {1}
}

@online{hofferTrainLongerGeneralize2018,
  title = {Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks},
  shorttitle = {Train Longer, Generalize Better},
  author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  date = {2018-01-01},
  url = {http://arxiv.org/abs/1705.08741},
  urldate = {2021-05-27},
  abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
  archiveprefix = {arXiv},
  eprint = {1705.08741},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Hoffer et al\\Hoffer et al_2018_Train longer, generalize better.pdf;C\:\\Users\\felix\\Zotero\\storage\\R3E6INJM\\1705.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2019-02-25},
  url = {http://arxiv.org/abs/1803.05407},
  urldate = {2021-06-15},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archiveprefix = {arXiv},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Izmailov et al\\Izmailov et al_2019_Averaging Weights Leads to Wider Optima and Better Generalization.pdf;C\:\\Users\\felix\\Zotero\\storage\\CH682FMV\\1803.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{jastrzebskiThreeFactorsInfluencing2018,
  title = {Three {{Factors Influencing Minima}} in {{SGD}}},
  author = {Jastrzębski, Stanisław and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  date = {2018-09-13},
  url = {http://arxiv.org/abs/1711.04623},
  urldate = {2021-05-27},
  abstract = {We investigate the dynamical and convergent properties of stochastic gradient descent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization, remains an open question. In order to tackle this problem we investigate the previously proposed approximation of SGD by a stochastic differential equation (SDE). We theoretically argue that three factors - learning rate, batch size and gradient covariance - influence the minima found by SGD. In particular we find that the ratio of learning rate to batch size is a key determinant of SGD dynamics and of the width of the final minima, and that higher values of the ratio lead to wider minima and often better generalization. We confirm these findings experimentally. Further, we include experiments which show that learning rate schedules can be replaced with batch size schedules and that the ratio of learning rate to batch size is an important factor influencing the memorization process.},
  archiveprefix = {arXiv},
  eprint = {1711.04623},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Jastrzębski et al\\Jastrzębski et al_2018_Three Factors Influencing Minima in SGD.pdf;C\:\\Users\\felix\\Zotero\\storage\\YPZIQZFC\\1711.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inproceedings{jiaInformationTheoreticLocalMinima2020,
  title = {Information-{{Theoretic Local Minima Characterization}} and {{Regularization}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Jia, Zhiwei and Su, Hao},
  date = {2020-11-21},
  pages = {4773--4783},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/jia20a.html},
  urldate = {2021-05-27},
  abstract = {Recent advances in deep learning theory have evoked the study of generalizability across different local minima of deep neural networks (DNNs). While current work focused on either discovering prop...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {C\:\\gdrive\\ZoteroPaper\\2020_Jia_Su\\Jia_Su_2020_Information-Theoretic Local Minima Characterization and Regularization.pdf;C\:\\Users\\felix\\Zotero\\storage\\9WX7XUPX\\jia20a.html},
  langid = {english}
}

@online{kawaguchiGeneralizationDeepLearning2020,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  date = {2020-07-27},
  url = {http://arxiv.org/abs/1710.05468},
  urldate = {2021-06-16},
  abstract = {This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.},
  archiveprefix = {arXiv},
  eprint = {1710.05468},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2020_Kawaguchi et al\\Kawaguchi et al_2020_Generalization in Deep Learning.pdf;C\:\\Users\\felix\\Zotero\\storage\\YZZMGICA\\1710.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{keskarLargeBatchTrainingDeep2017,
  title = {On {{Large}}-{{Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large}}-{{Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  date = {2017-02-09},
  url = {http://arxiv.org/abs/1609.04836},
  urldate = {2021-05-27},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arXiv},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Keskar et al\\Keskar et al_2017_On Large-Batch Training for Deep Learning.pdf;C\:\\Users\\felix\\Zotero\\storage\\MRJDPXL5\\1609.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  primaryclass = {cs, math}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2021-04-19},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Kingma_Ba\\Kingma_Ba_2017_Adam.pdf;C\:\\Users\\felix\\Zotero\\storage\\6WJBPWQT\\1412.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@inproceedings{lerouxTopmoumouteOnlineNatural2007,
  title = {Topmoumoute {{Online Natural Gradient Algorithm}}.},
  booktitle = {{{NIPS}}},
  author = {Le Roux, Nicolas and Manzagol, Pierre-Antoine and Bengio, Yoshua},
  date = {2007},
  pages = {849--856},
  publisher = {{Citeseer}},
  file = {C\:\\gdrive\\ZoteroPaper\\2007_Le Roux et al\\Le Roux et al_2007_Topmoumoute Online Natural Gradient Algorithm.pdf}
}

@online{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  date = {2018-11-07},
  url = {http://arxiv.org/abs/1712.09913},
  urldate = {2021-06-16},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix = {arXiv},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Li et al\\Li et al_2018_Visualizing the Loss Landscape of Neural Nets.pdf;C\:\\Users\\felix\\Zotero\\storage\\CI5Q2T87\\1712.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{mandtStochasticGradientDescent2018,
  ids = {mandtStochasticGradientDescent2017},
  title = {Stochastic {{Gradient Descent}} as {{Approximate Bayesian Inference}}},
  author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  date = {2018-01-19},
  url = {http://arxiv.org/abs/1704.04289},
  urldate = {2021-06-08},
  abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
  archiveprefix = {arXiv},
  eprint = {1704.04289},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Mandt et al\\Mandt et al_2017_Stochastic gradient descent as approximate bayesian inference.pdf;C\:\\gdrive\\ZoteroPaper\\2018_Mandt et al\\Mandt et al_2018_Stochastic Gradient Descent as Approximate Bayesian Inference.pdf;C\:\\Users\\felix\\Zotero\\storage\\3F563NK4\\1704.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inproceedings{martensDeepLearningHessianfree2010,
  title = {Deep Learning via {{Hessian}}-Free Optimization},
  booktitle = {{{ICML}}},
  author = {Martens, James},
  date = {2010},
  volume = {27},
  pages = {8},
  abstract = {We develop a 2nd-order optimization method based on the “Hessian-free” approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton \& Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn’t limited in applicability to autoencoders, or any specific model class. We also discuss the issue of “pathological curvature” as a possible explanation for the difficulty of deeplearning and how 2nd-order optimization, and our method in particular, effectively deals with it.},
  file = {C\:\\gdrive\\ZoteroPaper\\2010_Martens\\Martens_2010_Deep learning via hessian-free optimization2.pdf},
  langid = {english}
}

@online{martensOptimizingNeuralNetworks2020,
  title = {Optimizing {{Neural Networks}} with {{Kronecker}}-Factored {{Approximate Curvature}}},
  author = {Martens, James and Grosse, Roger},
  date = {2020-06-07},
  url = {http://arxiv.org/abs/1503.05671},
  urldate = {2021-06-17},
  abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network’s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC’s approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
  archiveprefix = {arXiv},
  eprint = {1503.05671},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2020_Martens_Grosse\\Martens_Grosse_2020_Optimizing Neural Networks with Kronecker-factored Approximate Curvature.pdf},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  langid = {english},
  primaryclass = {cs, stat}
}

@online{MirrorDecent,
  title = {Mirror {{Decent}}},
  url = {http://www.cs.cmu.edu/~15850/notes/lec19.pdf},
  urldate = {2021-07-07},
  file = {C\:\\Users\\felix\\Zotero\\storage\\GYI8GEZM\\lec19.pdf;C\:\\Users\\felix\\Zotero\\storage\\LN4AWSSJ\\mirror_descent.pdf}
}

@article{MomentumTermGradient1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  date = {1999-01-01},
  journaltitle = {Neural Networks},
  volume = {12},
  pages = {145--151},
  publisher = {{Pergamon}},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00116-6},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improv…},
  file = {C\:\\gdrive\\ZoteroPaper\\1999_\\1999_On the momentum term in gradient descent learning algorithms.pdf;C\:\\Users\\felix\\Zotero\\storage\\26VLHHCM\\S0893608098001166.html},
  langid = {english},
  number = {1}
}

@inproceedings{mulayoffUniquePropertiesFlat2020,
  title = {Unique {{Properties}} of {{Flat Minima}} in {{Deep Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Mulayoff, Rotem and Michaeli, Tomer},
  date = {2020-11-21},
  pages = {7108--7118},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/mulayoff20a.html},
  urldate = {2021-05-27},
  abstract = {It is well known that (stochastic) gradient descent has an implicit bias towards flat minima. In deep neural network training, this mechanism serves to screen out minima. However, the precise effec...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {C\:\\Users\\felix\\Zotero\\storage\\J5462BX9\\Mulayoff and Michaeli - 2020 - Unique Properties of Flat Minima in Deep Networks.pdf;C\:\\Users\\felix\\Zotero\\storage\\3Z7EQ2T8\\mulayoff20a.html},
  langid = {english}
}

@online{neelakantanAddingGradientNoise2015,
  title = {Adding {{Gradient Noise Improves Learning}} for {{Very Deep Networks}}},
  author = {Neelakantan, Arvind and Vilnis, Luke and Le, Quoc V. and Sutskever, Ilya and Kaiser, Lukasz and Kurach, Karol and Martens, James},
  date = {2015-11-20},
  url = {http://arxiv.org/abs/1511.06807},
  urldate = {2021-06-08},
  abstract = {Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. The main motivation for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72\% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures.},
  archiveprefix = {arXiv},
  eprint = {1511.06807},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2015_Neelakantan et al\\Neelakantan et al_2015_Adding Gradient Noise Improves Learning for Very Deep Networks.pdf;C\:\\Users\\felix\\Zotero\\storage\\PIBJNLF7\\1511.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{nesterovAcceleratingCubicRegularization2008,
  title = {Accelerating the Cubic Regularization of {{Newton}}’s Method on Convex Problems},
  author = {Nesterov, Yurii Evgen'evič},
  date = {2008-03-01},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  volume = {112},
  pages = {159--181},
  issn = {1436-4646},
  doi = {10.1007/s10107-006-0089-x},
  abstract = {In this paper we propose an accelerated version of the cubic regularization of Newton’s method (Nesterov and Polyak, in Math Program 108(1): 177–205, 2006). The original version, used for minimizing a convex function with Lipschitz-continuous Hessian, guarantees a global rate of convergence of order \$\$O\textbackslash big(\{1 \textbackslash over k\^2\}\textbackslash big)\$\$, where k is the iteration counter. Our modified version converges for the same problem class with order \$\$O\textbackslash big(\{1 \textbackslash over k\^3\}\textbackslash big)\$\$, keeping the complexity of each iteration unchanged. We study the complexity of both schemes on different classes of convex problems. In particular, we argue that for the second-order schemes, the class of non-degenerate problems is different from the standard class.},
  file = {C\:\\gdrive\\ZoteroPaper\\2008_Nesterov\\Nesterov_2008_Accelerating the cubic regularization of Newton’s method on convex problems.pdf;C\:\\gdrive\\ZoteroPaper\\2008_Nesterov\\Nesterov_2008_Accelerating the cubic regularization of Newton’s method on convex problems2.pdf},
  langid = {english},
  number = {1}
}

@book{nesterovIntroductoryLecturesConvex2004,
  title = {Introductory {{Lectures}} on {{Convex Optimization}}},
  author = {Nesterov, Yurii Evgen'evič},
  date = {2004},
  volume = {87},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4419-8853-9},
  abstract = {The first elementary exposition of core ideas of complexity theory for convex optimization, this book explores optimal methods and lower complexity bounds for smooth and non-smooth convex optimization. Also covers polynomial-time interior-point methods.},
  editorb = {Pardalos, Panos M. and Hearn, Donald W.},
  editorbtype = {redactor},
  file = {C\:\\gdrive\\ZoteroPaper\\2004_Nesterov\\Nesterov_2004_Introductory Lectures on Convex Optimization.pdf},
  isbn = {978-1-4613-4691-3 978-1-4419-8853-9},
  langid = {english},
  series = {Applied {{Optimization}}}
}

@book{nesterovLecturesConvexOptimization2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii Evgen'evič},
  date = {2018},
  edition = {Second edition},
  publisher = {{Springer}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-91578-4},
  abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author’s lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics, Introduction -- Part I Black-Box Optimization -- 1 Nonlinear Optimization -- 2 Smooth Convex Optimization -- 3 Nonsmooth Convex Optimization -- 4 Second-Order Methods -- Part II Structural Optimization -- 5 Polynomial-time Interior-Point Methods -- 6 Primal-Dual Model of Objective Function -- 7 Optimization in Relative Scale -- Bibliographical Comments -- Appendix A. Solving some Auxiliary Optimization Problems -- References -- Index},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Nesterov\\Nesterov_2018_Lectures on Convex Optimization.pdf},
  isbn = {978-3-319-91578-4},
  keywords = {Computer software; Optimization; Mathematical optimization; Algorithms,Konvexe Optimierung},
  langid = {english},
  series = {Springer Optimization and {{Its}} Applications; Volume 137}
}

@article{nesterovMethodSolvingConvex1983,
  title = {A Method for Solving the Convex Programming Problem with Convergence Rate \(O(1/k^2)\)},
  author = {Nesterov, Yurii Evgen'evič},
  date = {1983},
  journaltitle = {Dokl. Akad. Nauk SSSR},
  volume = {269},
  pages = {543--547},
  url = {http://m.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=46009&option_lang=eng},
  urldate = {2021-07-17},
  file = {C\:\\gdrive\\ZoteroPaper\\1983_Nesterov\\Nesterov_1983_A method for solving the convex programming problem with convergence rate.pdf;C\:\\Users\\felix\\Zotero\\storage\\LUQ6H5UH\\10029946121.html}
}

@article{olahResearchDebt2017,
  title = {Research {{Debt}}},
  author = {Olah, Chris and Carter, Shan},
  date = {2017-03-22},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  pages = {e5},
  issn = {2476-0757},
  doi = {10.23915/distill.00005},
  abstract = {Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...},
  file = {C\:\\Users\\felix\\Zotero\\storage\\N7U53ANN\\research-debt.html},
  langid = {english},
  number = {3}
}

@online{papyanFullSpectrumDeepnet2019,
  title = {The {{Full Spectrum}} of {{Deepnet Hessians}} at {{Scale}}: {{Dynamics}} with {{SGD Training}} and {{Sample Size}}},
  shorttitle = {The {{Full Spectrum}} of {{Deepnet Hessians}} at {{Scale}}},
  author = {Papyan, Vardan},
  date = {2019-06-02},
  url = {http://arxiv.org/abs/1811.07062},
  urldate = {2021-06-30},
  abstract = {We apply state-of-the-art tools in modern high-dimensional numerical linear algebra to approximate efficiently the spectrum of the Hessian of modern deepnets, with tens of millions of parameters, trained on real data. Our results corroborate previous findings, based on small-scale networks, that the Hessian exhibits "spiked" behavior, with several outliers isolated from a continuous bulk. We decompose the Hessian into different components and study the dynamics with training and sample size of each term individually.},
  archiveprefix = {arXiv},
  eprint = {1811.07062},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Papyan\\Papyan_2019_The Full Spectrum of Deepnet Hessians at Scale.pdf;C\:\\Users\\felix\\Zotero\\storage\\MTD5A8W3\\1811.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{pascanuRevisitingNaturalGradient2014,
  title = {Revisiting {{Natural Gradient}} for {{Deep Networks}}},
  author = {Pascanu, Razvan and Bengio, Yoshua},
  date = {2014-02-17},
  url = {http://arxiv.org/abs/1301.3584},
  urldate = {2021-06-30},
  abstract = {We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods for training deep models: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.},
  archiveprefix = {arXiv},
  eprint = {1301.3584},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2014_Pascanu_Bengio\\Pascanu_Bengio_2014_Revisiting Natural Gradient for Deep Networks.pdf;C\:\\gdrive\\ZoteroPaper\\2014_Pascanu_Bengio\\Pascanu_Bengio_2014_Revisiting Natural Gradient for Deep Networks2.pdf;C\:\\Users\\felix\\Zotero\\storage\\M4Y4MKA5\\1301.html;C\:\\Users\\felix\\Zotero\\storage\\PGMXBL9X\\1301.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  primaryclass = {cs}
}

@article{polyakAccelerationStochasticApproximation1992,
  title = {Acceleration of Stochastic Approximation by Averaging},
  author = {Polyak, Boris T. and Juditsky, Anatoli B.},
  date = {1992},
  journaltitle = {SIAM journal on control and optimization},
  volume = {30},
  pages = {838--855},
  publisher = {{SIAM}},
  file = {C\:\\gdrive\\ZoteroPaper\\1992_Polyak_Juditsky\\Polyak_Juditsky_1992_Acceleration of stochastic approximation by averaging.pdf;C\:\\Users\\felix\\Zotero\\storage\\C53WPIPY\\0330046.html},
  number = {4}
}

@article{polyakMethodsSpeedingConvergence1964,
  title = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author = {Polyak, B. T.},
  date = {1964-01-01},
  journaltitle = {USSR Computational Mathematics and Mathematical Physics},
  shortjournal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {4},
  pages = {1--17},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(64)90137-5},
  abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.},
  file = {C\:\\gdrive\\ZoteroPaper\\1964_Polyak\\Polyak_1964_Some methods of speeding up the convergence of iteration methods.pdf;C\:\\Users\\felix\\Zotero\\storage\\YHYZHVCI\\0041555364901375.html},
  langid = {english},
  number = {5}
}

@article{rebentrostQuantumGradientDescent2019,
  title = {Quantum Gradient Descent and {{Newton}}’s Method for Constrained Polynomial Optimization},
  author = {Rebentrost, Patrick and Schuld, Maria and Wossnig, Leonard and Petruccione, Francesco and Lloyd, Seth},
  date = {2019-07-17},
  journaltitle = {New Journal of Physics},
  shortjournal = {New J. Phys.},
  volume = {21},
  pages = {073023},
  publisher = {{IOP Publishing}},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/ab2a9e},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Rebentrost et al\\Rebentrost et al_2019_Quantum gradient descent and Newton’s method for constrained polynomial.pdf;C\:\\Users\\felix\\Zotero\\storage\\5MD8SPQ6\\ab2a9e.html},
  langid = {english},
  number = {7}
}

@online{reddiConvergenceAdam2019,
  title = {On the {{Convergence}} of {{Adam}} and {{Beyond}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  date = {2019-04-19},
  url = {http://arxiv.org/abs/1904.09237},
  urldate = {2021-06-08},
  abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  archiveprefix = {arXiv},
  eprint = {1904.09237},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Reddi et al\\Reddi et al_2019_On the Convergence of Adam and Beyond.pdf;C\:\\Users\\felix\\Zotero\\storage\\7HJCRKB3\\1904.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@online{ruderOverviewGradientDescent2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  date = {2017-06-15},
  url = {http://arxiv.org/abs/1609.04747},
  urldate = {2021-06-07},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  eprint = {1609.04747},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Ruder\\Ruder_2017_An overview of gradient descent optimization algorithms.pdf;C\:\\Users\\felix\\Zotero\\storage\\RZCQGVS2\\1609.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@online{sakuraiAlternatingOptimizationMethod2016,
  title = {Alternating Optimization Method Based on Nonnegative Matrix Factorizations for Deep Neural Networks},
  author = {Sakurai, Tetsuya and Imakura, Akira and Inoue, Yuto and Futamura, Yasunori},
  date = {2016-05-15},
  url = {http://arxiv.org/abs/1605.04639},
  urldate = {2021-06-08},
  abstract = {The backpropagation algorithm for calculating gradients has been widely used in computation of weights for deep neural networks (DNNs). This method requires derivatives of objective functions and has some difficulties finding appropriate parameters such as learning rate. In this paper, we propose a novel approach for computing weight matrices of fully-connected DNNs by using two types of semi-nonnegative matrix factorizations (semi-NMFs). In this method, optimization processes are performed by calculating weight matrices alternately, and backpropagation (BP) is not used. We also present a method to calculate stacked autoencoder using a NMF. The output results of the autoencoder are used as pre-training data for DNNs. The experimental results show that our method using three types of NMFs attains similar error rates to the conventional DNNs with BP.},
  archiveprefix = {arXiv},
  eprint = {1605.04639},
  eprinttype = {arxiv},
  file = {C\:\\Users\\felix\\Zotero\\storage\\RZVZADR9\\Sakurai et al. - 2016 - Alternating optimization method based on nonnegati.pdf},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  langid = {english},
  primaryclass = {cs, stat}
}

@online{schmidtDescendingCrowdedValley2021,
  title = {Descending through a {{Crowded Valley}} -- {{Benchmarking Deep Learning Optimizers}}},
  author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
  date = {2021-02-11},
  url = {http://arxiv.org/abs/2007.01547},
  urldate = {2021-06-15},
  abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of fifteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than \$50,000\$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific optimizers and parameter choices that generally lead to competitive results in our experiments: Adam remains a strong contender, with newer methods failing to significantly and consistently outperform it. Our open-sourced results are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.},
  archiveprefix = {arXiv},
  eprint = {2007.01547},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2021_Schmidt et al\\Schmidt et al_2021_Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers.pdf;C\:\\Users\\felix\\Zotero\\storage\\TJ3EARHJ\\2007.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{schneiderDeepOBSDeepLearning2019,
  title = {{{DeepOBS}}: {{A Deep Learning Optimizer Benchmark Suite}}},
  shorttitle = {{{DeepOBS}}},
  author = {Schneider, Frank and Balles, Lukas and Hennig, Philipp},
  date = {2019-03-13},
  url = {http://arxiv.org/abs/1903.05499},
  urldate = {2021-05-19},
  abstract = {Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary contribution, we present DeepOBS, a Python package of deep learning optimization benchmarks. The package addresses key challenges in the quantitative assessment of stochastic optimizers, and automates most steps of benchmarking. The library includes a wide and extensible set of ready-to-use realistic optimization problems, such as training Residual Networks for image classification on ImageNet or character-level language prediction models, as well as popular classics like MNIST and CIFAR-10. The package also provides realistic baseline results for the most popular optimizers on these test problems, ensuring a fair comparison to the competition when benchmarking new optimizers, and without having to run costly experiments. It comes with output back-ends that directly produce LaTeX code for inclusion in academic publications. It supports TensorFlow and is available open source.},
  archiveprefix = {arXiv},
  eprint = {1903.05499},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Schneider et al\\Schneider et al_2019_DeepOBS.pdf;C\:\\Users\\felix\\Zotero\\storage\\8MUAXKAE\\1903.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{schwartzRungeKuttaDiscretizationOptimal1996,
  title = {Runge-{{Kutta Discretization}} of {{Optimal Control Problems}}},
  author = {Schwartz, A. and Polak, E.},
  date = {1996-12-01},
  journaltitle = {IFAC Proceedings Volumes},
  shortjournal = {IFAC Proceedings Volumes},
  volume = {29},
  pages = {123--128},
  issn = {1474-6670},
  doi = {10.1016/S1474-6670(17)43687-1},
  abstract = {Runge-Kutta integration is used to construct finite-dimensional approximating problems that are consistent approximations, in the sense of Polak (1993), to an original optimal control problem. Stationary points and global solutions of these approximating discrete-time optimal control problems converge, as the discretization level is increased, to stationary points and global solutions of the original problem. The approximating problems involve finite-dimensional spaces of control coefficients. In solving the discrete-time approximating problems, a non-Euclidean inner product should be used on these coefficient spaces to avoid ill-conditioning. This result applies to any discretization method, not just Runge-Kutta integration. Significantly, not all Runge-Kutta methods (even full-order methods) lead to consistent approximations.},
  file = {C\:\\gdrive\\ZoteroPaper\\1996_Schwartz_Polak\\Schwartz_Polak_1996_Runge-Kutta Discretization of Optimal Control Problems.pdf;C\:\\Users\\felix\\Zotero\\storage\\C7VYCHVN\\S1474667017436871.html},
  keywords = {numerical methods,optimal control,runge-kutta integration,splines},
  langid = {english},
  number = {8},
  series = {10th {{IFAC Workshop}} on {{Control Applications}} of {{Optimization}} 1995, {{Haifa}}, {{Israel}}, 19-21 {{December}}}
}

@book{shewchukIntroductionConjugateGradient1994,
  title = {An Introduction to the Conjugate Gradient Method without the Agonizing Pain},
  author = {Shewchuk, Jonathan Richard},
  date = {1994},
  publisher = {{Carnegie-Mellon University. Department of Computer Science}},
  file = {C\:\\gdrive\\ZoteroPaper\\1994_Shewchuk\\Shewchuk_1994_An introduction to the conjugate gradient method without the agonizing pain.pdf}
}

@video{simonsinstituteucberkeleyOptimization,
  title = {Optimization {{I}}},
  url = {https://www.youtube.com/watch?v=6WeyTUnbwQQ},
  urldate = {2021-07-07},
  abstract = {Big Data Boot Camp http://simons.berkeley.edu/talks/ben-...},
  editora = {{Simons Institute UC Berkeley} and Recht, Benjamin},
  editoratype = {collaborator},
  file = {C\:\\Users\\felix\\Zotero\\storage\\E2DJ8Y78\\rechtslides.pdf}
}

@online{simsekliTailIndexAnalysisStochastic2019,
  title = {A {{Tail}}-{{Index Analysis}} of {{Stochastic Gradient Noise}} in {{Deep Neural Networks}}},
  author = {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  date = {2019-01-17},
  url = {http://arxiv.org/abs/1901.06053},
  urldate = {2021-05-27},
  abstract = {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed \$\textbackslash alpha\$-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a L\textbackslash '\{e\}vy motion. Such SDEs can incur `jumps', which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the \$\textbackslash alpha\$-stable assumption, we conduct extensive experiments on common deep learning architectures and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We further investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.},
  archiveprefix = {arXiv},
  eprint = {1901.06053},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2019_Simsekli et al\\Simsekli et al_2019_A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks.pdf;C\:\\Users\\felix\\Zotero\\storage\\L46UQ4VK\\1901.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@inreference{SimulatedAnnealing2021,
  title = {Simulated Annealing},
  booktitle = {Wikipedia},
  date = {2021-04-13T05:09:05Z},
  url = {https://en.wikipedia.org/w/index.php?title=Simulated_annealing&oldid=1017509035},
  urldate = {2021-06-07},
  abstract = {Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. It is often used when the search space is discrete (e.g., the traveling salesman problem). For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to exact algorithms such as gradient descent or branch and bound. The name of the algorithm comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material to increase the size of its crystals and reduce their defects. Both are attributes of the material that depend on their thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy or Gibbs energy. Simulated annealing can be used for very hard computational optimization problems where exact algorithms fail; even though it usually achieves an approximate solution to the global minimum, it could be enough for many practical problems. The problems solved by SA are currently formulated by an objective function of many variables, subject to several constraints. In practice, the constraint can be penalized as part of the objective function. Similar techniques have been independently introduced on several occasions, including Pincus (1970), Khachaturyan et al (1979, 1981), Kirkpatrick, Gelatt and Vecchi (1983), and Cerny (1985). In 1983, this approach was used by Kirkpatrick, Gelatt Jr., Vecchi, for a solution of the traveling salesman problem. They also proposed its current name, simulated annealing. This notion of slow cooling implemented in the simulated annealing algorithm is interpreted as a slow decrease in the probability of accepting worse solutions as the solution space is explored. Accepting worse solutions allows for a more extensive search for the global optimal solution. In general, simulated annealing algorithms work as follows. The temperature progressively decreases from an initial positive value to zero. At each time step, the algorithm randomly selects a solution close to the current one, measures its quality, and moves to it according to the temperature-dependent probabilities of selecting better or worse solutions, which during the search respectively remain at 1 (or positive) and decrease towards zero. The simulation can be performed either by a solution of kinetic equations for density functions or by using the stochastic sampling method. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, published by N. Metropolis et al. in 1953.},
  annotation = {Page Version ID: 1017509035},
  file = {C\:\\Users\\felix\\Zotero\\storage\\C8QQW6LI\\index.html},
  langid = {english}
}

@online{soudryExponentiallyVanishingSuboptimal2017,
  title = {Exponentially Vanishing Sub-Optimal Local Minima in Multilayer Neural Networks},
  author = {Soudry, Daniel and Hoffer, Elad},
  date = {2017-10-28},
  url = {http://arxiv.org/abs/1702.05777},
  urldate = {2021-06-30},
  abstract = {Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., "near" linear separability), or an unrealistic hidden layer with \$\textbackslash Omega\textbackslash left(N\textbackslash right)\$ units. Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of \$N\textbackslash rightarrow\textbackslash infty\$ datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension \$d\_\{0\}=\textbackslash tilde\{\textbackslash Omega\}\textbackslash left(\textbackslash sqrt\{N\}\textbackslash right)\$, and a more realistic number of \$d\_\{1\}=\textbackslash tilde\{\textbackslash Omega\}\textbackslash left(N/d\_\{0\}\textbackslash right)\$ hidden units. We demonstrate our results numerically: for example, \$0\textbackslash\%\$ binary classification training error on CIFAR with only \$N/d\_\{0\}\textbackslash approx 16\$ hidden neurons.},
  archiveprefix = {arXiv},
  eprint = {1702.05777},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Soudry_Hoffer\\Soudry_Hoffer_2017_Exponentially vanishing sub-optimal local minima in multilayer neural networks.pdf;C\:\\Users\\felix\\Zotero\\storage\\CRTJZ6GA\\1702.html},
  keywords = {Statistics - Machine Learning},
  primaryclass = {stat}
}

@online{suDifferentialEquationModeling2015,
  title = {A {{Differential Equation}} for {{Modeling Nesterov}}'s {{Accelerated Gradient Method}}: {{Theory}} and {{Insights}}},
  shorttitle = {A {{Differential Equation}} for {{Modeling Nesterov}}'s {{Accelerated Gradient Method}}},
  author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
  date = {2015-10-27},
  url = {http://arxiv.org/abs/1503.01243},
  urldate = {2021-07-12},
  abstract = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
  archiveprefix = {arXiv},
  eprint = {1503.01243},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2015_Su et al\\Su et al_2015_A Differential Equation for Modeling Nesterov's Accelerated Gradient Method.pdf;C\:\\Users\\felix\\Zotero\\storage\\Y73GE2KR\\1503.html},
  keywords = {Mathematics - Classical Analysis and ODEs,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {math, stat}
}

@inproceedings{sutskeverImportanceInitializationMomentum2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  date = {2013-05-26},
  pages = {1139--1147},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v28/sutskever13.html},
  urldate = {2021-06-07},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this pa...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  file = {C\:\\gdrive\\ZoteroPaper\\2013_Sutskever et al\\Sutskever et al_2013_On the importance of initialization and momentum in deep learning.pdf;C\:\\Users\\felix\\Zotero\\storage\\ZB8G53F7\\sutskever13.html},
  langid = {english}
}

@book{sutskeverTrainingRecurrentNeural2013,
  title = {Training Recurrent Neural Networks},
  author = {Sutskever, Ilya},
  date = {2013},
  publisher = {{University of Toronto Toronto, Canada}},
  file = {C\:\\gdrive\\ZoteroPaper\\2013_Sutskever\\Sutskever_2013_Training recurrent neural networks.pdf}
}

@online{swirszczLocalMinimaTraining2017,
  title = {Local Minima in Training of Neural Networks},
  author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  date = {2017-02-17},
  url = {http://arxiv.org/abs/1611.06310},
  urldate = {2021-06-16},
  abstract = {There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.},
  archiveprefix = {arXiv},
  eprint = {1611.06310},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Swirszcz et al\\Swirszcz et al_2017_Local minima in training of neural networks.pdf;C\:\\Users\\felix\\Zotero\\storage\\NN572T58\\1611.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{tiaoVisualizingAnimatingOptimization2016,
  title = {Visualizing and {{Animating Optimization Algorithms}} with {{Matplotlib}}},
  author = {Tiao, Louis},
  date = {2016-04-26T22:13:17+10:00},
  url = {http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/},
  urldate = {2021-06-08},
  abstract = {In this series of notebooks, we demonstrate some useful patterns and recipes for visualizing animating optimization algorithms using Matplotlib. In~[1]:      \%matplotlib inline In~[2]:},
  file = {C\:\\Users\\felix\\Zotero\\storage\\NW3E5TH8\\visualizing-and-animating-optimization-algorithms-with-matplotlib.html},
  langid = {english},
  organization = {{Louis Tiao}}
}

@online{truongFastSimpleModification2021,
  title = {A Fast and Simple Modification of {{Newton}}'s Method Helping to Avoid Saddle Points},
  author = {Truong, Tuyen Trung and To, Tat Dat and Nguyen, Tuan Hang and Nguyen, Thu Hang and Nguyen, Hoang Phuong and Helmy, Maged},
  date = {2021-03-15},
  url = {http://arxiv.org/abs/2006.01512},
  urldate = {2021-06-17},
  abstract = {We propose in this paper New Q-Newton's method. The update rule for the simplest version is \$x\_\{n+1\}=x\_n-w\_n\$ where \$w\_n=pr\_\{A\_n,+\}(v\_n)-pr\_\{A\_n,-\}(v\_n)\$, with \$A\_n=\textbackslash nabla \^2f(x\_n)+\textbackslash delta \_n||\textbackslash nabla f(x\_n)||\^2 .Id\$ and \$v\_n=A\_n\^\{-1\}.\textbackslash nabla f(x\_n)\$. Here \$\textbackslash delta \_n\$ is an appropriate real number so that \$A\_n\$ is invertible, and \$pr\_\{A\_n,\textbackslash pm\}\$ are projections to the vector subspaces generated by eigenvectors of positive (correspondingly negative) eigenvalues of \$A\_n\$. The main result of this paper roughly says that if \$f\$ is \$C\^3\$ and a sequence \$\textbackslash\{x\_n\textbackslash\}\$, constructed by the New Q-Newton's method from a random initial point \$x\_0\$, \{\textbackslash bf converges\}, then the limit point is a critical point and is not a saddle point, and the convergence rate is the same as that of Newton's method. At the end of the paper, we present some issues (saddle points and convergence) one faces when implementing Newton's method and modifications into Deep Neural Networks. In the appendix, we test the good performance of New Q-Newton's method on various benchmark test functions such as Rastrigin, Askley, Rosenbroch and many other, against algorithms such as Newton's method, BFGS, Adaptive Cubic Regularization, Random damping Newton's method and Inertial Newton's method, as well as Unbounded Two-way Backtracking Gradient Descent. The experiments demonstrate in particular that the assumption that \$f\$ is \$C\^3\$ is necessary for some conclusions in the main theoretical results.},
  archiveprefix = {arXiv},
  eprint = {2006.01512},
  eprinttype = {arxiv},
  file = {C\:\\Users\\felix\\Zotero\\storage\\8YKQI8GP\\Truong et al. - 2021 - A fast and simple modification of Newton's method .pdf;C\:\\Users\\felix\\Zotero\\storage\\8GQNPF9E\\2006.html},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@inproceedings{tsybakovOptimalRatesAggregation2003,
  title = {Optimal {{Rates}} of {{Aggregation}}},
  booktitle = {Learning {{Theory}} and {{Kernel Machines}}},
  author = {Tsybakov, Alexandre B.},
  editor = {Schölkopf, Bernhard and Warmuth, Manfred K.},
  date = {2003},
  pages = {303--313},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45167-9_23},
  abstract = {We study the problem of aggregation of M arbitrary estimators of a regression function with respect to the mean squared risk. Three main types of aggregation are considered: model selection, convex and linear aggregation. We define the notion of optimal rate of aggregation in an abstract context and prove lower bounds valid for any method of aggregation. We then construct procedures that attain these bounds, thus establishing optimal rates of linear, convex and model selection type aggregation.},
  file = {C\:\\gdrive\\ZoteroPaper\\2003_Tsybakov\\Tsybakov_2003_Optimal Rates of Aggregation.pdf},
  isbn = {978-3-540-45167-9},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{wilsonMarginalValueAdaptive2018,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  date = {2018-05-21},
  url = {http://arxiv.org/abs/1705.08292},
  urldate = {2021-04-19},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
  archiveprefix = {arXiv},
  eprint = {1705.08292},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Wilson et al\\Wilson et al_2018_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf;C\:\\Users\\felix\\Zotero\\storage\\59ZDTSIM\\1705.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{wuHowSGDSelects2018,
  title = {How {{SGD Selects}} the {{Global Minima}} in {{Over}}-Parameterized {{Learning}}: {{A Dynamical Stability Perspective}}},
  author = {Wu, Lei and Ma, Chao},
  date = {2018},
  pages = {10},
  abstract = {The question of which global minima are accessible by a stochastic gradient decent (SGD) algorithm with specific learning rate and batch size is studied from the perspective of dynamical stability. The concept of non-uniformity is introduced, which, together with sharpness, characterizes the stability property of a global minimum and hence the accessibility of a particular SGD algorithm to that global minimum. In particular, this analysis shows that learning rate and batch size play different roles in minima selection. Extensive empirical results seem to correlate well with the theoretical findings and provide further support to these claims.},
  file = {C\:\\gdrive\\ZoteroPaper\\2018_Wu_Ma\\Wu_Ma_2018_How SGD Selects the Global Minima in Over-parameterized Learning.pdf},
  langid = {english}
}

@online{zhangDeepLearningElastic2015,
  title = {Deep Learning with {{Elastic Averaging SGD}}},
  author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
  date = {2015-10-25},
  url = {http://arxiv.org/abs/1412.6651},
  urldate = {2021-06-08},
  abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
  archiveprefix = {arXiv},
  eprint = {1412.6651},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2015_Zhang et al\\Zhang et al_2015_Deep learning with Elastic Averaging SGD.pdf;C\:\\Users\\felix\\Zotero\\storage\\WHHMYVJ4\\1412.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@online{zhangUnderstandingDeepLearning2017,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  date = {2017-02-26},
  url = {http://arxiv.org/abs/1611.03530},
  urldate = {2021-06-16},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  archiveprefix = {arXiv},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  file = {C\:\\gdrive\\ZoteroPaper\\2017_Zhang et al\\Zhang et al_2017_Understanding deep learning requires rethinking generalization.pdf;C\:\\Users\\felix\\Zotero\\storage\\988LPWUC\\1611.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}

@inproceedings{zinkevichOnlineConvexProgramming2003,
  title = {Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
  booktitle = {Proceedings of the 20th International Conference on Machine Learning (Icml-03)},
  author = {Zinkevich, Martin},
  date = {2003},
  pages = {928--936},
  file = {C\:\\gdrive\\ZoteroPaper\\2003_Zinkevich\\Zinkevich_2003_Online convex programming and generalized infinitesimal gradient ascent.pdf}
}


