
@book{adlerRandomFieldsGeometry,
  title = {Random {{Fields}} and {{Geometry}}},
  author = {Adler, Robert J. and Taylor, Jonathan E.},
  series = {Springer {{Monographs}} in {{Mathematics}}},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  issn = {1439-7382},
  doi = {10.1007/978-0-387-48116-6},
  isbn = {978-0-387-48112-8},
  langid = {english},
  keywords = {Geometry,Mathematical Methods in Physics,Mathematics,Mathematics and Statistics,Probability Theory and Stochastic Processes,Statistics; general},
  file = {/Users/felix/gdrive/ZoteroPaper/_Adler_Taylor/Adler_Taylor_Random Fields and Geometry.pdf}
}

@online{alainNegativeEigenvaluesHessian2019,
  title = {Negative Eigenvalues of the {{Hessian}} in Deep Neural Networks},
  author = {Alain, Guillaume and Roux, Nicolas Le and Manzagol, Pierre-Antoine},
  date = {2019-02-06},
  eprint = {1902.02366},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1902.02366},
  urldate = {2021-06-17},
  abstract = {The loss function of deep networks is known to be non-convex but the precise nature of this nonconvexity is still an active area of research. In this work, we study the loss landscape of deep networks through the eigendecompositions of their Hessian matrix. In particular, we examine how important the negative eigenvalues are and the benefits one can observe in handling them appropriately.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Alain et al/Alain et al_2019_Negative eigenvalues of the Hessian in deep neural networks.pdf;/Users/felix/gdrive/ZoteroPaper/2019_Alain et al/Alain et al_2019_Negative eigenvalues of the Hessian in deep neural networks2.pdf;/Users/felix/Zotero/storage/PSVMGMJV/1902.html}
}

@book{amariDifferentialGeometricalMethodsStatistics1985,
  title = {Differential-{{Geometrical Methods}} in {{Statistics}}},
  author = {Amari, Shun-ichi},
  date = {1985},
  series = {Lecture {{Notes}} in {{Statistics}}},
  volume = {28},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4612-5056-2},
  abstract = {From the reviews: "In this Lecture Note volume the author describes his differential-geometric approach to parametrical statistical problems summarizing the results he had published in a series of papers in the last five years. The author provides a geometric framework for a special class of test and estimation procedures for curved exponential families. ... ... The material and ideas presented in this volume are important and it is recommended to everybody interested in the connection between statistics and geometry ..." \#Metrika\#1 "More than hundred references are given showing the growing interest in differential geometry with respect to statistics. The book can only strongly be recommended to a geodesist since it offers many new insights into statistics on a familiar ground." \#Manuscripta Geodaetica\#2},
  editorb = {Berger, J. and Fienberg, S. and Gani, J. and Krickeberg, K. and Olkin, I. and Singer, B.},
  editorbtype = {redactor},
  isbn = {978-0-387-96056-2 978-1-4612-5056-2},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/1985_Amari/Amari_1985_Differential-Geometrical Methods in Statistics.pdf}
}

@article{amariNaturalGradientWorks1998,
  title = {Natural {{Gradient Works Efficiently}} in {{Learning}}},
  author = {Amari, Shun-ichi},
  date = {1998-02-15},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {10},
  number = {2},
  pages = {251--276},
  issn = {0899-7667},
  doi = {10.1162/089976698300017746},
  abstract = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.},
  file = {/Users/felix/gdrive/ZoteroPaper/1998_Amari/Amari_1998_Natural Gradient Works Efficiently in Learning.pdf;/Users/felix/Zotero/storage/JC5KCTNQ/Natural-Gradient-Works-Efficiently-in-Learning.html}
}

@online{andrychowiczLearningLearnGradient2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
  options = {useprefix=true},
  date = {2016-11-30},
  eprint = {1606.04474},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.04474},
  urldate = {2021-06-17},
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/felix/gdrive/ZoteroPaper/2016_Andrychowicz et al/Andrychowicz et al_2016_Learning to learn by gradient descent by gradient descent.pdf;/Users/felix/Zotero/storage/M4UMDP6S/1606.html}
}

@unpublished{ankirchnerApproximatingStochasticGradient2021,
  title = {Approximating Stochastic Gradient Descent with Diffusions: Error Expansions and Impact of Learning Rate Schedules},
  shorttitle = {Approximating Stochastic Gradient Descent with Diffusions},
  author = {Ankirchner, Stefan and Perko, Stefan},
  date = {2021-06},
  url = {https://hal.archives-ouvertes.fr/hal-03262396},
  urldate = {2021-09-22},
  abstract = {Applying a stochastic gradient descent method for minimizing an objective gives rise to a discrete-time process of estimated parameter values. In order to better understand the dynamics of the estimated values it can make sense to approximate the discrete-time process with a continuous-time diffusion. We refine some results on the weak error of diffusion approximations. In particular, we explicitly compute the leading term in the error expansion of an ODE approximation with respect to a parameter h discretizing the learning rate schedule. The leading term changes if one extends the ODE with a Brownian diffusion component. Finally, we show that if the learning rate is time varying, then its rate of change needs to enter the drift coefficient in order to obtain an approximation of order 2.},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Ankirchner_Perko/Ankirchner_Perko_2021_Approximating stochastic gradient descent with diffusions.pdf}
}

@article{armijoMinimizationFunctionsHaving1966,
  title = {Minimization of Functions Having {{Lipschitz}} Continuous First Partial Derivatives.},
  author = {Armijo, Larry},
  date = {1966-01},
  journaltitle = {Pacific Journal of Mathematics},
  volume = {16},
  number = {1},
  pages = {1--3},
  publisher = {{Pacific Journal of Mathematics, A Non-profit Corporation}},
  issn = {0030-8730},
  url = {https://projecteuclid.org/journals/pacific-journal-of-mathematics/volume-16/issue-1/Minimization-of-functions-having-Lipschitz-continuous-first-partial-derivatives/pjm/1102995080.full},
  urldate = {2021-09-27},
  abstract = {Pacific Journal of Mathematics},
  keywords = {65.10,65.30},
  file = {/Users/felix/gdrive/ZoteroPaper/1966_Armijo/Armijo_1966_Minimization of functions having Lipschitz continuous first partial derivatives.pdf;/Users/felix/Zotero/storage/MU2L36LP/1102995080.html}
}

@online{assranConvergenceNesterovAccelerated2020,
  title = {On the {{Convergence}} of {{Nesterov}}'s {{Accelerated Gradient Method}} in {{Stochastic Settings}}},
  author = {Assran, Mahmoud and Rabbat, Michael},
  date = {2020-06-27},
  eprint = {2002.12414},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2002.12414},
  urldate = {2021-09-06},
  abstract = {We study Nesterov's accelerated gradient method with constant step-size and momentum parameters in the stochastic approximation setting (unbiased gradients with bounded variance) and the finite-sum setting (where randomness is due to sampling mini-batches). To build better insight into the behavior of Nesterov's method in stochastic settings, we focus throughout on objectives that are smooth, strongly-convex, and twice continuously differentiable. In the stochastic approximation setting, Nesterov's method converges to a neighborhood of the optimal point at the same accelerated rate as in the deterministic setting. Perhaps surprisingly, in the finite-sum setting, we prove that Nesterov's method may diverge with the usual choice of step-size and momentum, unless additional conditions on the problem related to conditioning and data coherence are satisfied. Our results shed light as to why Nesterov's method may fail to converge or achieve acceleration in the finite-sum setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Assran_Rabbat/Assran_Rabbat_2020_On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic.pdf;/Users/felix/Zotero/storage/IGLLTFCA/2002.html}
}

@online{ayadiStochasticRungeKuttaMethods2020,
  title = {Stochastic {{Runge}}-{{Kutta}} Methods and Adaptive {{SGD}}-{{G2}} Stochastic Gradient Descent},
  author = {Ayadi, Imen and Turinici, Gabriel},
  date = {2020-02-20},
  eprint = {2002.09304},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2002.09304},
  urldate = {2021-04-17},
  abstract = {The minimization of the loss function is of paramount importance in deep neural networks. On the other hand, many popular optimization algorithms have been shown to correspond to some evolution equation of gradient flow type. Inspired by the numerical schemes used for general evolution equations we introduce a second order stochastic Runge Kutta method and show that it yields a consistent procedure for the minimization of the loss function. In addition it can be coupled, in an adaptive framework, with a Stochastic Gradient Descent (SGD) to adjust automatically the learning rate of the SGD, without the need of any additional information on the Hessian of the loss functional. The adaptive SGD, called SGD-G2, is successfully tested on standard datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Ayadi_Turinici/Ayadi_Turinici_2020_Stochastic Runge-Kutta methods and adaptive SGD-G2 stochastic gradient descent.pdf;/Users/felix/Zotero/storage/6FAJEVZG/2002.html}
}

@online{bachNonstronglyconvexSmoothStochastic2013,
  title = {Non-Strongly-Convex Smooth Stochastic Approximation with Convergence Rate {{O}}(1/n)},
  author = {Bach, Francis and Moulines, Eric},
  date = {2013-06-10},
  eprint = {1306.2119},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1306.2119},
  urldate = {2021-07-15},
  abstract = {We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/n\^\{1/2\}). We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. For least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments on standard machine learning benchmarks showing that they often outperform existing approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2013_Bach_Moulines/Bach_Moulines_2013_Non-strongly-convex smooth stochastic approximation with convergence rate O(1-n).pdf;/Users/felix/Zotero/storage/CFCQP8CT/1306.html}
}

@online{bassilyExponentialConvergenceSGD2018,
  title = {On Exponential Convergence of {{SGD}} in Non-Convex over-Parametrized Learning},
  author = {Bassily, Raef and Belkin, Mikhail and Ma, Siyuan},
  date = {2018-11-05},
  eprint = {1811.02564},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1811.02564},
  urldate = {2021-09-21},
  abstract = {Large over-parametrized models learned via stochastic gradient descent (SGD) methods have become a key element in modern machine learning. Although SGD methods are very effective in practice, most theoretical analyses of SGD suggest slower convergence than what is empirically observed. In our recent work [8] we analyzed how interpolation, common in modern over-parametrized learning, results in exponential convergence of SGD with constant step size for convex loss functions. In this note, we extend those results to a much broader non-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A number of important non-convex problems in machine learning, including some classes of neural networks, have been recently shown to satisfy the PL condition. We argue that the PL condition provides a relevant and attractive setting for many machine learning problems, particularly in the over-parametrized regime.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Bassily et al/Bassily et al_2018_On exponential convergence of SGD in non-convex over-parametrized learning.pdf;/Users/felix/Zotero/storage/EL9D8S6I/1811.html}
}

@article{beckFastIterativeShrinkageThresholding2009,
  title = {A {{Fast Iterative Shrinkage}}-{{Thresholding Algorithm}} for {{Linear Inverse Problems}}},
  author = {Beck, A. and Teboulle, M.},
  date = {2009},
  journaltitle = {SIAM J. Imaging Sci.},
  doi = {10.1137/080716542},
  abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.}
}

@article{benkoNystromMethodsSingular2008,
  title = {Nyström Methods and Singular Second-Order Differential Equations},
  author = {Benko, David and Biles, Daniel C. and Robinson, Mark P. and Spraker, John S.},
  date = {2008-10-01},
  journaltitle = {Computers \& Mathematics with Applications},
  shortjournal = {Computers \& Mathematics with Applications},
  volume = {56},
  number = {8},
  pages = {1975--1980},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2008.04.023},
  abstract = {Chawla, Jain and Subramanian studied the application of Nyström methods to a class of singular initial value problems. Following their approach, we generalize this class by applying the Nyström method to the initial value problem for an equation of the form y″+p(t)y′+q(t,y(t))=0,t∈(0,1] where p has a certain specified type of singularity and q is sufficiently differentiable, and then we determine the order of convergence.~This is followed by computational evidence.},
  langid = {english},
  keywords = {Lane–Emden equation,Numerical approximation,Nyström method,Second-order differential equation,Singular differential equations},
  file = {/Users/felix/gdrive/ZoteroPaper/2008_Benko et al/Benko et al_2008_Nyström methods and singular second-order differential equations.pdf;/Users/felix/Zotero/storage/BRE8SCJA/S0898122108002757.html}
}

@article{bochiInequalitiesNumericalInvariants2003,
  title = {Inequalities for Numerical Invariants of Sets of Matrices},
  author = {Bochi, Jairo},
  date = {2003-07-15},
  journaltitle = {Linear Algebra and its Applications},
  shortjournal = {Linear Algebra and its Applications},
  volume = {368},
  pages = {71--81},
  issn = {0024-3795},
  doi = {10.1016/S0024-3795(02)00658-4},
  abstract = {We prove three inequalities relating some invariants of sets of matrices, such as the joint spectral radius. One of the inequalities, in which proof we use geometric invariant theory, has the generalized spectral radius theorem of Berger and Wang as an immediate corollary.},
  langid = {english},
  keywords = {Geometric invariant theory,Joint spectral radius},
  file = {/Users/felix/gdrive/ZoteroPaper/2003_Bochi/Bochi_2003_Inequalities for numerical invariants of sets of matrices.pdf}
}

@online{bottouOptimizationMethodsLargeScale2018,
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  date = {2018-02-08},
  eprint = {1606.04838},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1606.04838},
  urldate = {2021-06-08},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Bottou et al/Bottou et al_2018_Optimization Methods for Large-Scale Machine Learning.pdf;/Users/felix/Zotero/storage/5PK2UUXE/1606.html}
}

@article{bouttierConvergenceRateSimulated2019,
  title = {Convergence {{Rate}} of a {{Simulated Annealing Algorithm}} with {{Noisy Observations}}},
  author = {Bouttier, Clément and Gavra, Ioana},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  number = {4},
  pages = {1--45},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/16-588.html},
  urldate = {2021-07-05},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Bouttier_Gavra/Bouttier_Gavra_2019_Convergence Rate of a Simulated Annealing Algorithm with Noisy Observations.pdf;/Users/felix/Zotero/storage/3EVBRBLD/16-588.html}
}

@book{bovierMetastabilityPotentialTheoreticApproach2015,
  title = {Metastability: A {{Potential}}-{{Theoretic Approach}}},
  shorttitle = {Metastability},
  author = {Bovier, Anton and den Hollander, Frank},
  date = {2015},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-24777-9},
  abstract = {This monograph provides a concise presentation of a mathematical approach to metastability, a wide-spread phenomenon in the dynamics of non-linear systems - physical, chemical, biological or economic - subject to the action of temporal random forces typically referred to as noise, based on potential theory of reversible Markov processes. The authors shed new light on the metastability phenomenon as a sequence of visits of the path of the process to different metastable sets, and focuses on the precise analysis of the respective hitting probabilities and hitting times of these sets.The theory is illustrated with many examples, ranging from finite-state Markov chains, finite-dimensional diffusions and stochastic partial differential equations, via mean-field dynamics with and without disorder, to stochastic spin-flip and particle-hop dynamics and probabilistic cellular automata, unveiling the common universal features of these systems with respect to their metastable behaviour. The monograph will serve both as comprehensive introduction and as reference for graduate students and researchers interested in metastability.},
  isbn = {978-3-319-24775-5},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Bovier_Hollander/Bovier_Hollander_2015_Metastability.pdf;/Users/felix/Zotero/storage/2D2GI7LF/9783319247755.html}
}

@article{brayStatisticsCriticalPoints2007,
  title = {The Statistics of Critical Points of {{Gaussian}} Fields on Large-Dimensional Spaces},
  author = {Bray, Alan J. and Dean, David S.},
  date = {2007-04-10},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {98},
  number = {15},
  eprint = {cond-mat/0611023},
  eprinttype = {arxiv},
  pages = {150201},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.98.150201},
  abstract = {We calculate the average number of critical points of a Gaussian field on a high-dimensional space as a function of their energy and their index. Our results give a complete picture of the organization of critical points and are of relevance to glassy and disordered systems, and to landscape scenarios coming from the anthropic approach to string theory.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/Users/felix/gdrive/ZoteroPaper/2007_Bray_Dean/Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces.pdf;/Users/felix/Zotero/storage/RV2A6TST/0611023.html}
}

@online{bubeckConvexOptimizationAlgorithms2015,
  title = {Convex {{Optimization}}: Algorithms and {{Complexity}}},
  shorttitle = {Convex {{Optimization}}},
  author = {Bubeck, Sébastien},
  date = {2015-11-16},
  eprint = {1405.4980},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1405.4980},
  urldate = {2021-07-12},
  abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Bubeck/Bubeck_2015_Convex Optimization.pdf;/Users/felix/Zotero/storage/CLBZ7TQV/1405.html}
}

@online{bubeckGeometricAlternativeNesterov2015,
  title = {A Geometric Alternative to {{Nesterov}}'s Accelerated Gradient Descent},
  author = {Bubeck, Sébastien and Lee, Yin Tat and Singh, Mohit},
  date = {2015-06-26},
  eprint = {1506.08187},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/1506.08187},
  urldate = {2021-07-07},
  abstract = {We propose a new method for unconstrained optimization of a smooth and strongly convex function, which attains the optimal rate of convergence of Nesterov’s accelerated gradient descent. The new algorithm has a simple geometric interpretation, loosely inspired by the ellipsoid method. We provide some numerical evidence that the new method can be superior to Nesterov’s accelerated gradient descent.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Bubeck et al/Bubeck et al_2015_A geometric alternative to Nesterov's accelerated gradient descent.pdf}
}

@inproceedings{canAcceleratedLinearConvergence2019,
  title = {Accelerated {{Linear Convergence}} of {{Stochastic Momentum Methods}} in {{Wasserstein Distances}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Can, Bugra and Gurbuzbalaban, Mert and Zhu, Lingjiong},
  date = {2019-05-24},
  pages = {891--901},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/can19a.html},
  urldate = {2021-09-20},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Can et al/Can et al_2019_Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein.pdf;/Users/felix/Zotero/storage/BASWPP5A/Can et al. - 2019 - Accelerated Linear Convergence of Stochastic Momen.pdf}
}

@article{cauchyMethodeGeneralePour1847,
  title = {Méthode Générale Pour La Résolution Des Systemes d’équations Simultanées},
  author = {Cauchy, Augustin},
  date = {1847},
  journaltitle = {Comp. Rend. Sci. Paris},
  volume = {25},
  number = {1847},
  pages = {536--538},
  file = {/Users/felix/gdrive/ZoteroPaper/1847_Cauchy/Cauchy_1847_Méthode générale pour la résolution des systemes d’équations simultanées.pdf}
}

@article{chaudhariEntropysgdBiasingGradient2019,
  title = {Entropy-Sgd: Biasing Gradient Descent into Wide Valleys},
  shorttitle = {Entropy-Sgd},
  author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  date = {2019},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2019},
  number = {12},
  pages = {124018},
  publisher = {{IOP Publishing}},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Chaudhari et al/Chaudhari et al_2019_Entropy-sgd.pdf}
}

@online{cheeConvergenceDiagnosticsStochastic2018,
  title = {Convergence Diagnostics for Stochastic Gradient Descent with Constant Step Size},
  author = {Chee, Jerry and Toulis, Panos},
  date = {2018-02-22},
  eprint = {1710.06382},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1710.06382},
  urldate = {2021-09-20},
  abstract = {Many iterative procedures in stochastic optimization exhibit a transient phase followed by a stationary phase. During the transient phase the procedure converges towards a region of interest, and during the stationary phase the procedure oscillates in that region, commonly around a single point. In this paper, we develop a statistical diagnostic test to detect such phase transition in the context of stochastic gradient descent with constant learning rate. We present theory and experiments suggesting that the region where the proposed diagnostic is activated coincides with the convergence region. For a class of loss functions, we derive a closed-form solution describing such region. Finally, we suggest an application to speed up convergence of stochastic gradient descent by halving the learning rate each time stationarity is detected. This leads to a new variant of stochastic gradient descent, which in many settings is comparable to state-of-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Chee_Toulis/Chee_Toulis_2018_Convergence diagnostics for stochastic gradient descent with constant step size.pdf;/Users/felix/Zotero/storage/BB787665/1710.html}
}

@inproceedings{cheeConvergenceDiagnosticsStochastic2018a,
  title = {Convergence Diagnostics for Stochastic Gradient Descent with Constant Learning Rate},
  booktitle = {Proceedings of the {{Twenty}}-{{First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Chee, Jerry and Toulis, Panos},
  date = {2018-03-31},
  pages = {1476--1485},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v84/chee18a.html},
  urldate = {2021-09-20},
  abstract = {Many iterative procedures in stochastic optimization exhibit a transient phase followed by a stationary phase. During the transient phase the procedure converges towards a region of interest, and during the stationary phase the procedure oscillates in that region, commonly around a single point. In this paper, we develop a statistical diagnostic test to detect such phase transition in the context of stochastic gradient descent with constant learning rate. We present theory and experiments suggesting that the region where the proposed diagnostic is activated coincides with the convergence region. For a class of loss functions, we derive a closed-form solution describing such region. Finally, we suggest an application to speed up convergence of stochastic gradient descent by halving the learning rate each time stationarity is detected. This leads to a new variant of stochastic gradient descent, which in many settings is comparable to state-of-art.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Chee_Toulis/Chee_Toulis_2018_Convergence diagnostics for stochastic gradient descent with constant learning.pdf;/Users/felix/Zotero/storage/BRBEW465/Chee and Toulis - 2018 - Convergence diagnostics for stochastic gradient de.pdf}
}

@inproceedings{cheeUnderstandingDetectingConvergence2020,
  title = {Understanding and {{Detecting Convergence}} for {{Stochastic Gradient Descent}} with {{Momentum}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Chee, Jerry and Li, Ping},
  date = {2020-12},
  pages = {133--140},
  doi = {10.1109/BigData50022.2020.9378129},
  abstract = {Convergence detection of iterative stochastic optimization methods is of great practical interest. This paper considers stochastic gradient descent (SGD) with a constant learning rate and momentum. We show that there exists a transient phase in which iterates move towards a region of interest, and a stationary phase in which iterates remain bounded in that region around a minimum point. We construct a statistical diagnostic test for convergence to the stationary phase using the inner product between successive gradients and demonstrate that the proposed diagnostic works well. We theoretically and empirically characterize how momentum can affect the test statistic of the diagnostic, and how the test statistic captures a relatively sparse signal within the gradients in convergence. Finally, we demonstrate an application to automatically tune the learning rate by reducing it each time stationarity is detected, and show the procedure is robust to mis-specified initial rates.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Big Data,Convergence,Iterative methods,Monitoring,Reliability,Trajectory,Transient analysis},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Chee_Li/Chee_Li_2020_Understanding and Detecting Convergence for Stochastic Gradient Descent with.pdf}
}

@online{chenFiniteSampleAnalysisNonlinear2021,
  title = {Finite-{{Sample Analysis}} of {{Nonlinear Stochastic Approximation}} with {{Applications}} in {{Reinforcement Learning}}},
  author = {Chen, Zaiwei and Zhang, Sheng and Doan, Thinh T. and Clarke, John-Paul and Maguluri, Siva Theja},
  date = {2021-07-08},
  eprint = {1905.11425},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/1905.11425},
  urldate = {2021-09-28},
  abstract = {Motivated by applications in reinforcement learning (RL), we study a nonlinear stochastic approximation (SA) algorithm under Markovian noise, and establish its finite-sample convergence bounds under various stepsizes. Specifically, we show that when using constant stepsize (i.e., \$\textbackslash epsilon\_k\textbackslash equiv \textbackslash epsilon\$), the algorithm achieves exponential fast convergence with asymptotic accuracy \$\textbackslash mathcal\{O\}(\textbackslash epsilon\textbackslash log(1/\textbackslash epsilon))\$. When using diminishing stepsizes with appropriate decay rate, the algorithm converges with rate \$\textbackslash mathcal\{O\}(\textbackslash log(k)/k)\$. Our proof is based on the Lyapunov drift arguments, and to handle the Markovian noise, we exploit the fast mixing of the underlying Markov chain. To demonstrate the generality of our theoretical results on Markovian SA, we use it to derive the finite-sample bounds of the popular \$Q\$-learning with linear function approximation algorithm, under a condition on the behavior policy. Importantly, we do not need to make the unrealistic assumption that the samples are i.i.d., and do not require an additional projection step in the algorithm to maintain the boundedness of the iterates. Numerical simulations corroborate our theoretical findings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Chen et al/Chen et al_2021_Finite-Sample Analysis of Nonlinear Stochastic Approximation with Applications.pdf;/Users/felix/Zotero/storage/T56FWGND/1905.html}
}

@unpublished{chenLargeScaleOptimizationData2019,
  title = {Large-{{Scale Optimization}} for {{Data Science}}: Mirror {{Descent}}},
  author = {Chen, Yuxin},
  date = {2019},
  url = {http://www.princeton.edu/~yc5/ele522_optimization/lectures/mirror_descent.pdf},
  urldate = {2021-07-07},
  file = {/Users/felix/Zotero/storage/LN4AWSSJ/mirror_descent.pdf}
}

@online{choromanskaLossSurfacesMultilayer2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
  date = {2015-01-21},
  eprint = {1412.0233},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.0233},
  urldate = {2021-06-16},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Choromanska et al/Choromanska et al_2015_The Loss Surfaces of Multilayer Networks.pdf;/Users/felix/Zotero/storage/2AF42JU2/1412.html}
}

@inproceedings{darkenFasterStochasticGradient1991,
  title = {Towards Faster Stochastic Gradient Search},
  booktitle = {{{NIPs}}},
  author = {Darken, Christian and Moody, John},
  date = {1991},
  volume = {91},
  pages = {1009--1016},
  file = {/Users/felix/gdrive/ZoteroPaper/1991_Darken_Moody/Darken_Moody_1991_Towards faster stochastic gradient search.pdf}
}

@online{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  date = {2014},
  eprint = {1406.2572},
  eprinttype = {arxiv},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. This work extends the results of Pascanu et al. (2014).},
  archiveprefix = {arXiv},
  file = {/Users/felix/gdrive/ZoteroPaper/2014_Dauphin et al/Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional.pdf;/Users/felix/Zotero/storage/ZICT2QCI/1406.html}
}

@article{devolderFirstorderMethodsSmooth2014,
  title = {First-Order Methods of Smooth Convex Optimization with Inexact Oracle},
  author = {Devolder, Olivier and Glineur, François and Nesterov, Yurii},
  date = {2014-08-01},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  volume = {146},
  number = {1},
  pages = {37--75},
  issn = {1436-4646},
  doi = {10.1007/s10107-013-0677-5},
  abstract = {We introduce the notion of inexact first-order oracle and analyze the behavior of several first-order methods of smooth convex optimization used with such an oracle. This notion of inexact oracle naturally appears in the context of smoothing techniques, Moreau–Yosida regularization, Augmented Lagrangians and many other situations. We derive complexity estimates for primal, dual and fast gradient methods, and study in particular their dependence on the accuracy of the oracle and the desired accuracy of the objective function. We observe that the superiority of fast gradient methods over the classical ones is no longer absolute when an inexact oracle is used. We prove that, contrary to simple gradient schemes, fast gradient methods must necessarily suffer from error accumulation. Finally, we show that the notion of inexact oracle allows the application of first-order methods of smooth convex optimization to solve non-smooth or weakly smooth convex problems.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2014_Devolder et al/Devolder et al_2014_First-order methods of smooth convex optimization with inexact oracle.pdf}
}

@online{dieuleveutBridgingGapConstant2018,
  title = {Bridging the {{Gap}} between {{Constant Step Size Stochastic Gradient Descent}} and {{Markov Chains}}},
  author = {Dieuleveut, Aymeric and Durmus, Alain and Bach, Francis},
  date = {2018-04-11},
  eprint = {1707.06386},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1707.06386},
  urldate = {2021-10-22},
  abstract = {We consider the minimization of an objective function given access to unbiased estimates of its gradient through stochastic gradient descent (SGD) with constant step-size. While the detailed analysis was only performed for quadratic functions, we provide an explicit asymptotic expansion of the moments of the averaged SGD iterates that outlines the dependence on initial conditions, the effect of noise and the step-size, as well as the lack of convergence in the general (non-quadratic) case. For this analysis, we bring tools from Markov chain theory into the analysis of stochastic gradient. We then show that Richardson-Romberg extrapolation may be used to get closer to the global optimum and we show empirical improvements of the new extrapolation scheme.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Dieuleveut et al/Dieuleveut et al_2018_Bridging the Gap between Constant Step Size Stochastic Gradient Descent and.pdf;/Users/felix/Zotero/storage/7G7F988A/1707.html}
}

@online{dinhSharpMinimaCan2017,
  title = {Sharp {{Minima Can Generalize For Deep Nets}}},
  author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  date = {2017-05-15},
  eprint = {1703.04933},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.04933},
  urldate = {2021-06-16},
  abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Dinh et al/Dinh et al_2017_Sharp Minima Can Generalize For Deep Nets.pdf;/Users/felix/Zotero/storage/JT5F9JLJ/1703.html}
}

@online{doanConvergenceRatesAccelerated2020,
  title = {Convergence {{Rates}} of {{Accelerated Markov Gradient Descent}} with {{Applications}} in {{Reinforcement Learning}}},
  author = {Doan, Thinh T. and Nguyen, Lam M. and Pham, Nhan H. and Romberg, Justin},
  date = {2020-10-19},
  eprint = {2002.02873},
  eprinttype = {arxiv},
  primaryclass = {math},
  url = {http://arxiv.org/abs/2002.02873},
  urldate = {2021-09-28},
  abstract = {Motivated by broad applications in machine learning, we study the popular accelerated stochastic gradient descent (ASGD) algorithm for solving (possibly nonconvex) optimization problems. We characterize the finite-time performance of this method when the gradients are sampled from Markov processes, and hence biased and dependent from time step to time step; in contrast, the analysis in existing work relies heavily on the stochastic gradients being independent and sometimes unbiased. Our main contributions show that under certain (standard) assumptions on the underlying Markov chain generating the gradients, ASGD converges at the nearly the same rate with Markovian gradient samples as with independent gradient samples. The only difference is a logarithmic factor that accounts for the mixing time of the Markov chain. One of the key motivations for this study are complicated control problems that can be modeled by a Markov decision process and solved using reinforcement learning. We apply the accelerated method to several challenging problems in the OpenAI Gym and Mujoco, and show that acceleration can significantly improve the performance of the classic temporal difference learning and REINFORCE algorithms.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Doan et al/Doan et al_2020_Convergence Rates of Accelerated Markov Gradient Descent with Applications in.pdf;/Users/felix/Zotero/storage/QGKZNBDB/2002.html}
}

@online{doanFiniteTimeAnalysisStochastic2020,
  title = {Finite-{{Time Analysis}} of {{Stochastic Gradient Descent}} under {{Markov Randomness}}},
  author = {Doan, Thinh T. and Nguyen, Lam M. and Pham, Nhan H. and Romberg, Justin},
  date = {2020-04-01},
  eprint = {2003.10973},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/2003.10973},
  urldate = {2021-09-28},
  abstract = {Motivated by broad applications in reinforcement learning and machine learning, this paper considers the popular stochastic gradient descent (SGD) when the gradients of the underlying objective function are sampled from Markov processes. This Markov sampling leads to the gradient samples being biased and not independent. The existing results for the convergence of SGD under Markov randomness are often established under the assumptions on the boundedness of either the iterates or the gradient samples. Our main focus is to study the finite-time convergence of SGD for different types of objective functions, without requiring these assumptions. We show that SGD converges nearly at the same rate with Markovian gradient samples as with independent gradient samples. The only difference is a logarithmic factor that accounts for the mixing time of the Markov chain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Doan et al/Doan et al_2020_Finite-Time Analysis of Stochastic Gradient Descent under Markov Randomness.pdf;/Users/felix/Zotero/storage/F9BX6EKJ/2003.html}
}

@online{dontlooWhatDifferenceMomentum2016,
  title = {What's the Difference between Momentum Based Gradient Descent and {{Nesterov}}'s Accelerated Gradient Descent?},
  author = {{dontloo}, (https://stats.stackexchange.com/users/95569/dontloo)},
  date = {2016-01-21},
  url = {https://stats.stackexchange.com/q/191727},
  howpublished = {Cross Validated},
  organization = {{Cross Validated Stack Exchange}}
}

@article{dozatIncorporatingNesterovMomentum2016,
  title = {Incorporating {{Nesterov Momentum}} into {{Adam}}},
  author = {Dozat, Timothy},
  date = {2016-02-18},
  url = {https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ},
  urldate = {2021-11-16},
  abstract = {This work aims to improve upon the recently proposed and rapidly popular- ized optimization algorithm Adam (Kingma \& Ba, 2014). Adam has two main components—a momentum component and an adaptive...},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2016_Dozat/Dozat_2016_Incorporating Nesterov Momentum into Adam.pdf;/Users/felix/Zotero/storage/L89XBWYX/forum.html}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  date = {2011-07-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {12},
  pages = {2121--2159},
  issn = {1532-4435},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  issue = {null},
  file = {/Users/felix/gdrive/ZoteroPaper/2011_Duchi et al/Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf}
}

@online{dudaImprovingSGDConvergence2019,
  title = {Improving {{SGD}} Convergence by Online Linear Regression of Gradients in Multiple Statistically Relevant Directions},
  author = {Duda, Jarek},
  date = {2019-04-14},
  eprint = {1901.11457},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.11457},
  urldate = {2021-06-17},
  abstract = {Deep neural networks are usually trained with stochastic gradient descent (SGD), which minimizes objective function using very rough approximations of gradient, only averaging to the real gradient. Standard approaches like momentum or ADAM only consider a single direction, and do not try to model distance from extremum - neglecting valuable information from calculated sequence of gradients, often stagnating in some suboptimal plateau. Second order methods could exploit these missed opportunities, however, beside suffering from very large cost and numerical instabilities, many of them attract to suboptimal points like saddles due to negligence of signs of curvatures (as eigenvalues of Hessian).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Duda/Duda_2019_Improving SGD convergence by online linear regression of gradients in multiple.pdf}
}

@online{dziugaiteComputingNonvacuousGeneralization2017,
  title = {Computing {{Nonvacuous Generalization Bounds}} for {{Deep}} ({{Stochastic}}) {{Neural Networks}} with {{Many More Parameters}} than {{Training Data}}},
  author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
  date = {2017-10-18},
  eprint = {1703.11008},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.11008},
  urldate = {2021-06-16},
  abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this "deep learning" regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Dziugaite_Roy/Dziugaite_Roy_2017_Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural.pdf;/Users/felix/Zotero/storage/LLHIHT9F/1703.html}
}

@online{flammarionAveragingAccelerationThere2015,
  title = {From {{Averaging}} to {{Acceleration}}, {{There}} Is {{Only}} a {{Step}}-Size},
  author = {Flammarion, Nicolas and Bach, Francis},
  date = {2015-04-07},
  eprint = {1504.01577},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1504.01577},
  urldate = {2021-06-21},
  abstract = {We show that accelerated gradient descent, averaged gradient descent and the heavyball method for non-strongly-convex problems may be reformulated as constant parameter second-order difference equation algorithms, where stability of the system is equivalent to convergence at rate O(1/n2), where n is the number of iterations. We provide a detailed analysis of the eigenvalues of the corresponding linear dynamical system, showing various oscillatory and non-oscillatory behaviors, together with a sharp stability result with explicit constants. We also consider the situation where noisy gradients are available, where we extend our general convergence result, which suggests an alternative algorithm (i.e., with different step sizes) that exhibits the good aspects of both averaging and acceleration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Flammarion_Bach/Flammarion_Bach_2015_From Averaging to Acceleration, There is Only a Step-size.pdf}
}

@online{freemanTopologyGeometryHalfRectified2017,
  title = {Topology and {{Geometry}} of {{Half}}-{{Rectified Network Optimization}}},
  author = {Freeman, C. Daniel and Bruna, Joan},
  date = {2017-06-01},
  eprint = {1611.01540},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.01540},
  urldate = {2021-06-16},
  abstract = {The loss surface of deep neural networks has recently attracted interest in the optimization and machine learning communities as a prime example of high-dimensional non-convex problem. Some insights were recently gained using spin glass models and mean-field approximations, but at the expense of strongly simplifying the nonlinear nature of the model. In this work, we do not make any such assumption and study conditions on the data distribution and model architecture that prevent the existence of bad local minima. Our theoretical work quantifies and formalizes two important \textbackslash emph\{folklore\} facts: (i) the landscape of deep linear networks has a radically different topology from that of deep half-rectified ones, and (ii) that the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. Our main theoretical contribution is to prove that half-rectified single layer networks are asymptotically connected, and we provide explicit bounds that reveal the aforementioned interplay. The conditioning of gradient descent is the next challenge we address. We study this question through the geometry of the level sets, and we introduce an algorithm to efficiently estimate the regularity of such sets on large-scale networks. Our empirical results show that these level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays, in accordance to what is observed in practice with very low curvature attractors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Freeman_Bruna/Freeman_Bruna_2017_Topology and Geometry of Half-Rectified Network Optimization.pdf;/Users/felix/Zotero/storage/52A8JMKD/1611.html}
}

@online{garipovLossSurfacesMode2018,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2018-10-30},
  eprint = {1802.10026},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.10026},
  urldate = {2021-06-15},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Garipov et al/Garipov et al_2018_Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs.pdf;/Users/felix/Zotero/storage/SETXQE29/1802.html}
}

@article{gdawiecRobustNewtonMethod2021,
  title = {On the Robust {{Newton}}’s Method with the {{Mann}} Iteration and the Artistic Patterns from Its Dynamics},
  author = {Gdawiec, Krzysztof and Kotarski, Wiesław and Lisowska, Agnieszka},
  date = {2021-03-01},
  journaltitle = {Nonlinear Dynamics},
  shortjournal = {Nonlinear Dyn},
  volume = {104},
  number = {1},
  pages = {297--331},
  issn = {1573-269X},
  doi = {10.1007/s11071-021-06306-5},
  abstract = {There are two main aims of this paper. The first one is to show some improvement of the robust Newton’s method (RNM) introduced recently by Kalantari. The RNM is a generalisation of the well-known Newton’s root finding method. Since the base method is undefined at critical points, the RNM allows working also at such points. In this paper, we improve the RNM method by applying the Mann iteration instead of the standard Picard iteration. This leads to an essential decrease in the number of root finding steps without visible destroying the sharp boundaries among the basins of attractions presented in polynomiographs. Furthermore, we investigate visually the dynamics of the RNM with the Mann iteration together with the basins of attraction for varying Mann’s iteration parameter with the help of polynomiographs for several polynomials. The second aim of this paper is to present the intriguing polynomiographs obtained from the dynamics of the RNM with the Mann iteration under various sequences used in this iteration. The obtained polynomiographs differ considerably from the ones obtained with the RNM and are interesting from the artistic perspective. Moreover, they can easily find applications in wallpaper or fabric design.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Gdawiec et al/Gdawiec et al_2021_On the robust Newton’s method with the Mann iteration and the artistic patterns.pdf}
}

@inproceedings{geEscapingSaddlePoints2015,
  title = {Escaping {{From Saddle Points}} — {{Online Stochastic Gradient}} for {{Tensor Decomposition}}},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  date = {2015-06-26},
  pages = {797--842},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v40/Ge15.html},
  urldate = {2021-06-17},
  abstract = {We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradi...},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Ge et al/Ge et al_2015_Escaping From Saddle Points — Online Stochastic Gradient for Tensor.pdf;/Users/felix/Zotero/storage/9RW3A2KU/Ge15.html}
}

@article{gelfandNormierteRinge1941,
  title = {Normierte Ringe},
  author = {Gelfand, Israel},
  date = {1941},
  journaltitle = {Matematicheskii Sbornik},
  shortjournal = {Mat. Sb.},
  volume = {9 (51)},
  number = {1},
  pages = {3--24},
  url = {http://www.mathnet.ru/links/78bac43c7b4d5544986886ed8be19701/sm6046.pdf},
  urldate = {2021-11-08},
  langid = {german},
  file = {/Users/felix/gdrive/ZoteroPaper/1941_Gelfand/Gelfand_1941_Normierte Ringe.pdf}
}

@article{georgeAdaptiveStepsizesRecursive2006,
  title = {Adaptive Stepsizes for Recursive Estimation with Applications in Approximate Dynamic Programming},
  author = {George, Abraham P. and Powell, Warren B.},
  date = {2006-10},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {65},
  number = {1},
  pages = {167--198},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-006-8365-9},
  abstract = {We address the problem of determining optimal stepsizes for estimating parameters in the context of approximate dynamic programming. The sufficient conditions for convergence of the stepsize rules have been known for 50 years, but practical computational work tends to use formulas with parameters that have to be tuned for specific applications. The problem is that in most applications in dynamic programming, observations for estimating a value function typically come from a data series that can be initially highly transient. The degree of transience affects the choice of stepsize parameters that produce the fastest convergence. In addition, the degree of initial transience can vary widely among the value function parameters for the same dynamic program. This paper reviews the literature on deterministic and stochastic stepsize rules, and derives formulas for optimal stepsizes for minimizing estimation error. This formula assumes certain parameters are known, and an approximation is proposed for the case where the parameters are unknown. Experimental work shows that the approximation provides faster convergence than other popular formulas.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2006_George_Powell/George_Powell_2006_Adaptive stepsizes for recursive estimation with applications in approximate.pdf}
}

@inproceedings{ghadimiGlobalConvergenceHeavyball2015,
  title = {Global Convergence of the {{Heavy}}-Ball Method for Convex Optimization},
  booktitle = {2015 {{European Control Conference}} ({{ECC}})},
  author = {Ghadimi, Euhanna and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  date = {2015-07},
  pages = {310--315},
  doi = {10.1109/ECC.2015.7330562},
  abstract = {This paper establishes global convergence and provides global bounds of the rate of convergence for the Heavy-ball method for convex optimization. When the objective function has Lipschitz-continuous gradient, we show that the Cesáro average of the iterates converges to the optimum at a rate of O(1/k) where k is the number of iterations. When the objective function is also strongly convex, we prove that the Heavy-ball iterates converge linearly to the unique optimum. Numerical examples validate our theoretical findings.},
  eventtitle = {2015 {{European Control Conference}} ({{ECC}})},
  keywords = {Acceleration,Algorithm design and analysis,Convergence,Convex functions,Gradient methods,Linear programming,Radio frequency},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Ghadimi et al/Ghadimi et al_2015_Global convergence of the Heavy-ball method for convex optimization.pdf;/Users/felix/Zotero/storage/W2IV5QKI/7330562.html}
}

@online{gibianskyHessianFreeOptimization2014,
  title = {Hessian {{Free Optimization}}},
  author = {Gibiansky, Andrew},
  date = {2014-02-13},
  url = {https://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/},
  urldate = {2021-06-07},
  abstract = {Training a neural network involves minimizing its error with respect to its parameters, and since larger neural networks may have millions of parameters, this poses quite a challenge. Surprisingly, many of the recent advances in neural networks come not from improved data processing, neural network architectures, or other machine learning tricks, but instead from incredibly powerful methods for function minimization. In this notebook, we'll go through several optimization methods and work our way up to Hessian Free Optimization, a powerful optimization method adapted to neural networks by James Martens in 2010.},
  organization = {{Andrew Gibiansky :: Math -{$>$} [Code]}},
  file = {/Users/felix/Zotero/storage/BNVPEN9L/hessian-free-optimization.html}
}

@article{gohWhyMomentumReally2017,
  title = {Why {{Momentum Really Works}}},
  author = {Goh, Gabriel},
  date = {2017-04-04},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  number = {4},
  pages = {e6},
  issn = {2476-0757},
  doi = {10.23915/distill.00006},
  abstract = {We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.},
  langid = {english},
  file = {/Users/felix/Zotero/storage/SQTH5E9A/momentum.html}
}

@book{golubMatrixComputations2013,
  title = {Matrix Computations},
  author = {Golub, Gene H. and Van Loan, Charles F.},
  date = {2013},
  series = {Johns {{Hopkins}} Studies in the Mathematical Sciences},
  edition = {Fourth edition},
  publisher = {{The Johns Hopkins University Press}},
  location = {{Baltimore}},
  url = {http://math.ecnu.edu.cn/~jypan/Teaching/books/2013%20Matrix%20Computations%204th.pdf},
  urldate = {2021-11-10},
  isbn = {978-1-4214-0794-4},
  langid = {english},
  pagetotal = {756},
  keywords = {Data processing,Matrices},
  annotation = {OCLC: ocn824733531},
  file = {/Users/felix/gdrive/ZoteroPaper/2013_Golub_Van Loan/Golub_Van Loan_2013_Matrix computations.pdf}
}

@unpublished{guptaAdvancedAlgorithmsFall2020,
  title = {Advanced {{Algorithms}}, {{Fall}} 2020, {{Lecture}} 19},
  author = {Gupta, Anupam},
  date = {2020},
  url = {http://www.cs.cmu.edu/~15850/notes/lec19.pdf},
  urldate = {2021-07-07},
  abstract = {The gradient descent algorithm of the previous chapter is general and powerful: it allows us to (approximately) minimize convex functions over convex bodies. Moreover, it also works in the model of online convex optimization, where the convex function can vary over time, and we want to find a low-regret strategy—one which performs well against every fixed point x . This power and broad applicability means the algorithm is not always the best for specific classes of functions and bodies: for instance, for minimizing linear functions over the probability simplex Dn, we saw in §16.4.1 that the generic gradient descent algorithm does significantly worse than the specialized Hedge algorithm. Show that not only the analysis but the algorithm is bad. This suggests asking: can we somehow change gradient descent to adapt to the “geometry” of the problem? The mirror descent framework of this section allows us to do precisely this. There are many different (and essentially equivalent) ways to explain this framework, each with its positives. We present two of them here: the proximal point view, and the mirror map view, and only mention the others (the preconditioned or quasi-Newton gradient flow view, and the follow the regularized leader view) in passing.},
  file = {/Users/felix/gdrive/ZoteroPaper/_/Mirror Decent.pdf}
}

@online{haghighiNumericalOptimizationUnderstanding2014,
  title = {Numerical {{Optimization}}: Understanding {{L}}-{{BFGS}}},
  shorttitle = {Numerical {{Optimization}}},
  author = {Haghighi, Aria},
  date = {2014-12-02},
  url = {http://aria42.com/blog/2014/12/understanding-lbfgs},
  urldate = {2021-06-17},
  abstract = {Numerical optimization is at the core of much of machine learning. In this post, we derive the L-BFGS algorithm, commonly used in batch machine learning applications.},
  organization = {{aria42}},
  file = {/Users/felix/Zotero/storage/YPWGNHR8/understanding-lbfgs.html}
}

@book{hairerSolvingOrdinaryDifferential1993,
  title = {Solving {{Ordinary Differential Equations I}}: Nonstiff {{Problems}}},
  shorttitle = {Solving {{Ordinary Differential Equations I}}},
  author = {Hairer, Ernst},
  date = {1993},
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  edition = {Second Revised Edition},
  number = {8},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78862-1},
  abstract = {This book deals with methods for solving nonstiff ordinary differential equations. The first chapter describes the historical development of the classical theory from Newton, Leibniz, Euler, and Hamilton to limit cycles and strange attractors. In a second chapter a modern treatment of Runge-Kutta and extrapolation methods is given. Also included are continuous methods for dense output, parallel Runge-Kutta methods, special methods for Hamiltonian systems, second order differential equations and delay equations. The third chapter begins with the classical theory of multistep methods, and concludes with the theory of general linear methods. Many applications from physics, chemistry, biology, and astronomy together with computer programs and numerical comparisons are presented. The book will be immensely useful to graduate students and researchers in numerical analysis and scientific computing, and to scientists in the fields mentioned above. "This is the revised version of the first edition of Vol. I published in 1987. ….Vols. I and II (SSCM 14) of Solving Ordinary Differential Equations together are the standard text on numerical methods for ODEs. ..This book is well written and is together with Vol. II, the most comprehensive modern text on numerical integration methods for ODEs. It may serve a a text book for graduate courses, ..and also as a reference book for all those who have to solve ODE problems numerically." Zeitschrift für Angewandte Mathematik und Physik},
  editora = {Wanner, Gerhard and Nørsett, Syvert P.},
  editoratype = {collaborator},
  isbn = {978-3-540-78862-1},
  langid = {english},
  pagetotal = {xv+528},
  keywords = {Global analysis (Mathematics); Numerical analysis; Mathematics},
  file = {/Users/felix/gdrive/ZoteroPaper/1993_Hairer/Hairer_1993_Solving Ordinary Differential Equations I.pdf}
}

@online{hardtTrainFasterGeneralize2016,
  title = {Train Faster, Generalize Better: Stability of Stochastic Gradient Descent},
  shorttitle = {Train Faster, Generalize Better},
  author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  date = {2016-02-07},
  eprint = {1509.01240},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1509.01240},
  urldate = {2021-04-19},
  abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2016_Hardt et al/Hardt et al_2016_Train faster, generalize better.pdf;/Users/felix/Zotero/storage/Z4N4PGBE/1509.html}
}

@article{heConditionsConvergenceEvolutionary2001,
  title = {Conditions for the Convergence of Evolutionary Algorithms},
  author = {He, Jun and Yu, Xinghuo},
  date = {2001-07-01},
  journaltitle = {Journal of Systems Architecture},
  shortjournal = {Journal of Systems Architecture},
  series = {Evolutionary Computing},
  volume = {47},
  number = {7},
  pages = {601--612},
  issn = {1383-7621},
  doi = {10.1016/S1383-7621(01)00018-2},
  abstract = {This paper presents a theoretical analysis of the convergence conditions for evolutionary algorithms. The necessary and sufficient conditions, necessary conditions, and sufficient conditions for the convergence of evolutionary algorithms to the global optima are derived, which describe their limiting behaviors. Their relationships are explored. Upper and lower bounds of the convergence rates of the evolutionary algorithms are given.},
  langid = {english},
  keywords = {Convergence,Convergence rate,Evolutionary algorithms,Markov chain},
  file = {/Users/felix/gdrive/ZoteroPaper/2001_He_Yu/He_Yu_2001_Conditions for the convergence of evolutionary algorithms.pdf;/Users/felix/Zotero/storage/JT88CMBC/S1383762101000182.html}
}

@incollection{hendersonTheoryPracticeSimulated2003,
  title = {The {{Theory}} and {{Practice}} of {{Simulated Annealing}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Henderson, Darrall and Jacobson, Sheldon H. and Johnson, Alan W.},
  editor = {Glover, Fred and Kochenberger, Gary A.},
  date = {2003},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {287--319},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/0-306-48056-5_10},
  abstract = {Simulated annealing is a popular local search meta-heuristic used to address discrete and, to a lesser extent, continuous optimization problems. The key feature of simulated annealing is that it provides a means to escape local optima by allowing hill-climbing moves (i.e., moves which worsen the objective function value) in hopes of finding a global optimum. A brief history of simulated annealing is presented, including a review of its application to discrete and continuous optimization problems. Convergence theory for simulated annealing is reviewed, as well as recent advances in the analysis of finite time performance. Other local search algorithms are discussed in terms of their relationship to simulated annealing. The chapter also presents practical guidelines for the implementation of simulated annealing in terms of cooling schedules, neighborhood functions, and appropriate applications.},
  isbn = {978-0-306-48056-0},
  langid = {english},
  keywords = {Heuristics,Local Search Algorithms,Meta-heuristics,Simulated Annealing},
  file = {/Users/felix/gdrive/ZoteroPaper/2003_Henderson et al/Henderson et al_2003_The Theory and Practice of Simulated Annealing.pdf}
}

@online{heuselGANsTrainedTwo2018,
  title = {{{GANs Trained}} by a {{Two Time}}-{{Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2018-01-12},
  eprint = {1706.08500},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.08500},
  urldate = {2021-06-08},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Heusel et al/Heusel et al_2018_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf;/Users/felix/Zotero/storage/IF4TQMVR/1706.html}
}

@unpublished{hintonNeuralNetworksMachine2012,
  type = {Massive Open Online Course},
  title = {Neural {{Networks}} for {{Machine Learning}}},
  author = {Hinton, Geoffrey},
  date = {2012},
  url = {https://www.cs.toronto.edu/~hinton/coursera_lectures.html},
  urldate = {2021-11-16},
  editora = {Sirvastava, Nitish and Swersky, Kevin},
  editoratype = {collaborator},
  venue = {{Coursera}},
  annotation = {Lecture 6 Slides: http://www.cs.toronto.edu/\textasciitilde tijmen/csc321/slides/lecture\_slides\_lec6.pdf},
  file = {/Users/felix/gdrive/ZoteroPaper/2012_Hinton/Hinton_2012_Neural Networks for Machine Learning.pdf}
}

@incollection{hirschChapterLinearSystems1974,
  title = {Chapter 5: Linear {{Systems}} and {{Exponentials}} of {{Operators}}},
  shorttitle = {Chapter 5},
  booktitle = {Pure and {{Applied Mathematics}}},
  author = {Hirsch, Morris W. and Smale, Stephen},
  date = {1974},
  volume = {60},
  pages = {74--108},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-8169(08)60661-3},
  isbn = {978-0-12-349550-1},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/1974_Hirsch_Smale/Hirsch_Smale_1974_Chapter 5.pdf}
}

@article{hochreiterFlatMinima1997,
  title = {Flat {{Minima}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-01-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {9},
  number = {1},
  pages = {1--42},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.1.1},
  abstract = {We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a “flat” minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to “simple” networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a “good” weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and “optimal brain surgeon/optimal brain damage.”},
  file = {/Users/felix/gdrive/ZoteroPaper/1997_Hochreiter_Schmidhuber/Hochreiter_Schmidhuber_1997_Flat Minima.pdf;/Users/felix/Zotero/storage/T24KCAGB/Flat-Minima.html}
}

@online{hofferTrainLongerGeneralize2018,
  title = {Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks},
  shorttitle = {Train Longer, Generalize Better},
  author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  date = {2018-01-01},
  eprint = {1705.08741},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.08741},
  urldate = {2021-05-27},
  abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Hoffer et al/Hoffer et al_2018_Train longer, generalize better.pdf;/Users/felix/Zotero/storage/R3E6INJM/1705.html}
}

@article{huddeStochasticGronwallInequality2021,
  title = {A Stochastic {{Gronwall}} Inequality and Applications to Moments, Strong Completeness, Strong Local {{Lipschitz}} Continuity, and Perturbations},
  author = {Hudde, Anselm and Hutzenthaler, Martin and Mazzonetto, Sara},
  date = {2021-05},
  journaltitle = {Annales de l'Institut Henri Poincaré, Probabilités et Statistiques},
  volume = {57},
  number = {2},
  pages = {603--626},
  publisher = {{Institut Henri Poincaré}},
  issn = {0246-0203},
  doi = {10.1214/20-AIHP1064},
  abstract = {There are numerous applications of the classical (deterministic) Gronwall inequality. Recently, Michael Scheutzow discovered a stochastic Gronwall inequality which provides upper bounds for p-th moments, p∈(0,1), of the supremum of nonnegative scalar continuous processes which satisfy a linear integral inequality. In this article we complement this with upper bounds for p-th moments, p∈[2,∞), of the supremum of general Itô processes which satisfy a suitable one-sided affine-linear growth condition. As example applications, we improve known results on strong local Lipschitz continuity in the starting point of solutions of stochastic differential equations (SDEs), on (exponential) moment estimates for SDEs, on strong completeness of SDEs, and on perturbation estimates for SDEs.},
  keywords = {60E15,60H10,Exponential moments,martingale inequality,Perturbation theory,Stochastic Gronwall inequality,Stochastic Gronwall lemma,strong completeness,Strong local Lipschitz continuity},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Hudde et al/Hudde et al_2021_A stochastic Gronwall inequality and applications to moments, strong.pdf;/Users/felix/Zotero/storage/RF7V9J8Q/20-AIHP1064.html}
}

@article{imkellerFirstExitTimes2006,
  title = {First Exit Times of {{SDEs}} Driven by Stable {{Lévy}} Processes},
  author = {Imkeller, P. and Pavlyukevich, I.},
  date = {2006-04-01},
  journaltitle = {Stochastic Processes and their Applications},
  shortjournal = {Stochastic Processes and their Applications},
  volume = {116},
  number = {4},
  pages = {611--642},
  issn = {0304-4149},
  doi = {10.1016/j.spa.2005.11.006},
  abstract = {We study the exit problem of solutions of the stochastic differential equation dXtε=−U′(Xtε)dt+εdLt from bounded or unbounded intervals which contain the unique asymptotically stable critical point of the deterministic dynamical system Ẏt=−U′(Yt). The process L is composed of a standard Brownian motion and a symmetric α-stable Lévy process. Using probabilistic estimates we show that, in the small noise limit ε→0, the exit time of Xε from an interval is an exponentially distributed random variable and determine its expected value. Due to the heavy-tail nature of the α-stable component of L, the results differ strongly from the well known case in which the deterministic dynamical system undergoes purely Gaussian perturbations.},
  langid = {english},
  keywords = {-Stable process,Exit time law,Extreme events,First exit,Infinitely divisible distribution,Kramers’ law,Lévy flight,Lévy process},
  file = {/Users/felix/gdrive/ZoteroPaper/2006_Imkeller_Pavlyukevich/Imkeller_Pavlyukevich_2006_First exit times of SDEs driven by stable Lévy processes.pdf;/Users/felix/Zotero/storage/Z96RN5W4/S0304414905001614.html}
}

@article{imkellerFirstExitTimes2010,
  title = {First {{Exit Times}} of {{Non}}-Linear {{Dynamical Systems}} in~{{ℝdPerturbed}} by {{Multifractal Lévy Noise}}},
  author = {Imkeller, Peter and Pavlyukevich, Ilya and Stauch, Michael},
  date = {2010-10-01},
  journaltitle = {Journal of Statistical Physics},
  shortjournal = {J Stat Phys},
  volume = {141},
  number = {1},
  pages = {94--119},
  issn = {1572-9613},
  doi = {10.1007/s10955-010-0041-6},
  abstract = {In a domain \$\textbackslash mathcal\{G\}\textbackslash subset\textbackslash mathbb\{R\}\^\{d\}\$we study a dynamical system which is perturbed in finitely many directions i by one-dimensional Lévy processes with αi-stable components. We investigate the exit behavior of the system from the domain in the small noise limit. Using probabilistic estimates on the Laplace transform of the exit time we show that it is exponentially distributed with a parameter that depends on the smallest αi. Finally we prove that the system exits from the domain in the direction of the process with the smallest~αi.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2010_Imkeller et al/Imkeller et al_2010_First Exit Times of Non-linear Dynamical Systems in ℝdPerturbed by Multifractal.pdf}
}

@article{imkellerHierarchyExitTimes2010,
  title = {The Hierarchy of Exit Times of {{Lévy}}-Driven {{Langevin}} Equations},
  author = {Imkeller, P. and Pavlyukevich, I. and Wetzel, T.},
  date = {2010-12-01},
  journaltitle = {The European Physical Journal Special Topics},
  shortjournal = {Eur. Phys. J. Spec. Top.},
  volume = {191},
  number = {1},
  pages = {211--222},
  issn = {1951-6401},
  doi = {10.1140/epjst/e2010-01351-7},
  abstract = {In this paper we consider the first exit problem of an overdamped Lévy driven particle in a confining potential. We survey results obtained in recent years from our work on the Kramers’ times for dynamical systems of this type with Lévy perturbations containing heavy, and exponentially light jumps, and compare them to the well known case of dynamical systems with Gaussian perturbations. It turns out that exits induced by Lévy processes with jumps are always essentially faster than Gaussian exits.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2010_Imkeller et al/Imkeller et al_2010_The hierarchy of exit times of Lévy-driven Langevin equations.pdf}
}

@online{IMOmathHopitalTheorem,
  title = {{{IMOmath}}: L’{{Hopital}}’s {{Theorem}}},
  url = {https://www.imomath.com/index.php?options=686},
  urldate = {2021-09-15},
  file = {/Users/felix/Zotero/storage/CQ445NBU/index.html}
}

@online{israelHowWorkOut2012,
  title = {How to Work out This Problem Using {{Stolz}}'s Theorem?},
  author = {Israel, Robert (https://math.stackexchange.com/users/8508)},
  date = {2012-10-05},
  url = {https://math.stackexchange.com/q/207596},
  organization = {{Mathematics Stack Exchange}}
}

@online{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  date = {2019-02-25},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.05407},
  urldate = {2021-06-15},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Izmailov et al/Izmailov et al_2019_Averaging Weights Leads to Wider Optima and Better Generalization.pdf;/Users/felix/Zotero/storage/CH682FMV/1803.html}
}

@online{jastrzebskiThreeFactorsInfluencing2018,
  title = {Three {{Factors Influencing Minima}} in {{SGD}}},
  author = {Jastrzębski, Stanisław and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  date = {2018-09-13},
  eprint = {1711.04623},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.04623},
  urldate = {2021-05-27},
  abstract = {We investigate the dynamical and convergent properties of stochastic gradient descent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization, remains an open question. In order to tackle this problem we investigate the previously proposed approximation of SGD by a stochastic differential equation (SDE). We theoretically argue that three factors - learning rate, batch size and gradient covariance - influence the minima found by SGD. In particular we find that the ratio of learning rate to batch size is a key determinant of SGD dynamics and of the width of the final minima, and that higher values of the ratio lead to wider minima and often better generalization. We confirm these findings experimentally. Further, we include experiments which show that learning rate schedules can be replaced with batch size schedules and that the ratio of learning rate to batch size is an important factor influencing the memorization process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Jastrzębski et al/Jastrzębski et al_2018_Three Factors Influencing Minima in SGD.pdf;/Users/felix/Zotero/storage/YPZIQZFC/1711.html}
}

@inproceedings{jiaInformationTheoreticLocalMinima2020,
  title = {Information-{{Theoretic Local Minima Characterization}} and {{Regularization}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Jia, Zhiwei and Su, Hao},
  date = {2020-11-21},
  pages = {4773--4783},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/jia20a.html},
  urldate = {2021-05-27},
  abstract = {Recent advances in deep learning theory have evoked the study of generalizability across different local minima of deep neural networks (DNNs). While current work focused on either discovering prop...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Jia_Su/Jia_Su_2020_Information-Theoretic Local Minima Characterization and Regularization.pdf;/Users/felix/Zotero/storage/9WX7XUPX/jia20a.html}
}

@online{karimiLinearConvergenceGradient2020,
  title = {Linear {{Convergence}} of {{Gradient}} and {{Proximal}}-{{Gradient Methods Under}} the Polyak-\L{}ojasiewicz {{Condition}}},
  author = {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  date = {2020-09-12},
  eprint = {1608.04636},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1608.04636},
  urldate = {2021-09-21},
  abstract = {In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \textbackslash L\{\}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\textbackslash L\{\}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of randomized and greedy coordinate descent methods, sign-based gradient descent methods, and stochastic gradient methods in the classic setting (with decreasing or constant step-sizes) as well as the variance-reduced setting. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence of these methods. Along the way, we give simple convergence results for a wide variety of problems in machine learning: least squares, logistic regression, boosting, resilient backpropagation, L1-regularization, support vector machines, stochastic dual coordinate ascent, and stochastic variance-reduced gradient methods.},
  archiveprefix = {arXiv},
  keywords = {65K10,Computer Science - Machine Learning,G.1.6,I.2.6,Mathematics - Optimization and Control,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Karimi et al/Karimi et al_2020_Linear Convergence of Gradient and Proximal-Gradient Methods Under the.pdf;/Users/felix/Zotero/storage/GHLSLYL9/1608.html}
}

@online{kawaguchiGeneralizationDeepLearning2020,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  date = {2020-07-27},
  eprint = {1710.05468},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.05468},
  urldate = {2021-06-16},
  abstract = {This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Kawaguchi et al/Kawaguchi et al_2020_Generalization in Deep Learning.pdf;/Users/felix/Zotero/storage/YZZMGICA/1710.html}
}

@online{keskarLargeBatchTrainingDeep2017,
  title = {On {{Large}}-{{Batch Training}} for {{Deep Learning}}: Generalization {{Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large}}-{{Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  date = {2017-02-09},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/1609.04836},
  urldate = {2021-05-27},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Keskar et al/Keskar et al_2017_On Large-Batch Training for Deep Learning.pdf;/Users/felix/Zotero/storage/MRJDPXL5/1609.html}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: A {{Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2021-04-19},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Kingma_Ba/Kingma_Ba_2017_Adam.pdf;/Users/felix/Zotero/storage/6WJBPWQT/1412.html}
}

@article{kozyakinAccuracyApproximationSpectral2009,
  title = {On Accuracy of Approximation of the Spectral Radius by the {{Gelfand}} Formula},
  author = {Kozyakin, Victor},
  date = {2009-11-01},
  journaltitle = {Linear Algebra and its Applications},
  shortjournal = {Linear Algebra and its Applications},
  volume = {431},
  number = {11},
  pages = {2134--2141},
  issn = {0024-3795},
  doi = {10.1016/j.laa.2009.07.008},
  abstract = {The famous Gelfand formula ρ(A)=limsupn→∞‖An‖1/n for the spectral radius of a matrix is of great importance in various mathematical constructions. Unfortunately, the range of applicability of this formula is substantially restricted by a lack of estimates for the rate of convergence of the quantities ‖An‖1/n to ρ(A). In the paper this deficiency is made up to some extent. By using the Bochi inequalities we establish explicit computable estimates for the rate of convergence of the quantities ‖An‖1/n to ρ(A). The obtained estimates are then extended for evaluation of the joint spectral radius of matrix sets.},
  langid = {english},
  keywords = {Generalized spectral radius,Infinite matrix products,Joint spectral radius},
  file = {/Users/felix/gdrive/ZoteroPaper/2009_Kozyakin/Kozyakin_2009_On accuracy of approximation of the spectral radius by the Gelfand formula.pdf}
}

@incollection{krizekGeometricInterpretationsConjugate2004,
  title = {Geometric {{Interpretations}} of {{Conjugate Gradient}} and {{Related Methods}}},
  booktitle = {Conjugate {{Gradient Algorithms}} and {{Finite Element Methods}}},
  author = {Křížek, Michal and Korotov, Sergey},
  editor = {Křížek, Michal and Neittaanmäki, Pekka and Korotov, Sergey and Glowinski, Roland},
  date = {2004},
  series = {Scientific {{Computation}}},
  pages = {25--43},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-18560-1_3},
  isbn = {978-3-642-62159-8 978-3-642-18560-1}
}

@article{kushnerRatesConvergenceSequential1978,
  title = {Rates of {{Convergence}} for {{Sequential Monte Carlo Optimization Methods}}},
  author = {Kushner, Harold J.},
  date = {1978-01-01},
  journaltitle = {SIAM Journal on Control and Optimization},
  shortjournal = {SIAM J. Control Optim.},
  volume = {16},
  number = {1},
  pages = {150--168},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0363-0129},
  doi = {10.1137/0316012},
  abstract = {Sequential Monte Carlo methods of the stochastic approximation (SA) type, with and without constraints, are discussed. The rates of convergence are derived, and the quantities upon which the rates depend, are discussed. Let \$\textbackslash\{ \{X\_n \} \textbackslash\}\$ denote the SA sequence and define \$U\_n = (n + 1)\^\textbackslash beta X\_n \$ for a suitable \$\textbackslash beta {$>$} 0\$. The \$\textbackslash\{ \{U\_n \} \textbackslash\}\$ are interpolated into a natural continuous time process, and weak convergence theory is applied to develop the properties of the tails of the sequence. The technique has a number of advantages over past approaches—advantages which are discussed in the paper. It gives more insight (and is apparently more readily generalizable) than do other approaches—and suggests ways of improving the convergence. The particular “dynamical” nature of the approach allows one to say more about the “tail” process—and to do more “decision” (or “control”) analysis with it.},
  file = {/Users/felix/gdrive/ZoteroPaper/1978_Kushner/Kushner_1978_Rates of Convergence for Sequential Monte Carlo Optimization Methods.pdf}
}

@book{kushnerStochasticApproximationAlgorithms1997,
  title = {Stochastic Approximation Algorithms and Applications},
  author = {Kushner, Harold J.},
  date = {1997},
  series = {Applications of Mathematics; 35},
  publisher = {{u.a. Springer}},
  location = {{Berlin Heidelberg [u.a.]}},
  editora = {Yin, George},
  editoratype = {collaborator},
  isbn = {978-0-387-94916-1},
  langid = {english},
  pagetotal = {xxi+417},
  keywords = {Stochastic approximation,Stochastic approximation; Stochastic approximation,Stochastische Approximation},
  file = {/Users/felix/gdrive/ZoteroPaper/1997_Kushner/Kushner_1997_Stochastic approximation algorithms and applications.pdf}
}

@article{latzAnalysisStochasticGradient2021,
  title = {Analysis of {{Stochastic Gradient Descent}} in {{Continuous Time}}},
  author = {Latz, Jonas},
  date = {2021-07},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {31},
  number = {4},
  eprint = {2004.07177},
  eprinttype = {arxiv},
  pages = {39},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-021-10016-8},
  abstract = {Stochastic gradient descent is an optimisation method that combines classical gradient descent with random subsampling within the target functional. In this work, we introduce the stochastic gradient process as a continuous-time representation of stochastic gradient descent. The stochastic gradient process is a dynamical system that is coupled with a continuous-time Markov process living on a finite state space. The dynamical system -- a gradient flow -- represents the gradient descent part, the process on the finite state space represents the random subsampling. Processes of this type are, for instance, used to model clonal populations in fluctuating environments. After introducing it, we study theoretical properties of the stochastic gradient process: We show that it converges weakly to the gradient flow with respect to the full target function, as the learning rate approaches zero. We give conditions under which the stochastic gradient process with constant learning rate is exponentially ergodic in the Wasserstein sense. Then we study the case, where the learning rate goes to zero sufficiently slowly and the single target functions are strongly convex. In this case, the process converges weakly to the point mass concentrated in the global minimum of the full target function; indicating consistency of the method. We conclude after a discussion of discretisation strategies for the stochastic gradient process and numerical experiments.},
  archiveprefix = {arXiv},
  keywords = {90C30; 60J25; 37A25; 65C40; 68W20,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Probability,Statistics - Computation},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Latz/Latz_2021_Analysis of Stochastic Gradient Descent in Continuous Time.pdf;/Users/felix/Zotero/storage/ZD43ZS7P/2004.html}
}

@inproceedings{lerouxTopmoumouteOnlineNatural2007,
  title = {Topmoumoute {{Online Natural Gradient Algorithm}}.},
  booktitle = {{{NIPS}}},
  author = {Le Roux, Nicolas and Manzagol, Pierre-Antoine and Bengio, Yoshua},
  date = {2007},
  pages = {849--856},
  publisher = {{Citeseer}},
  file = {/Users/felix/gdrive/ZoteroPaper/2007_Le Roux et al/Le Roux et al_2007_Topmoumoute Online Natural Gradient Algorithm.pdf}
}

@article{lessardAnalysisDesignOptimization2016,
  title = {Analysis and {{Design}} of {{Optimization Algorithms}} via {{Integral Quadratic Constraints}}},
  author = {Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  date = {2016-01-01},
  journaltitle = {SIAM Journal on Optimization},
  shortjournal = {SIAM J. Optim.},
  volume = {26},
  number = {1},
  pages = {57--95},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1052-6234},
  doi = {10.1137/15M1009597},
  abstract = {This paper develops a new framework to analyze and design iterative optimization algorithms built on the notion of integral quadratic constraints (IQCs) from robust control theory. IQCs provide sufficient conditions for the stability of complicated interconnected systems, and these conditions can be checked by semidefinite programming. We discuss how to adapt IQC theory to study optimization algorithms, proving new inequalities about convex functions and providing a version of IQC theory adapted for use by optimization researchers. Using these inequalities, we derive numerical upper bounds on convergence rates for the Gradient method, the Heavy-ball method, Nesterov's accelerated method, and related variants by solving small, simple semidefinite programming problems. We also briefly show how these techniques can be used to search for optimization algorithms with desired performance characteristics, establishing a new methodology for algorithm design.},
  keywords = {90C22,90C25,90C30,93C10,93D99,control theory,convex optimization,first-order methods,Heavy-ball method,integral quadratic constraints,Nesterov's method,proximal gradient methods,semidefinite programming},
  file = {/Users/felix/gdrive/ZoteroPaper/2016_Lessard et al/Lessard et al_2016_Analysis and Design of Optimization Algorithms via Integral Quadratic.pdf}
}

@inproceedings{liStochasticModifiedEquations2017,
  title = {Stochastic {{Modified Equations}} and {{Adaptive Stochastic Gradient Algorithms}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
  date = {2017-07-17},
  pages = {2101--2110},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/li17f.html},
  urldate = {2021-09-23},
  abstract = {We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Li et al/Li et al_2017_Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms.pdf;/Users/felix/Zotero/storage/ZG5EBU4I/Li et al. - 2017 - Stochastic Modified Equations and Adaptive Stochas.pdf}
}

@article{liStochasticModifiedEquations2019,
  title = {Stochastic {{Modiﬁed Equations}} and {{Dynamics}} of {{Stochastic Gradient Algorithms I}}: Mathematical {{Foundations}}},
  author = {Li, Qianxiao and Tai, Cheng},
  date = {2019},
  pages = {47},
  abstract = {We develop the mathematical foundations of the stochastic modified equations (SME) framework for analyzing the dynamics of stochastic gradient algorithms, where the latter is approximated by a class of stochastic differential equations with small noise parameters. We prove that this approximation can be understood mathematically as an weak approximation, which leads to a number of precise and useful results on the approximations of stochastic gradient descent (SGD), momentum SGD and stochastic Nesterov’s accelerated gradient method in the general setting of stochastic objectives. We also demonstrate through explicit calculations that this continuous-time approach can uncover important analytical insights into the stochastic gradient algorithms under consideration that may not be easy to obtain in a purely discrete-time setting.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Li_Tai/Li_Tai_2019_Stochastic Modiﬁed Equations and Dynamics of Stochastic Gradient Algorithms I.pdf}
}

@online{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  date = {2018-11-07},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1712.09913},
  urldate = {2021-06-16},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Li et al/Li et al_2018_Visualizing the Loss Landscape of Neural Nets.pdf;/Users/felix/gdrive/ZoteroPaper/2018_Li et al/Li et al_2018_Visualizing the Loss Landscape of Neural Nets2.pdf;/Users/felix/Zotero/storage/CI5Q2T87/1712.html}
}

@article{mahsereciProbabilisticLineSearches2017,
  title = {Probabilistic {{Line Searches}} for {{Stochastic Optimization}}},
  author = {Mahsereci, Maren and Hennig, Philipp},
  date = {2017},
  pages = {59},
  abstract = {In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/_Mahsereci_Hennig/Mahsereci_Hennig_Probabilistic Line Searches for Stochastic Optimization.pdf}
}

@online{mandtStochasticGradientDescent2018,
  ids = {mandtStochasticGradientDescent2017},
  title = {Stochastic {{Gradient Descent}} as {{Approximate Bayesian Inference}}},
  author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  date = {2018-01-19},
  eprint = {1704.04289},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1704.04289},
  urldate = {2021-06-08},
  abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Mandt et al/Mandt et al_2017_Stochastic gradient descent as approximate bayesian inference.pdf;/Users/felix/gdrive/ZoteroPaper/2018_Mandt et al/Mandt et al_2018_Stochastic Gradient Descent as Approximate Bayesian Inference.pdf;/Users/felix/Zotero/storage/3F563NK4/1704.html}
}

@inproceedings{martensDeepLearningHessianfree2010,
  title = {Deep Learning via {{Hessian}}-Free Optimization},
  booktitle = {{{ICML}}},
  author = {Martens, James},
  date = {2010},
  volume = {27},
  pages = {8},
  abstract = {We develop a 2nd-order optimization method based on the “Hessian-free” approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton \& Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn’t limited in applicability to autoencoders, or any specific model class. We also discuss the issue of “pathological curvature” as a possible explanation for the difficulty of deeplearning and how 2nd-order optimization, and our method in particular, effectively deals with it.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2010_Martens/Martens_2010_Deep learning via hessian-free optimization2.pdf}
}

@online{martensOptimizingNeuralNetworks2020,
  title = {Optimizing {{Neural Networks}} with {{Kronecker}}-Factored {{Approximate Curvature}}},
  author = {Martens, James and Grosse, Roger},
  date = {2020-06-07},
  eprint = {1503.05671},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1503.05671},
  urldate = {2021-06-17},
  abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network’s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC’s approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Martens_Grosse/Martens_Grosse_2020_Optimizing Neural Networks with Kronecker-factored Approximate Curvature.pdf}
}

@online{metzGradientsAreNot2021,
  title = {Gradients Are {{Not All You Need}}},
  author = {Metz, Luke and Freeman, C. Daniel and Schoenholz, Samuel S. and Kachman, Tal},
  date = {2021-11-10},
  eprint = {2111.05803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2111.05803},
  urldate = {2021-11-16},
  abstract = {Differentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Metz et al/Metz et al_2021_Gradients are Not All You Need.pdf;/Users/felix/Zotero/storage/CUR7MCM6/2111.html}
}

@inproceedings{mulayoffUniquePropertiesFlat2020,
  title = {Unique {{Properties}} of {{Flat Minima}} in {{Deep Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Mulayoff, Rotem and Michaeli, Tomer},
  date = {2020-11-21},
  pages = {7108--7118},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/mulayoff20a.html},
  urldate = {2021-05-27},
  abstract = {It is well known that (stochastic) gradient descent has an implicit bias towards flat minima. In deep neural network training, this mechanism serves to screen out minima. However, the precise effec...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Mulayoff_Michaeli/Mulayoff_Michaeli_2020_Unique Properties of Flat Minima in Deep Networks.pdf;/Users/felix/Zotero/storage/3Z7EQ2T8/mulayoff20a.html}
}

@article{murataStatisticalStudyOnline1998,
  title = {A Statistical Study of On-Line Learning},
  author = {Murata, Noboru},
  date = {1998},
  journaltitle = {Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK},
  pages = {63--92},
  abstract = {In this paper we examine on-line learning with statistical framework. Firstly we study the cases with fixed and annealed learning rate. It can be shown that on-line learning with 1/t annealed learning rate minimizes the generalization error with the same rate as batch learning in the asymptotic regime, that is, on-line learning can be as effective as batch learning asymptotically. Using these analyses, we study an adaptive learning rate algorithm which is based on Sompolinsky-Barkai-Seung algorithm and which achieves 1/t-annealing automatically.},
  file = {/Users/felix/gdrive/ZoteroPaper/1998_Murata/Murata_1998_A statistical study of on-line learning.pdf}
}

@online{neelakantanAddingGradientNoise2015,
  title = {Adding {{Gradient Noise Improves Learning}} for {{Very Deep Networks}}},
  author = {Neelakantan, Arvind and Vilnis, Luke and Le, Quoc V. and Sutskever, Ilya and Kaiser, Lukasz and Kurach, Karol and Martens, James},
  date = {2015-11-20},
  eprint = {1511.06807},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1511.06807},
  urldate = {2021-06-08},
  abstract = {Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. The main motivation for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72\% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Neelakantan et al/Neelakantan et al_2015_Adding Gradient Noise Improves Learning for Very Deep Networks.pdf;/Users/felix/Zotero/storage/PIBJNLF7/1511.html}
}

@article{nemirovskiRobustStochasticApproximation2009,
  title = {Robust {{Stochastic Approximation Approach}} to {{Stochastic Programming}}},
  author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
  date = {2009-01-01},
  journaltitle = {SIAM Journal on Optimization},
  shortjournal = {SIAM J. Optim.},
  volume = {19},
  number = {4},
  pages = {1574--1609},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1052-6234},
  doi = {10.1137/070704277},
  abstract = {In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments.},
  keywords = {90C15,90C25,complexity,minimax problems,mirror descent algorithm,Monte Carlo sampling,saddle point,sample average approximation method,stochastic approximation,stochastic programming},
  file = {/Users/felix/gdrive/ZoteroPaper/2009_Nemirovski et al/Nemirovski et al_2009_Robust Stochastic Approximation Approach to Stochastic Programming.pdf}
}

@article{nesterovAcceleratingCubicRegularization2008a,
  title = {Accelerating the Cubic Regularization of {{Newton}}’s Method on Convex Problems},
  author = {Nesterov, Yurii Evgen'evič},
  date = {2008-03-01},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  volume = {112},
  number = {1},
  pages = {159--181},
  issn = {1436-4646},
  doi = {10.1007/s10107-006-0089-x},
  abstract = {In this paper we propose an accelerated version of the cubic regularization of Newton’s method (Nesterov and Polyak, in Math Program 108(1): 177–205, 2006). The original version, used for minimizing a convex function with Lipschitz-continuous Hessian, guarantees a global rate of convergence of order \$\$O\textbackslash big(\{1 \textbackslash over k\^2\}\textbackslash big)\$\$, where k is the iteration counter. Our modified version converges for the same problem class with order \$\$O\textbackslash big(\{1 \textbackslash over k\^3\}\textbackslash big)\$\$, keeping the complexity of each iteration unchanged. We study the complexity of both schemes on different classes of convex problems. In particular, we argue that for the second-order schemes, the class of non-degenerate problems is different from the standard class.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2008_Nesterov/Nesterov_2008_Accelerating the cubic regularization of Newton’s method on convex problems2.pdf;/Users/felix/gdrive/ZoteroPaper/2008_Nesterov/Nesterov_2008_Accelerating the cubic regularization of Newton’s method on convex problems3.pdf}
}

@book{nesterovIntroductoryLecturesConvex2004,
  title = {Introductory {{Lectures}} on {{Convex Optimization}}},
  author = {Nesterov, Yurii Evgen'evič},
  date = {2004},
  series = {Applied {{Optimization}}},
  volume = {87},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4419-8853-9},
  abstract = {The first elementary exposition of core ideas of complexity theory for convex optimization, this book explores optimal methods and lower complexity bounds for smooth and non-smooth convex optimization. Also covers polynomial-time interior-point methods.},
  editorb = {Pardalos, Panos M. and Hearn, Donald W.},
  editorbtype = {redactor},
  isbn = {978-1-4613-4691-3 978-1-4419-8853-9},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2004_Nesterov/Nesterov_2004_Introductory Lectures on Convex Optimization.pdf}
}

@book{nesterovLecturesConvexOptimization2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii Evgen'evič},
  date = {2018},
  series = {Springer Optimization and {{Its}} Applications; Volume 137},
  edition = {Second edition},
  publisher = {{Springer}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-91578-4},
  abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author’s lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics, Introduction -- Part I Black-Box Optimization -- 1 Nonlinear Optimization -- 2 Smooth Convex Optimization -- 3 Nonsmooth Convex Optimization -- 4 Second-Order Methods -- Part II Structural Optimization -- 5 Polynomial-time Interior-Point Methods -- 6 Primal-Dual Model of Objective Function -- 7 Optimization in Relative Scale -- Bibliographical Comments -- Appendix A. Solving some Auxiliary Optimization Problems -- References -- Index},
  isbn = {978-3-319-91578-4},
  langid = {english},
  keywords = {Computer software; Optimization; Mathematical optimization; Algorithms,Konvexe Optimierung},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Nesterov/Nesterov_2018_Lectures on Convex Optimization.pdf}
}

@article{nesterovMethodSolvingConvex1983,
  title = {A Method for Solving the Convex Programming Problem with Convergence Rate \(O(1/k^2)\)},
  author = {Nesterov, Yurii Evgen'evič},
  date = {1983},
  journaltitle = {Dokl. Akad. Nauk SSSR},
  volume = {269},
  pages = {543--547},
  url = {http://m.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=46009&option_lang=eng},
  urldate = {2021-07-17},
  file = {/Users/felix/gdrive/ZoteroPaper/1983_Nesterov/Nesterov_1983_A method for solving the convex programming problem with convergence rate.pdf;/Users/felix/Zotero/storage/LUQ6H5UH/10029946121.html}
}

@online{nguyenFirstExitTime2019,
  title = {First {{Exit Time Analysis}} of {{Stochastic Gradient Descent Under Heavy}}-{{Tailed Gradient Noise}}},
  author = {Nguyen, Thanh Huy and Şimşekli, Umut and Gürbüzbalaban, Mert and Richard, Gaël},
  date = {2019-06-21},
  eprint = {1906.09069},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.09069},
  urldate = {2021-09-24},
  abstract = {Stochastic gradient descent (SGD) has been widely used in machine learning due to its computational efficiency and favorable generalization properties. Recently, it has been empirically demonstrated that the gradient noise in several deep learning settings admits a non-Gaussian, heavy-tailed behavior. This suggests that the gradient noise can be modeled by using α-stable distributions, a family of heavy-tailed distributions that appear in the generalized central limit theorem. In this context, SGD can be viewed as a discretization of a stochastic differential equation (SDE) driven by a Lévy motion, and the metastability results for this SDE can then be used for illuminating the behavior of SGD, especially in terms of ‘preferring wide minima’. While this approach brings a new perspective for analyzing SGD, it is limited in the sense that, due to the time discretization, SGD might admit a significantly different behavior than its continuous-time limit. Intuitively, the behaviors of these two systems are expected to be similar to each other only when the discretization step is sufficiently small; however, to the best of our knowledge, there is no theoretical understanding on how small the step-size should be chosen in order to guarantee that the discretized system inherits the properties of the continuous-time system. In this study, we provide formal theoretical analysis where we derive explicit conditions for the step-size such that the metastability behavior of the discrete-time system is similar to its continuous-time limit. We show that the behaviors of the two systems are indeed similar for small step-sizes and we identify how the error depends on the algorithm and problem parameters. We illustrate our results with simulations on a synthetic model and neural networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Nguyen et al/Nguyen et al_2019_First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed.pdf}
}

@article{olahResearchDebt2017,
  title = {Research {{Debt}}},
  author = {Olah, Chris and Carter, Shan},
  date = {2017-03-22},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {2},
  number = {3},
  pages = {e5},
  issn = {2476-0757},
  doi = {10.23915/distill.00005},
  abstract = {Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...},
  langid = {english},
  file = {/Users/felix/Zotero/storage/N7U53ANN/research-debt.html}
}

@online{papyanFullSpectrumDeepnet2019,
  title = {The {{Full Spectrum}} of {{Deepnet Hessians}} at {{Scale}}: Dynamics with {{SGD Training}} and {{Sample Size}}},
  shorttitle = {The {{Full Spectrum}} of {{Deepnet Hessians}} at {{Scale}}},
  author = {Papyan, Vardan},
  date = {2019-06-02},
  eprint = {1811.07062},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.07062},
  urldate = {2021-06-30},
  abstract = {We apply state-of-the-art tools in modern high-dimensional numerical linear algebra to approximate efficiently the spectrum of the Hessian of modern deepnets, with tens of millions of parameters, trained on real data. Our results corroborate previous findings, based on small-scale networks, that the Hessian exhibits "spiked" behavior, with several outliers isolated from a continuous bulk. We decompose the Hessian into different components and study the dynamics with training and sample size of each term individually.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Papyan/Papyan_2019_The Full Spectrum of Deepnet Hessians at Scale.pdf;/Users/felix/Zotero/storage/MTD5A8W3/1811.html}
}

@online{pascanuRevisitingNaturalGradient2014,
  title = {Revisiting {{Natural Gradient}} for {{Deep Networks}}},
  author = {Pascanu, Razvan and Bengio, Yoshua},
  date = {2014-02-17},
  eprint = {1301.3584},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1301.3584},
  urldate = {2021-06-30},
  abstract = {We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient and three other recently proposed methods for training deep models: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {/Users/felix/gdrive/ZoteroPaper/2014_Pascanu_Bengio/Pascanu_Bengio_2014_Revisiting Natural Gradient for Deep Networks2.pdf;/Users/felix/Zotero/storage/M4Y4MKA5/1301.html;/Users/felix/Zotero/storage/PGMXBL9X/1301.html}
}

@online{pascanuSaddlePointProblem2014,
  title = {On the Saddle Point Problem for Non-Convex Optimization},
  author = {Pascanu, Razvan and Dauphin, Yann N. and Ganguli, Surya and Bengio, Yoshua},
  date = {2014-05-27},
  eprint = {1405.4604},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1405.4604},
  urldate = {2021-09-16},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for the ability of these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, and neural network theory, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new algorithm, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep neural network training, and provide preliminary numerical evidence for its superior performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/felix/gdrive/ZoteroPaper/2014_Pascanu et al/Pascanu et al_2014_On the saddle point problem for non-convex optimization.pdf;/Users/felix/Zotero/storage/7B28YP5G/1405.html}
}

@inproceedings{pesmeConvergenceDiagnosticBasedStep2020,
  title = {On {{Convergence}}-{{Diagnostic}} Based {{Step Sizes}} for {{Stochastic Gradient Descent}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Pesme, Scott and Dieuleveut, Aymeric and Flammarion, Nicolas},
  date = {2020-11-21},
  pages = {7641--7651},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/pesme20a.html},
  urldate = {2021-09-20},
  abstract = {Constant step-size Stochastic Gradient Descent exhibits two phases: a transient phase during which iterates make fast progress towards the optimum, followed by a stationary phase during which iterates oscillate around the optimal point. In this paper, we show that efficiently detecting this transition and appropriately decreasing the step size can lead to fast convergence rates. We analyse the classical statistical test proposed by Pflug (1983), based on the inner product between consecutive stochastic gradients. Even in the simple case where the objective function is quadratic we show that this test cannot lead to an adequate convergence diagnostic. We then propose a novel and simple statistical procedure that accurately detects stationarity and we provide experimental results showing state-of-the-art performance on synthetic and real-word datasets.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Pesme et al/Pesme et al_2020_On Convergence-Diagnostic based Step Sizes for Stochastic Gradient Descent.pdf;/Users/felix/Zotero/storage/GF333ESK/Pesme et al. - 2020 - On Convergence-Diagnostic based Step Sizes for Sto.pdf}
}

@article{polyakAccelerationStochasticApproximation1992,
  title = {Acceleration of Stochastic Approximation by Averaging},
  author = {Polyak, Boris T. and Juditsky, Anatoli B.},
  date = {1992},
  journaltitle = {SIAM journal on control and optimization},
  volume = {30},
  number = {4},
  pages = {838--855},
  publisher = {{SIAM}},
  file = {/Users/felix/gdrive/ZoteroPaper/1992_Polyak_Juditsky/Polyak_Juditsky_1992_Acceleration of stochastic approximation by averaging.pdf;/Users/felix/Zotero/storage/C53WPIPY/0330046.html}
}

@book{polyakIntroductionOptimization1987,
  title = {Introduction to Optimization},
  author = {Polyak, Boris T.},
  date = {1987},
  publisher = {{New York Optimization Software}},
  isbn = {978-0-911575-14-9},
  langid = {english},
  keywords = {Optimisation mathématique;Optimierung},
  file = {/Users/felix/gdrive/ZoteroPaper/1987_Polyak/Polyak_1987_Introduction to optimization.pdf}
}

@article{polyakMethodsSpeedingConvergence1964,
  title = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author = {Polyak, Boris T.},
  date = {1964-01-01},
  journaltitle = {USSR Computational Mathematics and Mathematical Physics},
  shortjournal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {4},
  number = {5},
  pages = {1--17},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(64)90137-5},
  abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/1964_Polyak/Polyak_1964_Some methods of speeding up the convergence of iteration methods.pdf;/Users/felix/Zotero/storage/YHYZHVCI/0041555364901375.html}
}

@article{qianMomentumTermGradient1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author = {Qian, Ning},
  date = {1999-01-01},
  journaltitle = {Neural Networks},
  volume = {12},
  number = {1},
  pages = {145--151},
  publisher = {{Pergamon}},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00116-6},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improv…},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/1999_Qian/Qian_1999_On the momentum term in gradient descent learning algorithms.pdf;/Users/felix/Zotero/storage/26VLHHCM/S0893608098001166.html}
}

@online{radfordVisualizingOptimizationAlgos2014,
  title = {Visualizing {{Optimization Algos}} - {{GIFs}}},
  author = {Radford, Alec},
  date = {2014-09-18},
  url = {https://imgur.com/a/Hqolp},
  urldate = {2021-11-16},
  abstract = {Algos without scaling based on gradient information really struggle to break symmetry here - SGD gets no where and Nesterov Accelerated Gradient / Momentum exhibits oscillations until they build up velocity in the optimization direction. Algos that scale step size based on the gradient quickly break symmetry and begin descent.},
  langid = {english},
  organization = {{Imgur}},
  file = {/Users/felix/Zotero/storage/EGMXKMTZ/Hqolp.html}
}

@article{rebentrostQuantumGradientDescent2019,
  title = {Quantum Gradient Descent and {{Newton}}’s Method for Constrained Polynomial Optimization},
  author = {Rebentrost, Patrick and Schuld, Maria and Wossnig, Leonard and Petruccione, Francesco and Lloyd, Seth},
  date = {2019-07-17},
  journaltitle = {New Journal of Physics},
  shortjournal = {New J. Phys.},
  volume = {21},
  number = {7},
  pages = {073023},
  publisher = {{IOP Publishing}},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/ab2a9e},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Rebentrost et al/Rebentrost et al_2019_Quantum gradient descent and Newton’s method for constrained polynomial.pdf;/Users/felix/Zotero/storage/5MD8SPQ6/ab2a9e.html}
}

@unpublished{rechtOptimization2013,
  title = {Optimization {{I}}},
  author = {Recht, Benjamin},
  date = {2013-09-04},
  url = {https://simons.berkeley.edu/talks/ben-recht-2013-09-04},
  urldate = {2021-09-03},
  abstract = {Machine learning and computational statistics problems involving large datasets have proved to be a rich source of interesting and challenging optimization problems in recent years. The challenges arising from the complexity of these problems and the special requirements for their solutions have brought a wide range of optimization algorithms into play. We start this talk by surveying the application space, outlining several important analysis and learning tasks, and describing the contexts in which such problems are posed. We then describe optimization approaches that are proving to be relevant, including stochastic gradient methods, sparse optimization methods, first-order methods, coordinate descent, higher-order methods, and augmented Lagrangian methods. We also discuss parallel variants of some of these approaches.},
  eventtitle = {Big {{Data Boot Camp}}},
  langid = {english},
  venue = {{Simons Institute for the Theory of Computing, UC Berkeley}},
  annotation = {https://youtu.be/6WeyTUnbwQQ},
  file = {/Users/felix/gdrive/ZoteroPaper/2013_Recht/Recht_2013_Optimization I.pdf;/Users/felix/Zotero/storage/FP36RD4H/ben-recht-2013-09-04.html}
}

@online{reddiConvergenceAdam2019,
  title = {On the {{Convergence}} of {{Adam}} and {{Beyond}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  date = {2019-04-19},
  eprint = {1904.09237},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1904.09237},
  urldate = {2021-06-08},
  abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Reddi et al/Reddi et al_2019_On the Convergence of Adam and Beyond.pdf;/Users/felix/Zotero/storage/7HJCRKB3/1904.html}
}

@online{ruderOverviewGradientDescent2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  date = {2017-06-15},
  eprint = {1609.04747},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1609.04747},
  urldate = {2021-06-07},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Ruder/Ruder_2017_An overview of gradient descent optimization algorithms.pdf;/Users/felix/Zotero/storage/RZCQGVS2/1609.html}
}

@online{sakuraiAlternatingOptimizationMethod2016,
  title = {Alternating Optimization Method Based on Nonnegative Matrix Factorizations for Deep Neural Networks},
  author = {Sakurai, Tetsuya and Imakura, Akira and Inoue, Yuto and Futamura, Yasunori},
  date = {2016-05-15},
  eprint = {1605.04639},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1605.04639},
  urldate = {2021-06-08},
  abstract = {The backpropagation algorithm for calculating gradients has been widely used in computation of weights for deep neural networks (DNNs). This method requires derivatives of objective functions and has some difficulties finding appropriate parameters such as learning rate. In this paper, we propose a novel approach for computing weight matrices of fully-connected DNNs by using two types of semi-nonnegative matrix factorizations (semi-NMFs). In this method, optimization processes are performed by calculating weight matrices alternately, and backpropagation (BP) is not used. We also present a method to calculate stacked autoencoder using a NMF. The output results of the autoencoder are used as pre-training data for DNNs. The experimental results show that our method using three types of NMFs attains similar error rates to the conventional DNNs with BP.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2016_Sakurai et al/Sakurai et al_2016_Alternating optimization method based on nonnegative matrix factorizations for.pdf}
}

@article{scheutzowStochasticGronwallLemma2013,
  title = {A Stochastic Gronwall Lemma},
  author = {Scheutzow, Michael},
  date = {2013-06-01},
  journaltitle = {Infinite Dimensional Analysis, Quantum Probability and Related Topics},
  shortjournal = {Infin. Dimens. Anal. Quantum. Probab. Relat. Top.},
  volume = {16},
  number = {02},
  pages = {1350019},
  publisher = {{World Scientific Publishing Co.}},
  issn = {0219-0257},
  doi = {10.1142/S0219025713500197},
  abstract = {We prove a stochastic Gronwall lemma of the following type: if Z is an adapted non-negative continuous process which satisfies a linear integral inequality with an added continuous local martingale M and a process H on the right-hand side, then for any p ∈ (0, 1) the pth moment of the supremum of Z is bounded by a constant κp (which does not depend on M) times the pth moment of the supremum of H. Our main tool is a martingale inequality which is due to D. Burkholder. We provide an alternative simple proof of the martingale inequality which provides an explicit numerical value for the constant cp appearing in the inequality which is at most four times as large as the optimal constant.},
  keywords = {martingale inequality,Stochastic Gronwall lemma},
  file = {/Users/felix/gdrive/ZoteroPaper/2013_Scheutzow/Scheutzow_2013_A stochastic gronwall lemma.pdf}
}

@online{schmidtDescendingCrowdedValley2021,
  title = {Descending through a {{Crowded Valley}} -- {{Benchmarking Deep Learning Optimizers}}},
  author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
  date = {2021-02-11},
  eprint = {2007.01547},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.01547},
  urldate = {2021-06-15},
  abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of fifteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than \$50,000\$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific optimizers and parameter choices that generally lead to competitive results in our experiments: Adam remains a strong contender, with newer methods failing to significantly and consistently outperform it. Our open-sourced results are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Schmidt et al/Schmidt et al_2021_Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers.pdf;/Users/felix/Zotero/storage/TJ3EARHJ/2007.html}
}

@online{schneiderDeepOBSDeepLearning2019,
  title = {{{DeepOBS}}: A {{Deep Learning Optimizer Benchmark Suite}}},
  shorttitle = {{{DeepOBS}}},
  author = {Schneider, Frank and Balles, Lukas and Hennig, Philipp},
  date = {2019-03-13},
  eprint = {1903.05499},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1903.05499},
  urldate = {2021-05-19},
  abstract = {Because the choice and tuning of the optimizer affects the speed, and ultimately the performance of deep learning, there is significant past and recent research in this area. Yet, perhaps surprisingly, there is no generally agreed-upon protocol for the quantitative and reproducible evaluation of optimization strategies for deep learning. We suggest routines and benchmarks for stochastic optimization, with special focus on the unique aspects of deep learning, such as stochasticity, tunability and generalization. As the primary contribution, we present DeepOBS, a Python package of deep learning optimization benchmarks. The package addresses key challenges in the quantitative assessment of stochastic optimizers, and automates most steps of benchmarking. The library includes a wide and extensible set of ready-to-use realistic optimization problems, such as training Residual Networks for image classification on ImageNet or character-level language prediction models, as well as popular classics like MNIST and CIFAR-10. The package also provides realistic baseline results for the most popular optimizers on these test problems, ensuring a fair comparison to the competition when benchmarking new optimizers, and without having to run costly experiments. It comes with output back-ends that directly produce LaTeX code for inclusion in academic publications. It supports TensorFlow and is available open source.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Schneider et al/Schneider et al_2019_DeepOBS.pdf;/Users/felix/Zotero/storage/8MUAXKAE/1903.html}
}

@article{schwartzRungeKuttaDiscretizationOptimal1996,
  title = {Runge-{{Kutta Discretization}} of {{Optimal Control Problems}}},
  author = {Schwartz, A. and Polak, E.},
  date = {1996-12-01},
  journaltitle = {IFAC Proceedings Volumes},
  shortjournal = {IFAC Proceedings Volumes},
  series = {10th {{IFAC Workshop}} on {{Control Applications}} of {{Optimization}} 1995, {{Haifa}}, {{Israel}}, 19-21 {{December}}},
  volume = {29},
  number = {8},
  pages = {123--128},
  issn = {1474-6670},
  doi = {10.1016/S1474-6670(17)43687-1},
  abstract = {Runge-Kutta integration is used to construct finite-dimensional approximating problems that are consistent approximations, in the sense of Polak (1993), to an original optimal control problem. Stationary points and global solutions of these approximating discrete-time optimal control problems converge, as the discretization level is increased, to stationary points and global solutions of the original problem. The approximating problems involve finite-dimensional spaces of control coefficients. In solving the discrete-time approximating problems, a non-Euclidean inner product should be used on these coefficient spaces to avoid ill-conditioning. This result applies to any discretization method, not just Runge-Kutta integration. Significantly, not all Runge-Kutta methods (even full-order methods) lead to consistent approximations.},
  langid = {english},
  keywords = {numerical methods,optimal control,runge-kutta integration,splines},
  file = {/Users/felix/gdrive/ZoteroPaper/1996_Schwartz_Polak/Schwartz_Polak_1996_Runge-Kutta Discretization of Optimal Control Problems.pdf;/Users/felix/Zotero/storage/C7VYCHVN/S1474667017436871.html}
}

@article{sejnowskiUnreasonableEffectivenessDeep2020,
  title = {The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence},
  author = {Sejnowski, Terrence J.},
  date = {2020-12-01},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {117},
  number = {48},
  eprint = {31992643},
  eprinttype = {pmid},
  pages = {30033--30038},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1907373117},
  abstract = {Deep learning networks have been trained to recognize speech, caption photographs, and translate text between languages at high levels of performance. Although applications of deep learning networks to real-world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and nonconvex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures, and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.},
  langid = {english},
  keywords = {artificial intelligence,deep learning,neural networks},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Sejnowski/Sejnowski_2020_The unreasonable effectiveness of deep learning in artificial intelligence.pdf;/Users/felix/Zotero/storage/L3VMZ7HC/30033.html}
}

@book{shewchukIntroductionConjugateGradient1994,
  title = {An Introduction to the Conjugate Gradient Method without the Agonizing Pain},
  author = {Shewchuk, Jonathan Richard},
  date = {1994},
  publisher = {{Carnegie-Mellon University. Department of Computer Science}},
  file = {/Users/felix/gdrive/ZoteroPaper/1994_Shewchuk/Shewchuk_1994_An introduction to the conjugate gradient method without the agonizing pain.pdf}
}

@online{simsekliTailIndexAnalysisStochastic2019,
  title = {A {{Tail}}-{{Index Analysis}} of {{Stochastic Gradient Noise}} in {{Deep Neural Networks}}},
  author = {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  date = {2019-01-17},
  eprint = {1901.06053},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.06053},
  urldate = {2021-05-27},
  abstract = {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed \$\textbackslash alpha\$-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a L\textbackslash '\{e\}vy motion. Such SDEs can incur `jumps', which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the \$\textbackslash alpha\$-stable assumption, we conduct extensive experiments on common deep learning architectures and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We further investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Simsekli et al/Simsekli et al_2019_A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks.pdf;/Users/felix/Zotero/storage/L46UQ4VK/1901.html}
}

@inreference{SimulatedAnnealing2021,
  title = {Simulated Annealing},
  booktitle = {Wikipedia},
  date = {2021-04-13T05:09:05Z},
  url = {https://en.wikipedia.org/w/index.php?title=Simulated_annealing&oldid=1017509035},
  urldate = {2021-06-07},
  abstract = {Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. It is often used when the search space is discrete (e.g., the traveling salesman problem). For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to exact algorithms such as gradient descent or branch and bound. The name of the algorithm comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material to increase the size of its crystals and reduce their defects. Both are attributes of the material that depend on their thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy or Gibbs energy. Simulated annealing can be used for very hard computational optimization problems where exact algorithms fail; even though it usually achieves an approximate solution to the global minimum, it could be enough for many practical problems. The problems solved by SA are currently formulated by an objective function of many variables, subject to several constraints. In practice, the constraint can be penalized as part of the objective function. Similar techniques have been independently introduced on several occasions, including Pincus (1970), Khachaturyan et al (1979, 1981), Kirkpatrick, Gelatt and Vecchi (1983), and Cerny (1985). In 1983, this approach was used by Kirkpatrick, Gelatt Jr., Vecchi, for a solution of the traveling salesman problem. They also proposed its current name, simulated annealing. This notion of slow cooling implemented in the simulated annealing algorithm is interpreted as a slow decrease in the probability of accepting worse solutions as the solution space is explored. Accepting worse solutions allows for a more extensive search for the global optimal solution. In general, simulated annealing algorithms work as follows. The temperature progressively decreases from an initial positive value to zero. At each time step, the algorithm randomly selects a solution close to the current one, measures its quality, and moves to it according to the temperature-dependent probabilities of selecting better or worse solutions, which during the search respectively remain at 1 (or positive) and decrease towards zero. The simulation can be performed either by a solution of kinetic equations for density functions or by using the stochastic sampling method. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, published by N. Metropolis et al. in 1953.},
  langid = {english},
  annotation = {Page Version ID: 1017509035},
  file = {/Users/felix/Zotero/storage/C8QQW6LI/index.html}
}

@online{smithDonDecayLearning2018,
  title = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  date = {2018-02-23},
  eprint = {1711.00489},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.00489},
  urldate = {2021-10-06},
  abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \$\textbackslash epsilon\$ and scaling the batch size \$B \textbackslash propto \textbackslash epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B \textbackslash propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1\textbackslash\%\$ validation accuracy in under 30 minutes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Smith et al/Smith et al_2018_Don't Decay the Learning Rate, Increase the Batch Size.pdf;/Users/felix/Zotero/storage/WUIKNVAH/1711.html}
}

@online{soudryExponentiallyVanishingSuboptimal2017,
  title = {Exponentially Vanishing Sub-Optimal Local Minima in Multilayer Neural Networks},
  author = {Soudry, Daniel and Hoffer, Elad},
  date = {2017-10-28},
  eprint = {1702.05777},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1702.05777},
  urldate = {2021-06-30},
  abstract = {Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., "near" linear separability), or an unrealistic hidden layer with \$\textbackslash Omega\textbackslash left(N\textbackslash right)\$ units. Results: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of \$N\textbackslash rightarrow\textbackslash infty\$ datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension \$d\_\{0\}=\textbackslash tilde\{\textbackslash Omega\}\textbackslash left(\textbackslash sqrt\{N\}\textbackslash right)\$, and a more realistic number of \$d\_\{1\}=\textbackslash tilde\{\textbackslash Omega\}\textbackslash left(N/d\_\{0\}\textbackslash right)\$ hidden units. We demonstrate our results numerically: for example, \$0\textbackslash\%\$ binary classification training error on CIFAR with only \$N/d\_\{0\}\textbackslash approx 16\$ hidden neurons.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Soudry_Hoffer/Soudry_Hoffer_2017_Exponentially vanishing sub-optimal local minima in multilayer neural networks.pdf;/Users/felix/Zotero/storage/CRTJZ6GA/1702.html}
}

@online{stankewitzInexactOptimizationLearning2021,
  title = {From Inexact Optimization to Learning via Gradient Concentration},
  author = {Stankewitz, Bernhard and Mücke, Nicole and Rosasco, Lorenzo},
  date = {2021-06-24},
  eprint = {2106.05397},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2106.05397},
  urldate = {2021-10-01},
  abstract = {Optimization was recently shown to control the inductive bias in a learning process, a property referred to as implicit, or iterative regularization. The estimator obtained iteratively minimizing the training error can generalise well with no need of further penalties or constraints. In this paper, we investigate this phenomenon in the context of linear models with smooth loss functions. In particular, we investigate and propose a proof technique combining ideas from inexact optimization and probability theory, specifically gradient concentration. The proof is easy to follow and allows to obtain sharp learning bounds. More generally, it highlights a way to develop optimization results into learning guarantees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Stankewitz et al/Stankewitz et al_2021_From inexact optimization to learning via gradient concentration.pdf;/Users/felix/Zotero/storage/EWM9GUKE/2106.html}
}

@online{suDifferentialEquationModeling2015,
  title = {A {{Differential Equation}} for {{Modeling Nesterov}}'s {{Accelerated Gradient Method}}: Theory and {{Insights}}},
  shorttitle = {A {{Differential Equation}} for {{Modeling Nesterov}}'s {{Accelerated Gradient Method}}},
  author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
  date = {2015-10-27},
  eprint = {1503.01243},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1503.01243},
  urldate = {2021-07-12},
  abstract = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Classical Analysis and ODEs,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Su et al/Su et al_2015_A Differential Equation for Modeling Nesterov's Accelerated Gradient Method.pdf;/Users/felix/Zotero/storage/Y73GE2KR/1503.html}
}

@online{sunMarkovChainGradient2018,
  title = {On {{Markov Chain Gradient Descent}}},
  author = {Sun, Tao and Sun, Yuejiao and Yin, Wotao},
  date = {2018-09-11},
  eprint = {1809.04216},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1809.04216},
  urldate = {2021-09-28},
  abstract = {Stochastic gradient methods are the workhorse (algorithms) of large-scale optimization problems in machine learning, signal processing, and other computational sciences and engineering. This paper studies Markov chain gradient descent, a variant of stochastic gradient descent where the random samples are taken on the trajectory of a Markov chain. Existing results of this method assume convex objectives and a reversible Markov chain and thus have their limitations. We establish new non-ergodic convergence under wider step sizes, for nonconvex problems, and for non-reversible finite-state Markov chains. Nonconvexity makes our method applicable to broader problem classes. Non-reversible finite-state Markov chains, on the other hand, can mix substatially faster. To obtain these results, we introduce a new technique that varies the mixing levels of the Markov chains. The reported numerical results validate our contributions.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Sun et al/Sun et al_2018_On Markov Chain Gradient Descent.pdf;/Users/felix/Zotero/storage/LVX3YPTI/1809.html}
}

@article{sunNonErgodicConvergenceAnalysis2019,
  title = {Non-{{Ergodic Convergence Analysis}} of {{Heavy}}-{{Ball Algorithms}}},
  author = {Sun, Tao and Yin, Penghang and Li, Dongsheng and Huang, Chun and Guan, Lei and Jiang, Hao},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {5033--5040},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33015033},
  abstract = {In this paper, we revisit the convergence of the Heavy-ball method, and present improved convergence complexity results in the convex setting. We provide the first non-ergodic O(1/k) rate result of the Heavy-ball algorithm with constant step size for coercive objective functions. For objective functions satisfying a relaxed strongly convex condition, the linear convergence is established under weaker assumptions on the step size and inertial parameter than made in the existing literature. We extend our results to multi-block version of the algorithm with both the cyclic and stochastic update rules. In addition, our results can also be extended to decentralized optimization, where the ergodic analysis is not applicable.},
  issue = {01},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Sun et al/Sun et al_2019_Non-Ergodic Convergence Analysis of Heavy-Ball Algorithms.pdf}
}

@inproceedings{sutskeverImportanceInitializationMomentum2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  date = {2013-05-26},
  pages = {1139--1147},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v28/sutskever13.html},
  urldate = {2021-06-07},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this pa...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2013_Sutskever et al/Sutskever et al_2013_On the importance of initialization and momentum in deep learning.pdf;/Users/felix/Zotero/storage/ZB8G53F7/sutskever13.html}
}

@book{sutskeverTrainingRecurrentNeural2013,
  title = {Training Recurrent Neural Networks},
  author = {Sutskever, Ilya},
  date = {2013},
  publisher = {{University of Toronto Toronto, Canada}},
  file = {/Users/felix/gdrive/ZoteroPaper/2013_Sutskever/Sutskever_2013_Training recurrent neural networks.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {526},
  keywords = {Reinforcement learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Sutton_Barto/Sutton_Barto_2018_Reinforcement learning.pdf;/Users/felix/gdrive/ZoteroPaper/2018_Sutton_Barto/Sutton_Barto_2018_Reinforcement learning2.pdf}
}

@online{swirszczLocalMinimaTraining2017,
  title = {Local Minima in Training of Neural Networks},
  author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  date = {2017-02-17},
  eprint = {1611.06310},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.06310},
  urldate = {2021-06-16},
  abstract = {There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Swirszcz et al/Swirszcz et al_2017_Local minima in training of neural networks.pdf;/Users/felix/Zotero/storage/NN572T58/1611.html}
}

@online{tiaoVisualizingAnimatingOptimization2016,
  title = {Visualizing and {{Animating Optimization Algorithms}} with {{Matplotlib}}},
  author = {Tiao, Louis},
  date = {2016-04-26T22:13:17+10:00},
  url = {http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/},
  urldate = {2021-06-08},
  abstract = {In this series of notebooks, we demonstrate some useful patterns and recipes for visualizing animating optimization algorithms using Matplotlib. In~[1]:      \%matplotlib inline In~[2]:},
  langid = {english},
  organization = {{Louis Tiao}},
  file = {/Users/felix/Zotero/storage/NW3E5TH8/visualizing-and-animating-optimization-algorithms-with-matplotlib.html}
}

@online{truongBacktrackingGradientDescent2019,
  title = {Backtracking Gradient Descent Method for General $C^1$ Functions, with Applications to {{Deep Learning}}},
  author = {Truong, Tuyen Trung and Nguyen, Tuan Hang},
  date = {2019-04-04},
  eprint = {1808.05160},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1808.05160},
  urldate = {2021-09-27},
  abstract = {While Standard gradient descent is one very popular optimisation method, its convergence cannot be proven beyond the class of functions whose gradient is globally Lipschitz continuous. As such, it is not actually applicable to realistic applications such as Deep Neural Networks. In this paper, we prove that its backtracking variant behaves very nicely, in particular convergence can be shown for all Morse functions. The main theoretical result of this paper is as follows. Theorem. Let \$f:\textbackslash mathbb\{R\}\^k\textbackslash rightarrow \textbackslash mathbb\{R\}\$ be a \$C\^1\$ function, and \$\textbackslash\{z\_n\textbackslash\}\$ a sequence constructed from the Backtracking gradient descent algorithm. (1) Either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_\{n+1\}-z\_n||=0\$. (2) Assume that \$f\$ has at most countably many critical points. Then either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash\{z\_n\textbackslash\}\$ converges to a critical point of \$f\$. (3) More generally, assume that all connected components of the set of critical points of \$f\$ are compact. Then either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash\{z\_n\textbackslash\}\$ is bounded. Moreover, in the latter case the set of cluster points of \$\textbackslash\{z\_n\textbackslash\}\$ is connected. Some generalised versions of this result, including an inexact version, are included. Another result in this paper concerns the problem of saddle points. We then present a heuristic argument to explain why Standard gradient descent method works so well, and modifications of the backtracking versions of GD, MMT and NAG. Experiments with datasets CIFAR10 and CIFAR100 on various popular architectures verify the heuristic argument also for the mini-batch practice and show that our new algorithms, while automatically fine tuning learning rates, perform better than current state-of-the-art methods such as MMT, NAG, Adagrad, Adadelta, RMSProp, Adam and Adamax.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2019_Truong_Nguyen/Truong_Nguyen_2019_Backtracking gradient descent method for general $C^1$ functions, with.pdf;/Users/felix/Zotero/storage/43XEVGBC/1808.html}
}

@article{truongBacktrackingGradientDescent2021,
  title = {Backtracking {{Gradient Descent Method}} and {{Some Applications}} in {{Large Scale Optimisation}}. {{Part}} 2: Algorithms and {{Experiments}}},
  shorttitle = {Backtracking {{Gradient Descent Method}} and {{Some Applications}} in {{Large Scale Optimisation}}. {{Part}} 2},
  author = {Truong, Tuyen Trung and Nguyen, Hang-Tuan},
  date = {2021-12-01},
  journaltitle = {Applied Mathematics \& Optimization},
  shortjournal = {Appl Math Optim},
  volume = {84},
  number = {3},
  pages = {2557--2586},
  issn = {1432-0606},
  doi = {10.1007/s00245-020-09718-8},
  abstract = {In this paper, we provide new results and algorithms (including backtracking versions of Nesterov accelerated gradient and Momentum) which are more applicable to large scale optimisation as in Deep Neural Networks. We also demonstrate that Backtracking Gradient Descent (Backtracking GD) can obtain good upper bound estimates for local Lipschitz constants for the gradient, and that the convergence rate of Backtracking GD is similar to that in classical work of Armijo.  Experiments with datasets CIFAR10 and CIFAR100 on various popular architectures verify a heuristic argument that Backtracking GD stabilises to a finite union of sequences constructed from Standard GD for the mini-batch practice, and show that our new algorithms (while automatically fine tuning learning rates) perform better than current state-of-the-art methods such as Adam, Adagrad, Adadelta, RMSProp, Momentum and Nesterov accelerated gradient. To help readers avoiding the confusion between heuristics and more rigorously justified algorithms, we also provide a review of the current state of convergence results for gradient descent methods. Accompanying source codes are available on GitHub.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Truong_Nguyen/Truong_Nguyen_2021_Backtracking Gradient Descent Method and Some Applications in Large Scale.pdf}
}

@online{truongFastSimpleModification2021,
  title = {A Fast and Simple Modification of {{Newton}}'s Method Helping to Avoid Saddle Points},
  author = {Truong, Tuyen Trung and To, Tat Dat and Nguyen, Tuan Hang and Nguyen, Thu Hang and Nguyen, Hoang Phuong and Helmy, Maged},
  date = {2021-03-15},
  eprint = {2006.01512},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2006.01512},
  urldate = {2021-06-17},
  abstract = {We propose in this paper New Q-Newton's method. The update rule for the simplest version is \$x\_\{n+1\}=x\_n-w\_n\$ where \$w\_n=pr\_\{A\_n,+\}(v\_n)-pr\_\{A\_n,-\}(v\_n)\$, with \$A\_n=\textbackslash nabla \^2f(x\_n)+\textbackslash delta \_n||\textbackslash nabla f(x\_n)||\^2 .Id\$ and \$v\_n=A\_n\^\{-1\}.\textbackslash nabla f(x\_n)\$. Here \$\textbackslash delta \_n\$ is an appropriate real number so that \$A\_n\$ is invertible, and \$pr\_\{A\_n,\textbackslash pm\}\$ are projections to the vector subspaces generated by eigenvectors of positive (correspondingly negative) eigenvalues of \$A\_n\$. The main result of this paper roughly says that if \$f\$ is \$C\^3\$ and a sequence \$\textbackslash\{x\_n\textbackslash\}\$, constructed by the New Q-Newton's method from a random initial point \$x\_0\$, \{\textbackslash bf converges\}, then the limit point is a critical point and is not a saddle point, and the convergence rate is the same as that of Newton's method. At the end of the paper, we present some issues (saddle points and convergence) one faces when implementing Newton's method and modifications into Deep Neural Networks. In the appendix, we test the good performance of New Q-Newton's method on various benchmark test functions such as Rastrigin, Askley, Rosenbroch and many other, against algorithms such as Newton's method, BFGS, Adaptive Cubic Regularization, Random damping Newton's method and Inertial Newton's method, as well as Unbounded Two-way Backtracking Gradient Descent. The experiments demonstrate in particular that the assumption that \$f\$ is \$C\^3\$ is necessary for some conclusions in the main theoretical results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Truong et al/Truong et al_2021_A fast and simple modification of Newton's method helping to avoid saddle points.pdf;/Users/felix/Zotero/storage/8GQNPF9E/2006.html}
}

@inproceedings{tsybakovOptimalRatesAggregation2003,
  title = {Optimal {{Rates}} of {{Aggregation}}},
  booktitle = {Learning {{Theory}} and {{Kernel Machines}}},
  author = {Tsybakov, Alexandre B.},
  editor = {Schölkopf, Bernhard and Warmuth, Manfred K.},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {303--313},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45167-9_23},
  abstract = {We study the problem of aggregation of M arbitrary estimators of a regression function with respect to the mean squared risk. Three main types of aggregation are considered: model selection, convex and linear aggregation. We define the notion of optimal rate of aggregation in an abstract context and prove lower bounds valid for any method of aggregation. We then construct procedures that attain these bounds, thus establishing optimal rates of linear, convex and model selection type aggregation.},
  isbn = {978-3-540-45167-9},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2003_Tsybakov/Tsybakov_2003_Optimal Rates of Aggregation.pdf}
}

@article{vanscoyFastestKnownGlobally2018,
  title = {The {{Fastest Known Globally Convergent First}}-{{Order Method}} for {{Minimizing Strongly Convex Functions}}},
  author = {Van Scoy, Bryan and Freeman, Randy A. and Lynch, Kevin M.},
  date = {2018-01},
  journaltitle = {IEEE Control Systems Letters},
  volume = {2},
  number = {1},
  pages = {49--54},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2017.2722406},
  abstract = {We design and analyze a novel gradient-based algorithm for unconstrained convex optimization. When the objective function is m-strongly convex and its gradient is L-Lipschitz continuous, the iterates and function values converge linearly to the optimum at rates p and p2, respectively, where p = 1 - √m/L. These are the fastest known guaranteed linear convergence rates for globally convergent first-order methods, and for high desired accuracies the corresponding iteration complexity is within a factor of two of the theoretical lower bound. We use a simple graphical design procedure based on integral quadratic constraints to derive closed-form expressions for the algorithm parameters. The new algorithm, which we call the triple momentum method, can be seen as an extension of methods such as gradient descent, Nesterov's accelerated gradient descent, and the heavy-ball method.},
  eventtitle = {{{IEEE Control Systems Letters}}},
  keywords = {Acceleration,Algorithm design and analysis,Complexity theory,Convergence,Linear programming,Optimization,Optimization algorithms,robust control,Transfer functions},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Van Scoy et al/Van Scoy et al_2018_The Fastest Known Globally Convergent First-Order Method for Minimizing.pdf;/Users/felix/Zotero/storage/S5FTAY9R/7967721.html}
}

@article{vapnikOverviewStatisticalLearning1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, V.N.},
  date = {1999-09},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {10},
  number = {5},
  pages = {988--999},
  issn = {1941-0093},
  doi = {10.1109/72.788640},
  abstract = {Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Algorithm design and analysis,Loss measurement,Machine learning,Multidimensional systems,Pattern recognition,Probability distribution,Risk management,Statistical learning,Support vector machines},
  file = {/Users/felix/gdrive/ZoteroPaper/1999_Vapnik/Vapnik_1999_An overview of statistical learning theory.pdf;/Users/felix/Zotero/storage/6FCWKDS3/788640.html}
}

@online{vieillardMomentumReinforcementLearning2020,
  title = {Momentum in {{Reinforcement Learning}}},
  author = {Vieillard, Nino and Scherrer, Bruno and Pietquin, Olivier and Geist, Matthieu},
  date = {2020-03-31},
  eprint = {1910.09322},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.09322},
  urldate = {2021-09-20},
  abstract = {We adapt the optimization's concept of momentum to reinforcement learning. Seeing the state-action value functions as an analog to the gradients in optimization, we interpret momentum as an average of consecutive \$q\$-functions. We derive Momentum Value Iteration (MoVI), a variation of Value Iteration that incorporates this momentum idea. Our analysis shows that this allows MoVI to average errors over successive iterations. We show that the proposed approach can be readily extended to deep learning. Specifically, we propose a simple improvement on DQN based on MoVI, and experiment it on Atari games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Vieillard et al/Vieillard et al_2020_Momentum in Reinforcement Learning.pdf;/Users/felix/Zotero/storage/FIGNTXJ9/1910.html}
}

@inproceedings{vinyalsKrylovSubspaceDescent2012,
  title = {Krylov {{Subspace Descent}} for {{Deep Learning}}},
  booktitle = {Proceedings of the {{Fifteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Vinyals, Oriol and Povey, Daniel},
  date = {2012-03-21},
  pages = {1261--1268},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v22/vinyals12.html},
  urldate = {2021-09-29},
  abstract = {In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high.  In our method, we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix, and then use a subset of the training data samples to optimize over this subspace.  As with the Hessian Free (HF) method of Martens (2010), the Hessian matrix is never explicitly constructed, and is computed using a subset of data.  In practice, as in HF, we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix.  We investigate the effectiveness of our proposed method on deep neural networks, and compare its performance to widely used methods such as stochastic gradient descent, conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster convergence than either L-BFGS or HF, and generally performs better than either of them in cross-validation accuracy.  It is also simpler and more general than HF, as it does not require a positive semidefinite approximation of the Hessian matrix to work well nor the setting of a damping parameter.  The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2012_Vinyals_Povey/Vinyals_Povey_2012_Krylov Subspace Descent for Deep Learning.pdf}
}

@online{wilsonMarginalValueAdaptive2018,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  date = {2018-05-21},
  eprint = {1705.08292},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.08292},
  urldate = {2021-04-19},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Wilson et al/Wilson et al_2018_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf;/Users/felix/Zotero/storage/59ZDTSIM/1705.html}
}

@article{wuHowSGDSelects2018,
  title = {How {{SGD Selects}} the {{Global Minima}} in {{Over}}-Parameterized {{Learning}}: A {{Dynamical Stability Perspective}}},
  author = {Wu, Lei and Ma, Chao},
  date = {2018},
  pages = {10},
  abstract = {The question of which global minima are accessible by a stochastic gradient decent (SGD) algorithm with specific learning rate and batch size is studied from the perspective of dynamical stability. The concept of non-uniformity is introduced, which, together with sharpness, characterizes the stability property of a global minimum and hence the accessibility of a particular SGD algorithm to that global minimum. In particular, this analysis shows that learning rate and batch size play different roles in minima selection. Extensive empirical results seem to correlate well with the theoretical findings and provide further support to these claims.},
  langid = {english},
  file = {/Users/felix/gdrive/ZoteroPaper/2018_Wu_Ma/Wu_Ma_2018_How SGD Selects the Global Minima in Over-parameterized Learning.pdf}
}

@online{yangUnifiedConvergenceAnalysis2016,
  title = {Unified {{Convergence Analysis}} of {{Stochastic Momentum Methods}} for {{Convex}} and {{Non}}-Convex {{Optimization}}},
  author = {Yang, Tianbao and Lin, Qihang and Li, Zhe},
  date = {2016-05-04},
  eprint = {1604.03257},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1604.03257},
  urldate = {2021-09-20},
  abstract = {Recently, \{\textbackslash it stochastic momentum\} methods have been widely adopted in training deep neural networks. However, their convergence analysis is still underexplored at the moment, in particular for non-convex optimization. This paper fills the gap between practice and theory by developing a basic convergence analysis of two stochastic momentum methods, namely stochastic heavy-ball method and the stochastic variant of Nesterov's accelerated gradient method. We hope that the basic convergence results developed in this paper can serve the reference to the convergence of stochastic momentum methods and also serve the baselines for comparison in future development of stochastic momentum methods. The novelty of convergence analysis presented in this paper is a unified framework, revealing more insights about the similarities and differences between different stochastic momentum methods and stochastic gradient method. The unified framework exhibits a continuous change from the gradient method to Nesterov's accelerated gradient method and finally the heavy-ball method incurred by a free parameter, which can help explain a similar change observed in the testing error convergence behavior for deep learning. Furthermore, our empirical results for optimizing deep neural networks demonstrate that the stochastic variant of Nesterov's accelerated gradient method achieves a good tradeoff (between speed of convergence in training error and robustness of convergence in testing error) among the three stochastic methods.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2016_Yang et al/Yang et al_2016_Unified Convergence Analysis of Stochastic Momentum Methods for Convex and.pdf;/Users/felix/Zotero/storage/3P5ILC4P/1604.html}
}

@article{yaoAdaptiveThreetermConjugate2018,
  title = {An Adaptive Three-Term Conjugate Gradient Method Based on Self-Scaling Memoryless {{BFGS}} Matrix},
  author = {Yao, Shengwei and Ning, Liangshuo},
  date = {2018-04-01},
  journaltitle = {Journal of Computational and Applied Mathematics},
  shortjournal = {Journal of Computational and Applied Mathematics},
  volume = {332},
  pages = {72--85},
  issn = {0377-0427},
  doi = {10.1016/j.cam.2017.10.013},
  abstract = {Due to its simplicity and low memory requirement, conjugate gradient methods are widely used for solving large-scale unconstrained optimization problems. In this paper, we propose a three-term conjugate gradient method. The search direction is given by a symmetrical Perry matrix, which contains a positive parameter. The value of this parameter is determined by minimizing the distance of this matrix and the self-scaling memoryless BFGS matrix in the Frobenius norm. The sufficient descent property of the generated directions holds independent of line searches. The global convergence of the given method is established under Wolfe line search for general non-convex functions. Numerical experiments show that the proposed method is promising.},
  langid = {english},
  keywords = {Conjugate gradient method,Global convergence,Self-scaling memoryless BFGS matrix,Unconstrained optimization},
  file = {/Users/felix/Zotero/storage/YYQV6SMD/S0377042717305034.html}
}

@online{zeilerADADELTAAdaptiveLearning2012,
  title = {{{ADADELTA}}: An {{Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  author = {Zeiler, Matthew D.},
  date = {2012-12-22},
  eprint = {1212.5701},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1212.5701},
  urldate = {2021-11-15},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2012_Zeiler/Zeiler_2012_ADADELTA.pdf;/Users/felix/Zotero/storage/KEXJ9289/1212.html}
}

@online{zhangDeepLearningElastic2015,
  title = {Deep Learning with {{Elastic Averaging SGD}}},
  author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
  date = {2015-10-25},
  eprint = {1412.6651},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1412.6651},
  urldate = {2021-06-08},
  abstract = {We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2015_Zhang et al/Zhang et al_2015_Deep learning with Elastic Averaging SGD.pdf;/Users/felix/Zotero/storage/WHHMYVJ4/1412.html}
}

@online{zhangUnderstandingDeepLearning2017,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  date = {2017-02-26},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.03530},
  urldate = {2021-06-16},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2017_Zhang et al/Zhang et al_2017_Understanding deep learning requires rethinking generalization.pdf;/Users/felix/Zotero/storage/988LPWUC/1611.html}
}

@online{zhangWhyFlatnessDoes2021,
  title = {Why Flatness Does and Does Not Correlate with Generalization for Deep Neural Networks},
  author = {Zhang, Shuofeng and Reid, Isaac and Pérez, Guillermo Valle and Louis, Ard},
  date = {2021-06-21},
  eprint = {2103.06219},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2103.06219},
  urldate = {2021-10-06},
  abstract = {The intuition that local flatness of the loss landscape is correlated with better generalization for deep neural networks (DNNs) has been explored for decades, spawning many different flatness measures. Recently, this link with generalization has been called into question by a demonstration that many measures of flatness are vulnerable to parameter re-scaling which arbitrarily changes their value without changing neural network outputs. Here we show that, in addition, some popular variants of SGD such as Adam and Entropy-SGD, can also break the flatness-generalization correlation. As an alternative to flatness measures, we use a function based picture and propose using the log of Bayesian prior upon initialization, \$\textbackslash log P(f)\$, as a predictor of the generalization when a DNN converges on function \$f\$ after training to zero error. The prior is directly proportional to the Bayesian posterior for functions that give zero error on a test set. For the case of image classification, we show that \$\textbackslash log P(f)\$ is a significantly more robust predictor of generalization than flatness measures are. Whilst local flatness measures fail under parameter re-scaling, the prior/posterior, which is global quantity, remains invariant under re-scaling. Moreover, the correlation with generalization as a function of data complexity remains good for different variants of SGD.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2021_Zhang et al/Zhang et al_2021_Why flatness does and does not correlate with generalization for deep neural.pdf;/Users/felix/Zotero/storage/NRZIMHZ7/2103.html}
}

@online{zhouTheoreticallyUnderstandingWhy2020,
  title = {Towards {{Theoretically Understanding Why SGD Generalizes Better Than ADAM}} in {{Deep Learning}}},
  author = {Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and HOI, Steven and E, Weinan},
  date = {2020-10-12},
  eprint = {2010.05627},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2010.05627},
  urldate = {2021-09-28},
  abstract = {It is not clear yet why ADAM-alike adaptive gradient algorithms suffer from worse generalization performance than SGD despite their faster training speed. This work aims to provide understandings on this generalization gap by analyzing their local convergence behaviors. Specifically, we observe the heavy tails of gradient noise in these algorithms. This motivates us to analyze these algorithms through their Levy-driven stochastic differential equations (SDEs) because of the similar convergence behaviors of an algorithm and its SDE. Then we establish the escaping time of these SDEs from a local basin. The result shows that (1) the escaping time of both SGD and ADAM\textasciitilde depends on the Radon measure of the basin positively and the heaviness of gradient noise negatively; (2) for the same basin, SGD enjoys smaller escaping time than ADAM, mainly because (a) the geometry adaptation in ADAM\textasciitilde via adaptively scaling each gradient coordinate well diminishes the anisotropic structure in gradient noise and results in larger Radon measure of a basin; (b) the exponential gradient average in ADAM\textasciitilde smooths its gradient and leads to lighter gradient noise tails than SGD. So SGD is more locally unstable than ADAM\textasciitilde at sharp minima defined as the minima whose local basins have small Radon measure, and can better escape from them to flatter ones with larger Radon measure. As flat minima here which often refer to the minima at flat or asymmetric basins/valleys often generalize better than sharp ones\textasciitilde\textbackslash cite\{keskar2016large,he2019asymmetric\}, our result explains the better generalization performance of SGD over ADAM. Finally, experimental results confirm our heavy-tailed gradient noise assumption and theoretical affirmation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/gdrive/ZoteroPaper/2020_Zhou et al/Zhou et al_2020_Towards Theoretically Understanding Why SGD Generalizes Better Than ADAM in.pdf;/Users/felix/Zotero/storage/VJGSCQN4/2010.html}
}

@inproceedings{zinkevichOnlineConvexProgramming2003,
  title = {Online Convex Programming and Generalized Infinitesimal Gradient Ascent},
  booktitle = {Proceedings of the 20th International Conference on Machine Learning (Icml-03)},
  author = {Zinkevich, Martin},
  date = {2003},
  pages = {928--936},
  abstract = {Convex programming involves a convex set F \textbackslash subseteq R\^n and a convex cost function c : F \textbackslash to R. The goal of convex programming is to find a point in F which minimizes c. In online convex programming, the convex set is known in advance, but in each step of some repeated optimization problem, one must select a point in F before seeing the cost function for that step. This can be used to model factory production, farm production, and many other industrial optimization problems where one is unaware of the value of the items produced until they have already been constructed. We introduce an algorithm for this domain. We also apply this algorithm to repeated games, and show that it is really a generalization of ininitesimal gradient ascent, and the results here imply that generalized infinitesimal gradient ascent (GIGA) is universally consistent.},
  file = {/Users/felix/gdrive/ZoteroPaper/2003_Zinkevich/Zinkevich_2003_Online convex programming and generalized infinitesimal gradient ascent.pdf}
}


